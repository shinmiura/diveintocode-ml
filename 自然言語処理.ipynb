{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "自然言語処理.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOAMZNobKB6HrKfJ4SvqBec",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinmiura/diveintocode-ml/blob/master/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am4JM2YrxKXB",
        "outputId": "b38d717b-eb2b-4345-ff39-ad2baa18d9ed"
      },
      "source": [
        "# nvidia-smiでNVIDIA GPU使用状況をモニタリングする\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Oct  2 06:57:25 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwhoOTInqJFf"
      },
      "source": [
        "**Sprintの目的**\n",
        "\n",
        "・自然言語処理の一連の流れを学ぶ\n",
        "\n",
        "・自然言語のベクトル化の方法を学ぶ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1YRRhZbqY-w"
      },
      "source": [
        "**どのように学ぶか**\n",
        "\n",
        "自然言語処理定番のデータセットを用いて、一連の流れを見ていきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqc84xotwcaF"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_files\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import itertools\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import lightgbm as lgb\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from gensim.models import word2vec\n",
        "from sklearn.preprocessing import normalize\n",
        "# 単語単位の分割ができるモジュール\n",
        "import re"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMkCCT8Oqk-K"
      },
      "source": [
        "# 2.自然言語のベクトル化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi4GYiHIqqD4"
      },
      "source": [
        "自然言語処理（NLP, Natural Language Processing） とは人間が普段使っている 自然言語 をコンピュータに処理させる技術のことです。ここではその中でも、機械学習の入力として自然言語を用いることを考えていきます。\n",
        "\n",
        "\n",
        "多くの機械学習手法は 数値データ（量的変数） の入力を前提にしていますので、自然言語の テキストデータ を数値データに変換する必要があります。これを 自然言語のベクトル化 と呼びます。ベクトル化の際にテキストデータの特徴をうまく捉えられるよう、さまざまな手法が考えられてきていますので、このSprintではそれらを学びます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r36KVPhE2s5Q"
      },
      "source": [
        "**非構造化データ**\n",
        "\n",
        "データの分類として、表に数値がまとめられたようなコンピュータが扱いやすい形を 構造化データ 、人間が扱いやすい画像・動画・テキスト・音声などを 非構造化データ と呼ぶことがあります。自然言語のベクトル化は、非構造化データを構造化データに変換する工程と言えます。同じ非構造化データでも、画像に対してはディープラーニングを用いる場合この変換作業はあまり必要がありませんでしたが、テキストにおいてはこれをどう行うかが重要です。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ9p5Z5_21CX"
      },
      "source": [
        "**自然言語処理により何ができるか**\n",
        "\n",
        "機械学習の入力や出力に自然言語のテキストを用いることでさまざまなことができます。入力も出力もテキストである例としては 機械翻訳 があげられ、実用化されています。入力は画像で出力がテキストである 画像キャプション生成 やその逆の文章からの画像生成も研究が進んでいます。\n",
        "\n",
        "\n",
        "しかし、出力をテキストや画像のような非構造化データとすることは難易度が高いです。比較的簡単にできることとしては、入力をテキスト、出力をカテゴリーとする テキスト分類 です。\n",
        "\n",
        "\n",
        "アヤメやタイタニック、手書き数字のような定番の存在として、IMDB映画レビューデータセット の感情分析があります。レビューの文書が映画に対して肯定的か否定的かを2値分類します。文書ごとの肯定・否定はラベルが与えられています。このSprintではこれを使っていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwEgfeah28SE"
      },
      "source": [
        "# 3.IMDB映画レビューデータセットの準備\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tejaaBaa3Csi"
      },
      "source": [
        "IMDB映画レビューデータセットを準備します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43Bm5Xt3MNq"
      },
      "source": [
        "**ダウンロード**\n",
        "\n",
        "次のwgetコマンドによってダウンロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OOshFBu0YK42",
        "outputId": "118be5d5-ca63-47bd-8fb3-d3bc3e01f1ae"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpwtvN3ZYRKf",
        "outputId": "ad72f33d-ca9b-4a90-ffdd-49f97ca96a9a"
      },
      "source": [
        "%cd drive/My Drive/Colab Notebooks/NLP"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Q6e3i00YTlL",
        "outputId": "fb98c0f9-25c0-419b-df9b-991cfd35dcef"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34maclImdb\u001b[0m/           aclImdb_v1.tar.gz.1  Sprint21.ipynb\n",
            "aclImdb_v1.tar.gz  aclImdb_v1.tar.gz.2  自然言語処理.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hjvoryLgkjD",
        "outputId": "f447bc8c-1e8e-4123-8c6e-cbfc89b7ad7d"
      },
      "source": [
        "!wget　--trust-server-names \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: wget　--trust-server-names: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zin_0-vqp7KQ",
        "outputId": "434928d9-3a53-47f9-c456-7312a654ed59"
      },
      "source": [
        "# IMDBをカレントフォルダにダウンロード\n",
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# 解凍\n",
        "!tar zxf aclImdb_v1.tar.gz\n",
        "# aclImdb/train/unsupはラベル無しのため削除\n",
        "!rm -rf aclImdb/train/unsup\n",
        "# IMDBデータセットの説明を表示\n",
        "!cat aclImdb/README"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-28 14:28:51--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  53.8MB/s    in 1.5s    \n",
            "\n",
            "2021-10-28 14:28:52 (53.8 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n",
            "Large Movie Review Dataset v1.0\n",
            "\n",
            "Overview\n",
            "\n",
            "This dataset contains movie reviews along with their associated binary\n",
            "sentiment polarity labels. It is intended to serve as a benchmark for\n",
            "sentiment classification. This document outlines how the dataset was\n",
            "gathered, and how to use the files provided. \n",
            "\n",
            "Dataset \n",
            "\n",
            "The core dataset contains 50,000 reviews split evenly into 25k train\n",
            "and 25k test sets. The overall distribution of labels is balanced (25k\n",
            "pos and 25k neg). We also include an additional 50,000 unlabeled\n",
            "documents for unsupervised learning. \n",
            "\n",
            "In the entire collection, no more than 30 reviews are allowed for any\n",
            "given movie because reviews for the same movie tend to have correlated\n",
            "ratings. Further, the train and test sets contain a disjoint set of\n",
            "movies, so no significant performance is obtained by memorizing\n",
            "movie-unique terms and their associated with observed labels.  In the\n",
            "labeled train/test sets, a negative review has a score <= 4 out of 10,\n",
            "and a positive review has a score >= 7 out of 10. Thus reviews with\n",
            "more neutral ratings are not included in the train/test sets. In the\n",
            "unsupervised set, reviews of any rating are included and there are an\n",
            "even number of reviews > 5 and <= 5.\n",
            "\n",
            "Files\n",
            "\n",
            "There are two top-level directories [train/, test/] corresponding to\n",
            "the training and test sets. Each contains [pos/, neg/] directories for\n",
            "the reviews with binary labels positive and negative. Within these\n",
            "directories, reviews are stored in text files named following the\n",
            "convention [[id]_[rating].txt] where [id] is a unique id and [rating] is\n",
            "the star rating for that review on a 1-10 scale. For example, the file\n",
            "[test/pos/200_8.txt] is the text for a positive-labeled test set\n",
            "example with unique id 200 and star rating 8/10 from IMDb. The\n",
            "[train/unsup/] directory has 0 for all ratings because the ratings are\n",
            "omitted for this portion of the dataset.\n",
            "\n",
            "We also include the IMDb URLs for each review in a separate\n",
            "[urls_[pos, neg, unsup].txt] file. A review with unique id 200 will\n",
            "have its URL on line 200 of this file. Due the ever-changing IMDb, we\n",
            "are unable to link directly to the review, but only to the movie's\n",
            "review page.\n",
            "\n",
            "In addition to the review text files, we include already-tokenized bag\n",
            "of words (BoW) features that were used in our experiments. These \n",
            "are stored in .feat files in the train/test directories. Each .feat\n",
            "file is in LIBSVM format, an ascii sparse-vector format for labeled\n",
            "data.  The feature indices in these files start from 0, and the text\n",
            "tokens corresponding to a feature index is found in [imdb.vocab]. So a\n",
            "line with 0:7 in a .feat file means the first word in [imdb.vocab]\n",
            "(the) appears 7 times in that review.\n",
            "\n",
            "LIBSVM page for details on .feat file format:\n",
            "http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n",
            "\n",
            "We also include [imdbEr.txt] which contains the expected rating for\n",
            "each token in [imdb.vocab] as computed by (Potts, 2011). The expected\n",
            "rating is a good way to get a sense for the average polarity of a word\n",
            "in the dataset.\n",
            "\n",
            "Citing the dataset\n",
            "\n",
            "When using this dataset please cite our ACL 2011 paper which\n",
            "introduces it. This paper also contains classification results which\n",
            "you may want to compare against.\n",
            "\n",
            "\n",
            "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
            "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
            "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
            "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
            "  month     = {June},\n",
            "  year      = {2011},\n",
            "  address   = {Portland, Oregon, USA},\n",
            "  publisher = {Association for Computational Linguistics},\n",
            "  pages     = {142--150},\n",
            "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
            "}\n",
            "\n",
            "References\n",
            "\n",
            "Potts, Christopher. 2011. On the negativity of negation. In Nan Li and\n",
            "David Lutz, eds., Proceedings of Semantics and Linguistic Theory 20,\n",
            "636-659.\n",
            "\n",
            "Contact\n",
            "\n",
            "For questions/comments/corrections please contact Andrew Maas\n",
            "amaas@cs.stanford.edu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLGex2Z53Zhw"
      },
      "source": [
        "以下のサイトで公開されているデータセットです。\n",
        "\n",
        "\n",
        "Sentiment Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqs2AbPZ3cf_"
      },
      "source": [
        "読み込み\n",
        "\n",
        "scikit-learnのload_filesを用いて読み込みます。\n",
        "\n",
        "\n",
        "sklearn.datasets.load_files — scikit-learn 0.21.3 documentation\n",
        "\n",
        "《読み込むコード》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70xug4Pw3YDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1ab345-d1aa-4a37-820d-7318307ddef1"
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "train_review = load_files('./aclImdb/train/', encoding='utf-8')\n",
        "x_train, y_train = train_review.data, train_review.target\n",
        "test_review = load_files('./aclImdb/test/', encoding='utf-8')\n",
        "x_test, y_test = test_review.data, test_review.target\n",
        "# ラベルの0,1と意味の対応の表示\n",
        "print(train_review.target_names)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg', 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7Tyyq4c3oGi"
      },
      "source": [
        "このデータセットについて\n",
        "\n",
        "中身を見てみると、英語の文章が入っていることが分かります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkJzRZm03wOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a7d613-e4fd-4a4d-e905-d851c04c6437"
      },
      "source": [
        "print(\"x : {}\".format(x_train[0]))\n",
        "print(np.array(x_train).shape, np.array(x_test).shape, np.array(y_train).shape, np.array(y_test).shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x : Zero Day leads you to think, even re-think why two boys/young men would do what they did - commit mutual suicide via slaughtering their classmates. It captures what must be beyond a bizarre mode of being for two humans who have decided to withdraw from common civility in order to define their own/mutual world via coupled destruction.<br /><br />It is not a perfect movie but given what money/time the filmmaker and actors had - it is a remarkable product. In terms of explaining the motives and actions of the two young suicide/murderers it is better than 'Elephant' - in terms of being a film that gets under our 'rationalistic' skin it is a far, far better film than almost anything you are likely to see. <br /><br />Flawed but honest with a terrible honesty.\n",
            "(25000,) (25000,) (25000,) (25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifw_kko731ry"
      },
      "source": [
        "IMDBはInternet Movie Databaseの略で、映画のデータベースサイトです。\n",
        "\n",
        "\n",
        "Ratings and Reviews for New Movies and TV Shows - IMDb\n",
        "\n",
        "\n",
        "このサイトではユーザが映画に対して1から10点の評価とコメントを投稿することができます。そのデータベースから訓練データは25000件、テストデータは25000件のデータセットを作成しています。\n",
        "\n",
        "\n",
        "4点以下を否定的、7点以下を肯定的なレビューとして2値のラベル付けしており、これにより感情の分類を行います。5,6点の中立的なレビューはデータセットに含んでいません。また、ラベルは訓練用・テスト用それぞれで均一に入っています。詳細はダウンロードしたREADMEを確認してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwVqYZ9r34D2"
      },
      "source": [
        "# 4.古典的な手法\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2sbwLK437xg"
      },
      "source": [
        "古典的ながら現在でも強力な手法であるBoWとTF-IDFを見ていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsyKvYL03-99"
      },
      "source": [
        "# 5.BoW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCKZ7hAW4CPX"
      },
      "source": [
        "単純ながら効果的な方法として BoW (Bag of Words) があります。これは、サンプルごとに単語などの 登場回数 を数えたものをベクトルとする方法です。単語をカテゴリとして捉え one-hot表現 していることになります。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8Nq8d3c4Fmh"
      },
      "source": [
        "例\n",
        "\n",
        "例として、IMDBデータセットからある3文の最初の5単語を抜き出したものを用意しました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLQo3AVHD-ys"
      },
      "source": [
        "mini_dataset = [\"This movie is very good.\", \"This film is a good\", \"Very bad. Very, very bad.\"]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tBYbthj4R7v"
      },
      "source": [
        "この3文にBoWを適用させてみます。scikit-learnのCountVectorizerを利用します。\n",
        "\n",
        "\n",
        "sklearn.feature_extraction.text.CountVectorizer — scikit-learn 0.21.3 documentation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "_f2V5kc1BrU_",
        "outputId": "87c0f8e0-a366-4845-d6a6-0e743c79bc15"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "# DataFrameにまとめる\n",
        "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>bad</th>\n",
              "      <th>film</th>\n",
              "      <th>good</th>\n",
              "      <th>is</th>\n",
              "      <th>movie</th>\n",
              "      <th>this</th>\n",
              "      <th>very</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a  bad  film  good  is  movie  this  very\n",
              "0  0    0     0     1   1      1     1     1\n",
              "1  1    0     1     1   1      0     1     0\n",
              "2  0    2     0     0   0      0     0     3"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN6NeAqu4Zrx"
      },
      "source": [
        "実行すると次のような表が得られます。\n",
        "\n",
        "https://t.gyazo.com/teams/diveintocode/cb8a221501808a5f8cd95225183bbb61.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXGiARxY4luw"
      },
      "source": [
        "例にあげた3文の中で登場する8種類の単語が列名になり、0,1,2番目のサンプルでそれらが何回登場しているかを示しています。2番目のサンプル「Very bad. Very, very bad.」ではbadが2回、veryが3回登場しています。列名になっている言葉はデータセットが持つ 語彙 と呼びます。\n",
        "\n",
        "\n",
        "テキストはBoWにより各サンプルが語彙数の次元を持つ特徴量となり、機械学習モデルへ入力できるようになります。この時使用したテキスト全体のことを コーパス と呼びます。語彙はコーパスに含まれる言葉よって決まり、それを特徴量としてモデルの学習を行います。そのため、テストデータではじめて登場する語彙はベクトル化される際に無視されます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2m7MVlv4pJ9"
      },
      "source": [
        "**前処理**\n",
        "\n",
        "CountVectorizerクラスでは大文字は小文字に揃えるという 前処理 が自動的に行われています。こういった前処理は自然言語処理において大切で、不要な記号などの消去（テキストクリーニング）や表記揺れの統一といったことを別途行うことが一般的です。\n",
        "\n",
        "\n",
        "語形が「see」「saw」「seen」のように変化する単語に対して語幹に揃える ステミング と呼ばれる処理を行うこともあります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MdZ26dZ4uDd"
      },
      "source": [
        "**トークン**\n",
        "\n",
        "BoWは厳密には単語を数えているのではなく、 トークン（token） として定めた固まりを数えます。\n",
        "\n",
        "\n",
        "何をトークンとするかはCountVectorizerでは引数token_patternで 正規表現 の記法により指定されます。デフォルトはr'(?u)\\b\\w\\w+\\b'ですが、上の例ではr'(?u)\\b\\w+\\b'としています。\n",
        "\n",
        "\n",
        "デフォルトでは空白・句読点・スラッシュなどに囲まれた2文字以上の文字を1つのトークンとして抜き出すようになっているため、「a」や「I」などがカウントされません。英語では1文字の単語は文章の特徴をあまり表さないため、除外されることもあります。しかし、上の例では1文字の単語もトークンとして抜き出すように引数を指定しています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN96FWbn416-"
      },
      "source": [
        "**《正規表現》**\n",
        "\n",
        "\n",
        "正規表現は前処理の際にも活用しますが、ここでは詳細は扱いません。Pythonではreモジュールによって正規表現操作ができます。\n",
        "\n",
        "\n",
        "re — 正規表現操作\n",
        "\n",
        "\n",
        "正規表現を利用する際はリアルタイムで結果を確認できる以下のようなサービスが便利です。\n",
        "\n",
        "\n",
        "Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZcm_ZYt47bu"
      },
      "source": [
        "**形態素解析**\n",
        "\n",
        "英語などの多くの言語では空白という分かりやすい基準でトークン化が行えますが、日本語ではそれが行えません。\n",
        "\n",
        "\n",
        "日本語では名詞や助詞、動詞のように異なる 品詞 で分けられる単位で 分かち書き することになります。例えば「私はプログラミングを学びます」という日本語の文は「私/は/プログラミング/を/学び/ます」という風になります。\n",
        "\n",
        "\n",
        "これには MeCab や Janome のような形態素解析ツールを用います。Pythonから利用することも可能です。MeCabをウェブ上で簡単に利用できるWeb茶まめというサービスも国立国語研究所が提供しています。\n",
        "\n",
        "\n",
        "自然言語では新しい言葉も日々生まれますので、それにどれだけ対応できるかも大切です。MeCab用の毎週更新される辞書として mecab-ipadic-NEologd がオープンソースで存在しています。\n",
        "\n",
        "\n",
        "mecab-ipadic-neologd/README.ja.md at master · neologd/mecab-ipadic-neologd\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC0mqsLQ5COx"
      },
      "source": [
        "**n-gram**\n",
        "\n",
        "上のBoWの例では1つの単語（トークン）毎の登場回数を数えましたが、これでは語順はまったく考慮されていません。\n",
        "\n",
        "\n",
        "考慮するために、隣あう単語同士をまとめて扱う n-gram という考え方を適用することがあります。2つの単語をまとめる場合は 2-gram (bigram) と呼び、次のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nomqZpqS5HIR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "9f69b212-31b1-4bf0-d8c3-e94412fe0b72"
      },
      "source": [
        "# ngram_rangeで利用するn-gramの範囲を指定する\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), token_pattern=r'(?u)\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a good</th>\n",
              "      <th>bad very</th>\n",
              "      <th>film is</th>\n",
              "      <th>is a</th>\n",
              "      <th>is very</th>\n",
              "      <th>movie is</th>\n",
              "      <th>this film</th>\n",
              "      <th>this movie</th>\n",
              "      <th>very bad</th>\n",
              "      <th>very good</th>\n",
              "      <th>very very</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a good  bad very  film is  is a  ...  this movie  very bad  very good  very very\n",
              "0       0         0        0     0  ...           1         0          1          0\n",
              "1       1         0        1     1  ...           0         0          0          0\n",
              "2       0         1        0     0  ...           0         2          0          1\n",
              "\n",
              "[3 rows x 11 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gRgiWPaVNTT"
      },
      "source": [
        "https://diveintocode.gyazo.com/64e20e72c04543743bd52576ec8f9380"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrlCyqXs4fIW"
      },
      "source": [
        "**【問題1】BoWのスクラッチ実装**\n",
        "\n",
        "以下の3文のBoWを求められるプログラムをscikit-learnを使わずに作成してください。1-gramと2-gramで計算してください。\n",
        "\n",
        "This movie is SOOOO funny!!!\n",
        "\n",
        "What a movie! I never\n",
        "\n",
        "best movie ever!!!!! this movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "0HEEZXAtVn9o",
        "outputId": "9de14434-b3d2-4511-b46e-f335bff95a73"
      },
      "source": [
        "# sklearnの場合\n",
        "# Diverから与えられたデータ\n",
        "mini_dataset = ['This movie is SOOOO funny!!!', 'What a movie! I never', 'best movie ever!!!!! this movie']\n",
        "\n",
        "# インスタンス化\n",
        "vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
        "# 実行\n",
        "bow = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "# Dataframe化(見やすくするために)\n",
        "df = pd.DataFrame(bow, columns=vectorizer.get_feature_names())\n",
        "# 出力\n",
        "display(df)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>best</th>\n",
              "      <th>ever</th>\n",
              "      <th>funny</th>\n",
              "      <th>i</th>\n",
              "      <th>is</th>\n",
              "      <th>movie</th>\n",
              "      <th>never</th>\n",
              "      <th>soooo</th>\n",
              "      <th>this</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a  best  ever  funny  i  is  movie  never  soooo  this  what\n",
              "0  0     0     0      1  0   1      1      0      1     1     0\n",
              "1  1     0     0      0  1   0      1      1      0     0     1\n",
              "2  0     1     1      0  0   0      2      0      0     1     0"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "mCQ1Qf8sI1dk",
        "outputId": "f970d4e6-7ab4-4cfb-a861-bf4818152f6d"
      },
      "source": [
        "# スクラッチ実装の場合\n",
        "def bow(data):\n",
        "  \"\"\"\n",
        "  BoW算出\n",
        "  Parameters\n",
        "  ------------\n",
        "  \"\"\"\n",
        "  # 単語リスト作成(小文字に統一、!を除去、半角スペースを基準に文字列を分割、リスト化)\n",
        "  row_data = [i.lower().replace('!', '').split(' ') for i in data]\n",
        "  # 1次元のリストに(単語リスト)\n",
        "  feature_names = set(list(itertools.chain.from_iterable(row_data)))\n",
        "\n",
        "  # bow計算\n",
        "  bow = []\n",
        "  # 1つずつ文章でループ\n",
        "  for index, row in enumerate(data):\n",
        "    bow.append([])\n",
        "    # 単語リストでループ\n",
        "    for feature_name in feature_names:\n",
        "      # 何個含まれているか\n",
        "      num = row_data[index].count(feature_name)\n",
        "      # 追加\n",
        "      bow[index].append(num)\n",
        "  return feature_names, bow\n",
        "\n",
        "# 仮データの定義\n",
        "mini_dataset = ['This movie is SOOOO funny!!!', 'What a movie! I never', 'best movie ever!!!!! this movie']\n",
        "# bowメソッドの実行\n",
        "feature_names,bow = bow(mini_dataset)\n",
        "# Dataframe化(見やすくするために)\n",
        "df = pd.DataFrame(bow, columns=feature_names)\n",
        "# 出力\n",
        "display(df)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>what</th>\n",
              "      <th>i</th>\n",
              "      <th>soooo</th>\n",
              "      <th>a</th>\n",
              "      <th>this</th>\n",
              "      <th>never</th>\n",
              "      <th>movie</th>\n",
              "      <th>ever</th>\n",
              "      <th>best</th>\n",
              "      <th>is</th>\n",
              "      <th>funny</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   what  i  soooo  a  this  never  movie  ever  best  is  funny\n",
              "0     0  0      1  0     1      0      1     0     0   1      1\n",
              "1     1  1      0  1     0      1      1     0     0   0      0\n",
              "2     0  0      0  0     1      0      2     1     1   0      0"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00xS2uNY3vOJ"
      },
      "source": [
        "# 6.TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51uu06q7VueW"
      },
      "source": [
        "BoWの発展的手法として TF-IDF もよく使われます。これは Term Frequency (TF) と Inverse Document Frequency (IDF) という2つの指標の組み合わせです。\n",
        "\n",
        "\n",
        "《標準的なTF-IDFの式》\n",
        "\n",
        "\n",
        "Term Frequency:　　\n",
        "$$\n",
        "tf(t,d) = \\frac{n_{t,d}}{\\sum_{s \\in d}n_{s,d}}\n",
        "$$\n",
        "\n",
        "$n_{t,d}$ : サンプルd内のトークンtの出現回数（BoWと同じ）\n",
        "\n",
        "\n",
        "$\\sum_{s \\in d}n_{s,d}$ : サンプルdの全トークンの出現回数の和\n",
        "\n",
        "\n",
        "Inverse Document Frequency:\n",
        "\n",
        "$$\n",
        "idf(t) = \\log{\\frac{N}{df(t)}}\n",
        "$$\n",
        "\n",
        "$N$ : サンプル数\n",
        "\n",
        "\n",
        "$df(t)$ : トークンtが出現するサンプル数\n",
        "\n",
        "\n",
        "＊logの底は任意の値\n",
        "\n",
        "\n",
        "TF-IDF:\n",
        "\n",
        "$$\n",
        "tfidf(t, d) = tf(t, d) \\times idf(t)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R4EOn9qXraK"
      },
      "source": [
        "IDF\n",
        "\n",
        "IDFはそのトークンがデータセット内で珍しいほど値が大きくなる指標です。\n",
        "\n",
        "\n",
        "サンプル数 $N$ をIMDB映画レビューデータセットの訓練データに合わせ25000として、トークンが出現するサンプル数 $df(t)$ を変化させたグラフを確認してみると、次のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "bPmNe63VX14T",
        "outputId": "cb07445e-cd3f-45b4-f001-8b5baac633a8"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "n_samples = 25000\n",
        "idf = np.log(n_samples/np.arange(1,n_samples))\n",
        "plt.title(\"IDF\")\n",
        "plt.xlabel(\"df(t)\")\n",
        "plt.ylabel(\"IDF\")\n",
        "plt.plot(idf)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrrkOMOtYf2f"
      },
      "source": [
        "https://diveintocode.gyazo.com/2a1868fa9e70deb138114819f5f20348"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OqQ07EmYoMV"
      },
      "source": [
        "TF-IDFではこの数を出現回数に掛け合わせるので、珍しいトークンの登場に重み付けを行なっていることになります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ka_9kb4Yr72"
      },
      "source": [
        "**ストップワード**\n",
        "\n",
        "あまりにも頻繁に登場するトークンは、値を小さくするだけでなく、取り除くという前処理を加えることもあります。取り除くもののことを ストップワード と呼びます。既存のストップワード一覧を利用したり、しきい値によって求めたりします。\n",
        "\n",
        "scikit-learnのCountVectorizerでは引数stop_wordsにリストで指定することで処理を行なってくれます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ZMmG7vy0Y2XM",
        "outputId": "696bf0c2-3237-4d16-e3d6-242db3b7981d"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=[\"is\"], token_pattern=r'\\b\\w+\\b')\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>best</th>\n",
              "      <th>ever</th>\n",
              "      <th>funny</th>\n",
              "      <th>i</th>\n",
              "      <th>movie</th>\n",
              "      <th>never</th>\n",
              "      <th>soooo</th>\n",
              "      <th>this</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a  best  ever  funny  i  movie  never  soooo  this  what\n",
              "0  0     0     0      1  0      1      0      1     1     0\n",
              "1  1     0     0      0  1      1      1      0     0     1\n",
              "2  0     1     1      0  0      2      0      0     1     0"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqEXFjNaY9Rq"
      },
      "source": [
        "代表的な既存のストップワード一覧としては、NLTK という自然言語処理のライブラリのものがあげられます。あるデータセットにおいては特別重要な意味を持つ単語が一覧に含まれている可能性もあるため、使用する際は中身を確認することが望ましいです。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Teukes6_ZGew",
        "outputId": "74bf170d-1c81-4a94-b2e4-5f053b328aa8"
      },
      "source": [
        "# はじめて使う場合はストップワードをダウンロード(DIVERのコード)\n",
        "import nltk\n",
        "stop_words = nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "print(\"stop word : {}\".format(stop_words)) # 'i', 'me', 'my', ..."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4zWh5vwbLWs"
      },
      "source": [
        "逆に、登場回数が特に少ないトークンも取り除くことが多いです。すべてのトークンを用いるとベクトルの次元数が著しく大きくなってしまい計算コストが高まるためです。\n",
        "\n",
        "\n",
        "scikit-learnのCountVectorizerでは引数max_featuresに最大の語彙数を指定することで処理を行なってくれます。以下の例では出現数が多い順に5個でベクトル化しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "RFObXXCvbWZ4",
        "outputId": "fe193bd1-3666-4aa4-d733-5a5212e23183"
      },
      "source": [
        "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features = 5)\n",
        "bow_train = (vectorizer.fit_transform(mini_dataset)).toarray()\n",
        "df = pd.DataFrame(bow_train, columns=vectorizer.get_feature_names())\n",
        "display(df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>best</th>\n",
              "      <th>ever</th>\n",
              "      <th>movie</th>\n",
              "      <th>this</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   a  best  ever  movie  this\n",
              "0  0     0     0      1     1\n",
              "1  1     0     0      1     0\n",
              "2  0     1     1      2     1"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlaeI12LbcMR"
      },
      "source": [
        "**【問題2】TF-IDFの計算**\n",
        "\n",
        "IMDB映画レビューデータセットをTF-IDFによりベクトル化してください。NLTKのストップワードを利用し、最大の語彙数は5000程度に設定してください。テキストクリーニングやステミングなどの前処理はこの問題では要求しません。\n",
        "\n",
        "\n",
        "TF-IDFの計算にはscikit-learnの以下のどちらかのクラスを使用してください。\n",
        "\n",
        "\n",
        "sklearn.feature_extraction.text.TfidfVectorizer — scikit-learn 0.21.3 documentation\n",
        "sklearn.feature_extraction.text.TfidfTransformer — scikit-learn 0.21.3 documentation\n",
        "\n",
        "\n",
        "なお、scikit-learnでは標準的な式とは異なる式が採用されています。\n",
        "\n",
        "\n",
        "また、デフォルトではnorm=\"l2\"の引数が設定されており、各サンプルにL2正規化が行われます。norm=Noneとすることで正規化は行われなくなります。\n",
        "\n",
        "\n",
        "Term Frequency:\n",
        "\n",
        "$$\n",
        "tf(t,d) = n_{t,d}\n",
        "$$\n",
        "\n",
        "$n_{t,d}$ : サンプルd内のトークンtの出現回数\n",
        "\n",
        "\n",
        "scikit-learnのTFは分母がなくなりBoWと同じ計算になります。\n",
        "\n",
        "\n",
        "Inverse Document Frequency:\n",
        "\n",
        "$$\n",
        "idf(t) = \\log{\\frac{1+N}{1+df(t)}}+1\n",
        "$$\n",
        "\n",
        "$N$ : サンプル数\n",
        "\n",
        "\n",
        "$df(t)$ : トークンtが出現するサンプル数\n",
        "\n",
        "\n",
        "＊logの底はネイピア数e\n",
        "\n",
        "\n",
        "詳細は以下のドキュメントを確認してください。\n",
        "\n",
        "\n",
        "5.2.3.4. Tf–idf term weighting — scikit-learn 0.21.3 documentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeNoSLxN0egP",
        "outputId": "aaff36eb-4046-43a6-dfd1-589e015b93c3"
      },
      "source": [
        "# nltkライブラリのstopwordsを利用\n",
        "stop_words = nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "print('stop word : {}'.format(stop_words))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "stop word : ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H23JdOW28Oh1"
      },
      "source": [
        "# tfidfの算出\n",
        "vectorizer = TfidfVectorizer(stop_words = stop_words, max_features = 5000)\n",
        "X_train = vectorizer.fit_transform(x_train)\n",
        "X_test = vectorizer.fit_transform(x_test)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYvCfopDUGMN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ece225-5f10-4fe4-9fc5-65a5a53ad301"
      },
      "source": [
        "# テスト出力\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 5000) (25000, 5000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUpv5YDJdQjo"
      },
      "source": [
        "**【問題3】TF-IDFを用いた学習**\n",
        "\n",
        "問題2で求めたベクトルを用いてIMDB映画レビューデータセットの学習・推定を行なってください。モデルは2値分類が行える任意のものを利用してください。\n",
        "\n",
        "\n",
        "ここでは精度の高さは求めませんが、最大の語彙数やストップワード、n-gramの数を変化させて影響を検証してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oT4nTQfAVR2-"
      },
      "source": [
        "# lightGBMを用いた学習\n",
        "lgb = lgb.LGBMClassifier().fit(X_train, y_train)\n",
        "# 推定\n",
        "y_pred = lgb.predict(X_test)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMgs01mA9v4m",
        "outputId": "6b02abef-b2ab-4394-a87e-e9f4deb21a13"
      },
      "source": [
        "# 結果出力\n",
        "print('{}'.format(lgb.score(X_test, y_test)))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.57548\n",
            "[[7031 5469]\n",
            " [5144 7356]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rt2PHAnrdbww"
      },
      "source": [
        "**【問題4】TF-IDFのスクラッチ実装**\n",
        "\n",
        "以下の3文のTF-IDFを求められるプログラムをscikit-learnを使わずに作成してください。標準的な式と、scikit-learnの採用している式の2種類を作成してください。正規化は不要です。\n",
        "\n",
        "This movie is SOOOO funny!!!\n",
        "\n",
        "What a movie! I never\n",
        "\n",
        "best movie ever!!!!! this movie"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "ljN4KBykLQnY",
        "outputId": "42774cf9-87b2-4819-858c-237e69cf9c74"
      },
      "source": [
        "# sklearnの場合\n",
        "# 仮データの定義\n",
        "mini_dataset = ['This movie is SOOOO funny!!!', 'What a movie! I never', 'best movie ever!!!!! this movie']\n",
        "\n",
        "# インスタンス化\n",
        "tfidf_model = TfidfVectorizer()\n",
        "# 実行\n",
        "tfidf = tfidf_model.fit_transform(mini_dataset)\n",
        "# Dataframe化(見やすくするために)\n",
        "tfidf = pd.DataFrame(tfidf.toarray(), columns=tfidf_model.get_feature_names())\n",
        "# 出力\n",
        "display(tfidf)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>best</th>\n",
              "      <th>ever</th>\n",
              "      <th>funny</th>\n",
              "      <th>is</th>\n",
              "      <th>movie</th>\n",
              "      <th>never</th>\n",
              "      <th>soooo</th>\n",
              "      <th>this</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.504611</td>\n",
              "      <td>0.504611</td>\n",
              "      <td>0.298032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.504611</td>\n",
              "      <td>0.383770</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.385372</td>\n",
              "      <td>0.652491</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.652491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.501651</td>\n",
              "      <td>0.501651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.592567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.381519</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       best      ever     funny  ...     soooo      this      what\n",
              "0  0.000000  0.000000  0.504611  ...  0.504611  0.383770  0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.652491\n",
              "2  0.501651  0.501651  0.000000  ...  0.000000  0.381519  0.000000\n",
              "\n",
              "[3 rows x 9 columns]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPcjnT6QDn7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "f8d1a662-346b-4b52-8f97-54e41b9cada4"
      },
      "source": [
        "# インスタンス化\n",
        "cv_model = CountVectorizer()\n",
        "# 計算\n",
        "cv= cv_model.fit_transform(mini_dataset)\n",
        "# 扱いやすいように配列化\n",
        "cv_array = cv.toarray()\n",
        "\n",
        "# TF値計算\n",
        "N = cv_array.shape[0]\n",
        "tf = np.array([cv_array[i, :] / np.sum(cv_array, axis=1)[i] for i in range(N)])\n",
        "\n",
        "# IDF値計算\n",
        "df = np.count_nonzero(cv_array, axis=0)\n",
        "idf = np.log((1 + N) / (1 + df)) + 1\n",
        "\n",
        "# normalize\n",
        "tfidf = normalize(tf*idf)\n",
        "tfidf = pd.DataFrame(tfidf, columns=cv_model.get_feature_names())\n",
        "tfidf"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>best</th>\n",
              "      <th>ever</th>\n",
              "      <th>funny</th>\n",
              "      <th>is</th>\n",
              "      <th>movie</th>\n",
              "      <th>never</th>\n",
              "      <th>soooo</th>\n",
              "      <th>this</th>\n",
              "      <th>what</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.504611</td>\n",
              "      <td>0.504611</td>\n",
              "      <td>0.298032</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.504611</td>\n",
              "      <td>0.383770</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.385372</td>\n",
              "      <td>0.652491</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.652491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.501651</td>\n",
              "      <td>0.501651</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.592567</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.381519</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       best      ever     funny  ...     soooo      this      what\n",
              "0  0.000000  0.000000  0.504611  ...  0.504611  0.383770  0.000000\n",
              "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.652491\n",
              "2  0.501651  0.501651  0.000000  ...  0.000000  0.381519  0.000000\n",
              "\n",
              "[3 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHM2k0Ecd4pc"
      },
      "source": [
        "# 7.Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJSHWa4AeDIS"
      },
      "source": [
        "ニューラルネットワークを用いてベクトル化を行う手法が Word2Vec です。\n",
        "\n",
        "\n",
        "BoWやTF-IDFはone-hot表現であったため、得られるベクトルの次元は語彙数分になります。そのため、語彙数を増やしにくいという問題があります。一方で、Word2Vecでは単語を任意の次元のベクトルに変換します。これをを Word Embedding（単語埋め込み） や 分散表現 と呼びます。変換操作を「ベクトル空間に埋め込む」と言うことが多いです。\n",
        "\n",
        "\n",
        "Word2VecにはCBoWとSkip-gramという2種類の仕組みがあるため順番に見ていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88gvUKgZeMbE"
      },
      "source": [
        "**CBoW**\n",
        "\n",
        "CBoW (Continuous Bag-of-Words) によるWord2Vecではある単語とある単語の間に来る単語を推定できるように全結合層2層のニューラルネットワークを学習します。\n",
        "\n",
        "\n",
        "単語はコーパスの語彙数次元のone-hot表現を行なっておきます。そのため、入力と出力の次元は語彙数と同じになります。一方で、中間のノード数をWord2Vecにより得たい任意の次元数とします。これにより全結合層の重みは「得たい次元のノード数×語彙数」になります。このネットワークにより学習を行なった後、出力側の重みを取り出すことで、各語彙を表すベクトルを手に入れることができます。\n",
        "\n",
        "\n",
        "間の単語の推定を行なっているため、同じ箇所で代替可能な言葉は似たベクトルになるというメリットもあります。これはBoWやTF-IDFでは得られない情報です。\n",
        "\n",
        "\n",
        "あるテキストは「そのテキストの長さ（単語数）×Word2Vecで得た分散表現の次元数」の配列になりますが、各入力の配列を揃える必要があるモデルに入力するためには、短いテキストは空白を表す単語を加える パディング を行なったり、長いテキストは単語を消したりします。テキストを 固定長 にすると呼びます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jZIz6ace6Zj"
      },
      "source": [
        "**ウィンドウサイズ**\n",
        "\n",
        "入力する単語は推定する前後1つずつだけでなく、複数個とする場合もあります。前後いくつを見るかの大きさを ウィンドウサイズ と呼びます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH4pgqLWfDzf"
      },
      "source": [
        "**Skip-gram**\n",
        "\n",
        "CBoWとは逆にある単語の前後の単語を推定できるように全結合層2層のニューラルネットワークを学習する方法が Skip-gram です。学習を行なった後は入力側の重みを取り出し各語彙を表すベクトルとします。現在一般的に使われているのはCBoWよりもSki-gramです。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BF2kSgAf2_2"
      },
      "source": [
        "利用方法\n",
        "\n",
        "Pythonでは Gensim ライブラリを用いて扱うことができます。\n",
        "\n",
        "\n",
        "gensim: models.word2vec – Word2vec embeddings\n",
        "\n",
        "\n",
        "BoWの例と同じ文章で学習してみます。CountVectorizerと異なり前処理を自動的に行なってはくれないため、単語（トークン）はリストで分割しておきます。また、大文字は小文字に揃え、記号は取り除きます。\n",
        "\n",
        "\n",
        "デフォルトのパラメータではCBoWで計算されます。また、ウィンドウサイズはwindow=5に設定されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0HVK3nZgBto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "832a2655-fc2c-488b-8072-496b14328810"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['this', 'movie', 'is', 'very', 'good'], ['this', 'film', 'is', 'a', 'good'], ['very', 'bad', 'very', 'very', 'bad']]\n",
        "model = Word2Vec(min_count=1, size=10) # 次元数を10に設定\n",
        "model.build_vocab(sentences) # 準備\n",
        "model.train(sentences, total_examples=model.corpus_count, epochs=model.iter) # 学習\n",
        "\n",
        "print(\"語彙の一覧 : {}\".format(model.wv.vocab.keys()))\n",
        "for vocab in model.wv.vocab.keys():\n",
        "  print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "語彙の一覧 : dict_keys(['this', 'movie', 'is', 'very', 'good', 'film', 'a', 'bad'])\n",
            "thisのベクトル : \n",
            "[ 0.02448659 -0.02258527 -0.03638428 -0.03728524 -0.0386382  -0.03442909\n",
            " -0.00339081  0.00506924 -0.02983295  0.02718567]\n",
            "movieのベクトル : \n",
            "[-0.01975352 -0.04974889 -0.00021662  0.03743569  0.03818025  0.03479855\n",
            "  0.04919528 -0.01532895 -0.02194545 -0.00446168]\n",
            "isのベクトル : \n",
            "[-0.01656016  0.04415523  0.04040301 -0.02953821 -0.00049854  0.01733456\n",
            " -0.04991534 -0.00642089 -0.00103154 -0.02883399]\n",
            "veryのベクトル : \n",
            "[ 0.01956089  0.01937082  0.00011196  0.00065178 -0.02691839 -0.0118428\n",
            "  0.02865055  0.02500338 -0.04254784  0.01628557]\n",
            "goodのベクトル : \n",
            "[ 0.04437617  0.00714008 -0.0291023  -0.0006632   0.00911255 -0.03991409\n",
            " -0.04172549 -0.02125249 -0.03785334  0.02045375]\n",
            "filmのベクトル : \n",
            "[ 0.00093078 -0.04159413 -0.00137179  0.0129107   0.00324553 -0.01712264\n",
            " -0.011067    0.04361027  0.04210928 -0.01672216]\n",
            "aのベクトル : \n",
            "[-0.01053376 -0.01615691 -0.00260882  0.03117631  0.01185322 -0.04794431\n",
            "  0.03275092 -0.00160121  0.02441066  0.02921974]\n",
            "badのベクトル : \n",
            "[ 0.04488649  0.03139098 -0.02885148  0.03962441  0.01630573 -0.03272609\n",
            " -0.0080754  -0.02298408 -0.0469444   0.04226529]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOIt66iHgDDO"
      },
      "source": [
        "このようにしてベクトルが得られます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H1arrB7gH6v"
      },
      "source": [
        "**単語の距離**\n",
        "\n",
        "ベクトル間で計算を行うことで、ある単語に似たベクトルを持つ単語を見つけることができます。例えばgoodに似たベクトルの単語を3つ探します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3jM8Ff_gNO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6d2ee6-029f-4f50-f941-c2fdffed3155"
      },
      "source": [
        "model.wv.most_similar(positive=\"good\", topn=3)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bad', 0.8063161969184875),\n",
              " ('this', 0.5671584606170654),\n",
              " ('very', 0.2176794707775116)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kenpa9otgQAi"
      },
      "source": [
        "今の例では3文しか学習していませんので効果を発揮しませんが、大きなコーパスで学習することで、並列関係のものが近くに来たりなど面白い結果が得られます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn-94A3_ghLM"
      },
      "source": [
        "**可視化**\n",
        "\n",
        "2次元に圧縮することで単語ごとの位置関係を可視化することができます。以下はt-SNEを用いた例です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDcHEt6YgWNY"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "vocabs = model.wv.vocab.keys()\n",
        "tsne_model = TSNE(perplexity=40, n_components=2, init=\"pca\", n_iter=5000, random_state=23)\n",
        "vectors_tsne = tsne_model.fit_transform(model[vocabs])\n",
        "fig, ax = plt.subplots(figsize=(5,5))\n",
        "ax.scatter(vectors_tsne[:, 0], vectors_tsne[:, 1])\n",
        "for i, word in enumerate(list(vocabs)):\n",
        "    plt.annotate(word, xy=(vectors_tsne[i, 0], vectors_tsne[i, 1]))\n",
        "ax.set_yticklabels([])\n",
        "ax.set_xticklabels([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap6tukfwgVDE"
      },
      "source": [
        "# 8.IMDB映画レビューデータセットの分散表現\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm5EayULUSLC"
      },
      "source": [
        "IMDB映画レビューデータセットの訓練データをコーパスとしてWord2Vecを学習させ分散表現を獲得しましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIL3Qs3jhlzu"
      },
      "source": [
        "**【問題5】コーパスの前処理**\n",
        "\n",
        "コーパスの前処理として、特殊文字（!など）やURLの除去、大文字の小文字化といったことを行なってください。また、単語（トークン）はリストで分割してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjhTZFHsUT57",
        "outputId": "4e751a97-cf95-4756-970e-c563e5ad360b"
      },
      "source": [
        "# 簡単のため、URL含んでそうな1文抜き出す\n",
        "with_url = 0\n",
        "for i, s in enumerate(x_train):\n",
        "    if 'www' in s:\n",
        "        with_url = i\n",
        "        print('-----before processing')\n",
        "        print(s)\n",
        "        break\n",
        "no_preprocessing = x_train[with_url]\n",
        "\n",
        "# urlは除外\n",
        "after_preprocessing1 = re.sub(r'https?://[\\w/:%#\\$&\\?\\(\\)~\\.=\\+\\-…]+', \"\", no_preprocessing) \n",
        "# タグ除去\n",
        "after_preprocessing2 = re.sub(r'<[^>]+>', \" \", after_preprocessing1) \n",
        "# 数字と英字以外除去\n",
        "after_preprocessing3 = re.sub(r\"[^0-9a-zA-Z ]\", \"\", after_preprocessing2) \n",
        "# 小文字に統一\n",
        "after_preprocessing = after_preprocessing3.lower() \n",
        "\n",
        "print('-----after processing')\n",
        "print(after_preprocessing)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----before processing\n",
            "I don't hand out \"ones\" often, but if there was ever a film that deserved this sort of attention, it's \"Gas!\" This is self-indulgent crap that reaches for some of the ambiance of M*A*S*H and falls completely flat on its face in the attempt.<br /><br />I see what Corman was going for - Malcolm Marmorstein and Elliott Gould tried to reproduce Gould's deathless role in the original movie version of M*A*S*H with a similar plot (in the movie \"Whiffs\" - look it up here in IMDb, http://www.imdb.com/title/tt0073891/ for more information).<br /><br />Marmorstein and Gould got closer to the brass ring with \"Whiffs\" than Corman did with \"Gas!\" but didn't quite get there. Neither one of those films even got close to the success of M*A*S*H.<br /><br />What's wrong with \"Gas!\"? What isn't? No one comes close to really acting at a level above junior high school theatrics. The production values stink. Someone else here mentioned the magically regenerating headlights on a getaway car, and there's more of that lack of attention to detail. Nothing works the way it's supposed to in this film, and nobody cares.<br /><br />\"Gas!\" actually put me to sleep. It's not a sure cure for insomnia, but really close. On the Cinematic Sleep Induction scale, \"Gas!\" falls somewhere between \"Last Year at Marienbad\" and George Clooney's remake of \"Solaris\" (which itself was remarkable for being more boring than the Mosfilm original, despite that studio's seeming unfamiliarity with the idea of keeping the audience's attention by judicious editing).<br /><br />Judicious editing would have decimated \"Gas!\" to about twenty minutes. The result would be pointless, but no more so than the original film.<br /><br />Certain films are so bad that they have a compelling quality that makes them worth watching anyway. This isn't one of them. Don't waste your time. It's not even amusingly bad.\n",
            "-----after processing\n",
            "i dont hand out ones often but if there was ever a film that deserved this sort of attention its gas this is selfindulgent crap that reaches for some of the ambiance of mash and falls completely flat on its face in the attempt  i see what corman was going for  malcolm marmorstein and elliott gould tried to reproduce goulds deathless role in the original movie version of mash with a similar plot in the movie whiffs  look it up here in imdb  for more information  marmorstein and gould got closer to the brass ring with whiffs than corman did with gas but didnt quite get there neither one of those films even got close to the success of mash  whats wrong with gas what isnt no one comes close to really acting at a level above junior high school theatrics the production values stink someone else here mentioned the magically regenerating headlights on a getaway car and theres more of that lack of attention to detail nothing works the way its supposed to in this film and nobody cares  gas actually put me to sleep its not a sure cure for insomnia but really close on the cinematic sleep induction scale gas falls somewhere between last year at marienbad and george clooneys remake of solaris which itself was remarkable for being more boring than the mosfilm original despite that studios seeming unfamiliarity with the idea of keeping the audiences attention by judicious editing  judicious editing would have decimated gas to about twenty minutes the result would be pointless but no more so than the original film  certain films are so bad that they have a compelling quality that makes them worth watching anyway this isnt one of them dont waste your time its not even amusingly bad\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em5xnlTth1Kw"
      },
      "source": [
        "**【問題6】Word2Vecの学習**\n",
        "\n",
        "Word2Vecの学習を行なってください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mRR9C2sUYSM"
      },
      "source": [
        "# 単語リスト\n",
        "word_list = [after_preprocessing.split(' ')]\n",
        "\n",
        "# vector_size: 圧縮次元数\n",
        "# min_count: 出現頻度の低いものをカットする\n",
        "# window: 前後の単語を拾う際の窓の広さを決める\n",
        "# epochs: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
        "model = word2vec.Word2Vec(word_list,min_count=1) "
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y83fltSUa4y",
        "outputId": "7c665ae1-6f02-4366-df8c-132b6f98f7e4"
      },
      "source": [
        "# 確認\n",
        "model.wv['hand']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-4.6506428e-04,  3.9508307e-04, -3.0654841e-03,  1.3102759e-03,\n",
              "        1.7432475e-03, -4.8676301e-03, -3.8929114e-03,  4.0278379e-03,\n",
              "       -6.4504973e-04,  1.0522434e-03,  6.4104248e-04,  2.8759867e-04,\n",
              "       -7.2337530e-04, -2.6564249e-03, -1.2042486e-03, -4.9712732e-03,\n",
              "        4.6266047e-03,  2.9813803e-03,  1.7951798e-03, -2.4095377e-04,\n",
              "       -2.7310378e-03, -3.3457167e-03,  3.1014548e-03, -1.0606311e-03,\n",
              "        1.1501765e-03, -1.0185485e-05,  4.3510790e-03,  4.2610769e-03,\n",
              "       -4.2948206e-03,  7.2804012e-04, -2.3936257e-03,  3.1777457e-03,\n",
              "        3.4332240e-03,  4.4545974e-03,  4.3429439e-03, -4.7623226e-03,\n",
              "       -3.4956243e-03,  2.1836496e-04, -2.5654552e-03,  2.4253041e-03,\n",
              "        2.9785584e-03,  2.0196347e-04, -3.0201976e-04,  7.0340361e-04,\n",
              "        4.9999435e-03, -2.2740951e-03,  5.4951967e-04,  3.3088503e-03,\n",
              "        3.5001398e-03, -4.0223869e-03,  4.9407575e-03, -2.3565454e-04,\n",
              "        3.8655312e-03,  1.8595834e-03, -4.6195439e-03, -3.1373007e-03,\n",
              "       -3.8831991e-03, -2.0834508e-03, -4.6513611e-03, -2.0428151e-03,\n",
              "       -4.1550514e-03, -3.2789409e-04,  2.8373860e-03,  2.2477289e-03,\n",
              "        1.8894888e-03, -4.0505589e-03, -2.1605934e-03,  2.5345704e-03,\n",
              "        1.3528861e-03, -5.3958818e-05,  4.2604790e-03, -1.9450464e-03,\n",
              "       -4.4785836e-03,  8.1734953e-04,  4.2793443e-03, -4.9597514e-03,\n",
              "        5.2511157e-04,  2.6582947e-03, -4.0639015e-03,  1.1743356e-03,\n",
              "        4.0477468e-03, -7.1718963e-04,  4.7710282e-03,  4.2363084e-03,\n",
              "        4.0797410e-03,  7.2078616e-04,  1.8452093e-03,  2.9582641e-04,\n",
              "        1.7585441e-03, -2.4993226e-03,  4.0081441e-03, -3.3861743e-03,\n",
              "       -3.8738872e-03,  2.4548015e-03,  8.5707835e-04,  3.1418551e-03,\n",
              "       -3.5130670e-03,  4.6478407e-03,  2.2113468e-03,  4.6353557e-04],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fUTVteBh9_L"
      },
      "source": [
        "**【問題7】（アドバンス課題）ベクトルの可視化**\n",
        "\n",
        "得られたベクトルをt-SNEにより可視化してください。また、いくつかの単語を選びwv.most_similarを用いて似ている単語を調べてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pApyDF-ZiGse"
      },
      "source": [
        "**学習済みベクトル**\n",
        "\n",
        "巨大なコーパスで学習して得たベクトルも公開されているため、自分で学習をせずに利用することもできます。オリジナルのWord2Vecの他に同じ作者の発展系である FastText やスタンフォード大の GloVe があり、それぞれ公開されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0Lrmh0jiS7f"
      },
      "source": [
        "**【問題8】（アドバンス課題）Word2Vecを用いた映画レビューの分類**\n",
        "\n",
        "問題6で学習して得たベクトルや公開されている学習済みベクトルを用いてIMDB映画レビューデータセットの感情分類の学習・推定を行なってください。"
      ]
    }
  ]
}