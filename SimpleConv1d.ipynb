{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleConv1d.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1stGvFNH68_hTbuVb-AGKPqO0xUr0cQFz",
      "authorship_tag": "ABX9TyOhFO3ytCkf3iE7y8q/B9aY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinmiura/diveintocode-ml/blob/master/SimpleConv1d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL2CSQc_0kjA"
      },
      "source": [
        "**Sprintの目的**\n",
        "\n",
        "スクラッチを通してCNNの基礎を理解する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEN8WuZz0u-T"
      },
      "source": [
        "**どのように学ぶか**\n",
        "\n",
        "スクラッチで1次元用畳み込みニューラルネットワークを実装した後、学習と検証を行なっていきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV0Cik7HIyoq"
      },
      "source": [
        "# 必要なライブラリのインポート\n",
        "import numpy as np\n",
        "import math\n",
        "from keras.datasets import mnist\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8eOHpyU04H_"
      },
      "source": [
        "# 2.1次元の畳み込みニューラルネットワークスクラッチ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WplNfzjH0743"
      },
      "source": [
        "畳み込みニューラルネットワーク（CNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
        "\n",
        "\n",
        "このSprintでは1次元の 畳み込み層 を作成し、畳み込みの基礎を理解することを目指します。次のSprintでは2次元畳み込み層とプーリング層を作成することで、一般的に画像に対して利用されるCNNを完成させます。\n",
        "\n",
        "\n",
        "クラスの名前はScratch1dCNNClassifierとしてください。クラスの構造などは前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3FOzjEm0_Cy"
      },
      "source": [
        "1次元畳み込み層とは\n",
        "\n",
        "CNNでは画像に対しての2次元畳み込み層が定番ですが、ここでは理解しやすくするためにまずは1次元畳み込み層を実装します。1次元畳み込みは実用上は自然言語や波形データなどの 系列データ で使われることが多いです。\n",
        "\n",
        "\n",
        "畳み込みは任意の次元に対して考えることができ、立体データに対しての3次元畳み込みまではフレームワークで一般的に用意されています。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRDKJXvN1Had"
      },
      "source": [
        "データセットの用意\n",
        "\n",
        "検証には引き続きMNISTデータセットを使用します。1次元畳み込みでは全結合のニューラルネットワークと同様に平滑化されたものを入力します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BygSG6y1PCc"
      },
      "source": [
        "チャンネル数を1に限定した1次元畳み込み層のクラスSimpleConv1dを作成してください。基本構造は前のSprintで作成した全結合層のFCクラスと同じになります。なお、重みの初期化に関するクラスは必要に応じて作り変えてください。Xavierの初期値などを使う点は全結合層と同様です。\n",
        "\n",
        "\n",
        "ここでは パディング は考えず、ストライド も1に固定します。また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。この部分の拡張はアドバンス課題とします。\n",
        "\n",
        "\n",
        "フォワードプロパゲーションの数式は以下のようになります。\n",
        "\n",
        "$$\n",
        "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
        "$$\n",
        "\n",
        "$a_i$ : 出力される配列のi番目の値\n",
        "\n",
        "\n",
        "$F$ : フィルタのサイズ\n",
        "\n",
        "\n",
        "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値\n",
        "\n",
        "\n",
        "$w_s$ : 重みの配列のs番目の値\n",
        "\n",
        "\n",
        "$b$ : バイアス項\n",
        "\n",
        "\n",
        "すべてスカラーです。\n",
        "\n",
        "\n",
        "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。\n",
        "\n",
        "$$\n",
        "w_s^{\\prime} = w_s - \\alpha \\frac{\\partial L}{\\partial w_s} \\\\\n",
        "b^{\\prime} = b - \\alpha \\frac{\\partial L}{\\partial b}\n",
        "$$\n",
        "\n",
        "$\\alpha$ : 学習率\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial w_s} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}x_{(i+s)}\\\\\n",
        "\\frac{\\partial L}{\\partial b} = \\sum_{i=0}^{N_{out}-1} \\frac{\\partial L}{\\partial a_i}\n",
        "$$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値\n",
        "\n",
        "\n",
        "$N_{out}$ : 出力のサイズ\n",
        "\n",
        "\n",
        "前の層に流す誤差の数式は以下です。\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial x_j} = \\sum_{s=0}^{F-1} \\frac{\\partial L}{\\partial a_{(j-s)}}w_s\n",
        "$$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値\n",
        "\n",
        "\n",
        "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。\n",
        "\n",
        "\n",
        "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差をすべて足すことで勾配を求めます。計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEJRpGIPNNmr"
      },
      "source": [
        "class Sigmoid:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return self.sigmoid(A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        _sig = self.sigmoid(self.A)\n",
        "        return dZ * (1 - _sig)*_sig\n",
        "    \n",
        "    def sigmoid(self, X):\n",
        "        return 1 / (1 + np.exp(-X))\n",
        "\n",
        "class Tanh:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.tanh(A)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * (1 - (np.tanh(self.A))**2)\n",
        "\n",
        "class Softmax:\n",
        "    \n",
        "    def forward(self, X):\n",
        "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\n",
        "        return self.Z\n",
        "    \n",
        "    def backward(self, Y):\n",
        "        self.loss = self.loss_func(Y)\n",
        "        return self.Z - Y\n",
        "    \n",
        "    def loss_func(self, Y, Z=None):\n",
        "        if Z is None:\n",
        "            Z = self.Z\n",
        "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\n",
        "\n",
        "class ReLU:\n",
        "    \n",
        "    def forward(self, A):\n",
        "        self.A = A\n",
        "        return np.clip(A, 0, None)\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        return dZ * np.clip(np.sign(self.A), 0, None)\n",
        "\n",
        "class FC:\n",
        "\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
        "        self.B = initializer.B(n_nodes2)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        A = X@self.W + self.B\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        dZ = dA@self.W.T\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        self.dW = self.X.T@dA\n",
        "        self.optimizer.update(self)\n",
        "        return dZ\n",
        "\n",
        "class XavierInitializer:\n",
        "    \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(1 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "    \n",
        "class HeInitializer():\n",
        "    \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        self.sigma = math.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        B = self.sigma * np.random.randn(n_nodes2)\n",
        "        return B\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "    \n",
        "    def update(self, layer):\n",
        "        layer.W -= self.lr * layer.dW\n",
        "        layer.B -= self.lr * layer.dB\n",
        "        return\n",
        "\n",
        "class AdaGrad:\n",
        "    \n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "    \n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\n",
        "        \n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1] \n",
        "    \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    \n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]\n",
        "\n",
        "class SimpleInitializer:\n",
        "\n",
        "    def __init__(self, sigma):\n",
        "        self.sigma = sigma\n",
        "        \n",
        "    def W(self, *shape):\n",
        "        W = self.sigma * np.random.randn(*shape)\n",
        "        return W\n",
        "    \n",
        "    def B(self, *shape):\n",
        "        B = self.sigma * np.random.randn(*shape)\n",
        "        return B\n",
        "    \n",
        "class AdaGrad:\n",
        "    \n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "        self.HW = 1\n",
        "        self.HB = 1\n",
        "    \n",
        "    def update(self, layer):\n",
        "        self.HW += layer.dW**2\n",
        "        self.HB += layer.dB**2\n",
        "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\n",
        "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nldRaGliJAlc"
      },
      "source": [
        "# チャンネル数を1に限定した1次元畳み込み層のクラス(SimpleConv1d)の作成\n",
        "class SimpleConv1d():\n",
        "    \"\"\"\n",
        "    畳み込みクラス\n",
        "    \"\"\"\n",
        "    def forward(self, x, w, b):\n",
        "        \"\"\"\n",
        "        順伝播\n",
        "        Parameters\n",
        "        -----------\n",
        "        x : 入力配列\n",
        "        w : 重み\n",
        "        b : バイアス\n",
        "        \"\"\"\n",
        "        # 返り値入力配列\n",
        "        a = []\n",
        "        # 1づつずらしながら畳み込み計算\n",
        "        for i in range(len(w) - 1):\n",
        "            a.append((x[i:i+len(w)] @ w) + b[0])\n",
        "            \n",
        "        return np.array(a)\n",
        "    \n",
        "    def backward(self, x, w, da):\n",
        "        \"\"\"\n",
        "        逆伝播\n",
        "        x : 入力配列\n",
        "        w : 重み\n",
        "        da : 逆伝播の値\n",
        "        \"\"\"\n",
        "        # バイアスの勾配\n",
        "        db = np.sum(da)\n",
        "        \n",
        "        # 重みの勾配\n",
        "        dw = []\n",
        "        for i in range(len(w)):\n",
        "            dw.append(da @ x[i:i+len(da)])\n",
        "        dw = np.array(dw)\n",
        "        \n",
        "        # 逆伝播の値\n",
        "        dx = []\n",
        "        # 逆畳込み計算用配列\n",
        "        new_w = np.insert(w[::-1], 0, 0)\n",
        "        new_w = np.append(new_w, 0)\n",
        "        for i in range(len(new_w)-1):\n",
        "            dx.append(new_w[i:i+len(da)] @ da)\n",
        "        dx = np.array(dx[::-1])\n",
        "        return db, dw, dx"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj0B5oO81X3u"
      },
      "source": [
        "**【問題2】1次元畳み込み後の出力サイズの計算**\n",
        "\n",
        "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。パディングやストライドも含めています。この計算を行う関数を作成してください。\n",
        "\n",
        "$$\n",
        "N_{out} =  \\frac{N_{in}+2P-F}{S} + 1\\\\\n",
        "$$\n",
        "\n",
        "$N_{out}$ : 出力のサイズ（特徴量の数）\n",
        "\n",
        "\n",
        "$N_{in}$ : 入力のサイズ（特徴量の数）\n",
        "\n",
        "\n",
        "$P$ : ある方向へのパディングの数\n",
        "\n",
        "\n",
        "$F$ : フィルタのサイズ\n",
        "\n",
        "\n",
        "$S$ : ストライドのサイズ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVjrwdXcJVJ6"
      },
      "source": [
        "def output_size_calculation(n_in, F, P=0, S=1):\n",
        "    \"\"\"\n",
        "    出力サイズ計算\n",
        "    n_in : 入力サイズ\n",
        "    F : フィルターサイズ\n",
        "    P : パッディング数\n",
        "    S : ストライド数\n",
        "    \"\"\"\n",
        "    # 出力サイズの計算\n",
        "    n_out = int((n_in + 2*P - F) / S + 1)\n",
        "    \n",
        "    return n_out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsYDW9a92WSS"
      },
      "source": [
        "**【問題3】小さな配列での1次元畳み込み層の実験**\n",
        "\n",
        "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。\n",
        "\n",
        "\n",
        "入力x、重みw、バイアスbを次のようにします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3SY-hKf0ebq"
      },
      "source": [
        "# テストデータ\n",
        "x = np.array([1,2,3,4])\n",
        "w = np.array([3, 5, 7])\n",
        "b = np.array([1])\n",
        "da = np.array([10, 20])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAi5vQ4oJ7pk"
      },
      "source": [
        "# インスタンス化\n",
        "simple_conv_1d = SimpleConv1d()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYrT71l8KC9H",
        "outputId": "0dd75751-64bb-4732-ae41-574b2af58099"
      },
      "source": [
        "# 順伝播\n",
        "simple_conv_1d.forward(x, w, b)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([35, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPG5WfcvGs_d"
      },
      "source": [
        "フォワードプロパゲーションをすると出力は次のようになります。\n",
        "\n",
        "a = np.array([35, 50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKWzWSKXGzaD"
      },
      "source": [
        "次にバックプロパゲーションを考えます。誤差は次のようであったとします。\n",
        "\n",
        "delta_a = np.array([10, 20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF9dcXnFG5Ag"
      },
      "source": [
        "バックプロパゲーションをすると次のような値になります。\n",
        "\n",
        "delta_b = np.array([30])\n",
        "\n",
        "delta_w = np.array([50, 80, 110])\n",
        "\n",
        "delta_x = np.array([30, 110, 170, 140])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtAq7aonKWPL",
        "outputId": "6beb1bea-c433-4367-95cc-dd61c221a7d2"
      },
      "source": [
        "# 逆伝播\n",
        "db, dw, dx = simple_conv_1d.backward(x, w, da)\n",
        "print(db)\n",
        "print(dw)\n",
        "print(dx)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30\n",
            "[ 50  80 110]\n",
            "[ 30 110 170 140]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JKeKhqtYKmL9",
        "outputId": "0d39a81d-149e-4f3e-8aa2-1fe6a95bb5fd"
      },
      "source": [
        "# 逆伝播の値\n",
        "dx = []\n",
        "# 逆畳込み計算用配列\n",
        "print('w : ',w)\n",
        "new_w = np.insert(w[::-1], 0, 0)\n",
        "print('new_w : ',new_w)\n",
        "new_w = np.append(new_w, 0)\n",
        "print('new_w : ',new_w)\n",
        "for i in range(len(new_w)-1):\n",
        "    print('--------')\n",
        "    print(new_w[i:i+len(da)])\n",
        "    print(da)\n",
        "    dx.append(new_w[i:i+len(da)] @ da)\n",
        "dx = np.array(dx[::-1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w :  [3 5 7]\n",
            "new_w :  [0 7 5 3]\n",
            "new_w :  [0 7 5 3 0]\n",
            "--------\n",
            "[0 7]\n",
            "[10 20]\n",
            "--------\n",
            "[7 5]\n",
            "[10 20]\n",
            "--------\n",
            "[5 3]\n",
            "[10 20]\n",
            "--------\n",
            "[3 0]\n",
            "[10 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq0XkI6cHBre"
      },
      "source": [
        "**実装上の工夫**\n",
        "\n",
        "畳み込みを実装する場合は、まずはfor文を重ねていく形で構いません。しかし、できるだけ計算は効率化させたいため、以下の式を一度に計算する方法を考えることにします。\n",
        "\n",
        "$$\n",
        "a_i = \\sum_{s=0}^{F-1}x_{(i+s)}w_s+b\n",
        "$$\n",
        "\n",
        "バイアス項は単純な足し算のため、重みの部分を見ます。\n",
        "\n",
        "$$\n",
        "\\sum_{s=0}^{F-1}x_{(i+s)}w_s\n",
        "$$\n",
        "\n",
        "これは、xの一部を取り出した配列とwの配列の内積です。具体的な状況を考えると、以下のようなコードで計算できます。この例では流れを分かりやすくするために、各要素同士でアダマール積を計算してから合計を計算しています。これは結果的に内積と同様です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SWAU1H0HjHP"
      },
      "source": [
        "x = np.array([1, 2, 3, 4])\n",
        "w = np.array([3, 5, 7])\n",
        "a = np.empty((2, 3))\n",
        "indexes0 = np.array([0, 1, 2]).astype(np.int)\n",
        "indexes1 = np.array([1, 2, 3]).astype(np.int)\n",
        "a[0] = x[indexes0]*w # x[indexes0]は([1, 2, 3])である\n",
        "a[1] = x[indexes1]*w # x[indexes1]は([2, 3, 4])である\n",
        "\n",
        "a = a.sum(axis=1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxIcYE4-Hlog"
      },
      "source": [
        "ndarrayは配列を使ったインデックス指定ができることを利用した方法です。\n",
        "\n",
        "\n",
        "また、二次元配列を使えば一次元配列から二次元配列が取り出せます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc2YTO-pHoth",
        "outputId": "ca8c7774-ab35-4939-dc11-d2ab6864b8ed"
      },
      "source": [
        "x = np.array([1, 2, 3, 4])\n",
        "indexes = np.array([[0, 1, 2], [1, 2, 3]]).astype(np.int)\n",
        "print(x[indexes]) # ([[1, 2, 3], [2, 3, 4]])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 3]\n",
            " [2 3 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-E8Qks8HsZg"
      },
      "source": [
        "このこととブロードキャストなどをうまく組み合わせることで、一度にまとめて計算することも可能です。\n",
        "\n",
        "\n",
        "畳み込みの計算方法に正解はないので、自分なりに効率化していってください。\n",
        "\n",
        "\n",
        "《参考》\n",
        "\n",
        "\n",
        "以下のページのInteger array indexingの部分がこの方法についての記述です。\n",
        "\n",
        "\n",
        "Indexing — NumPy v1.17 Manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XcOcH6zHtlI"
      },
      "source": [
        "**【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成**\n",
        "\n",
        "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。\n",
        "\n",
        "\n",
        "例えば以下のようなx, w, bがあった場合は、"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDEDnleHH4wn"
      },
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\n",
        "w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\n",
        "b = np.array([1, 2, 3]) # （出力チャンネル数）"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn3EgpnZH7yj"
      },
      "source": [
        "出力は次のようになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urDz67PFHruT"
      },
      "source": [
        "a = np.array([[16, 22], [17, 23], [18, 24]]) # shape(3, 2)で、（出力チャンネル数、特徴量数）である。"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HU192jvXIBil"
      },
      "source": [
        "入力が2チャンネル、出力が3チャンネルの例です。計算グラフを書いた上で、バックプロパゲーションも手計算で考えてみましょう。計算グラフの中には和と積しか登場しないので、微分を新たに考える必要はありません。\n",
        "\n",
        "\n",
        "《補足》\n",
        "\n",
        "\n",
        "チャンネル数を加える場合、配列をどういう順番にするかという問題があります。(バッチサイズ、チャンネル数、特徴量数)または(バッチサイズ、特徴量数、チャンネル数)が一般的で、ライブラリによって順番は異なっています。（切り替えて使用できるものもあります）\n",
        "\n",
        "\n",
        "今回のスクラッチでは自身の実装上どちらが効率的かを考えて選んでください。上記の例ではバッチサイズは考えておらず、(チャンネル数、特徴量数)です。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIlyJSwDK8Ng"
      },
      "source": [
        "class Conv1d_Arbitrary_Strides:\n",
        "    \"\"\"畳み込みクラス\n",
        "    \"\"\"\n",
        "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride = 1):\n",
        "        \"\"\"コンストラクタ\n",
        "        b_size : フィルターサイズ\n",
        "        initializer : 初期化クラス\n",
        "        optimizer : 最適化手法クラス\n",
        "        n_in_channels : 入力チャンネル数\n",
        "        n_out_channels : 出力チャンネル数\n",
        "        pa : パディング数\n",
        "        \"\"\"\n",
        "        self.b_size = b_size\n",
        "        self.optimizer = optimizer\n",
        "        self.pa = pa\n",
        "        self.stride = stride\n",
        "        # 重みの初期化\n",
        "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\n",
        "        # バイアスの初期化\n",
        "        self.B = initializer.B(n_out_channels)\n",
        "        self.n_in_channels = n_in_channels\n",
        "        self.n_out_channels = n_out_channels\n",
        "        self.n_out = None\n",
        "        \n",
        "    def forward(self, X):\n",
        "        \"\"\"順伝播\n",
        "        X : 入力配列\n",
        "        \"\"\"\n",
        "        self.n_samples = X.shape[0]\n",
        "        # 入力配列の特徴量数\n",
        "        self.n_in = X.shape[-1]\n",
        "        # 出力サイズ\n",
        "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\n",
        "        # 計算のために逆転させる\n",
        "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\n",
        "        # 0埋め実施\n",
        "        self.X = np.pad(X, ((0,0), (0, 0), ((self.b_size-1), 0)))\n",
        "        # 出力配列（A）の計算のためゼロ配列X1を用意する\n",
        "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\n",
        "        # 重みの長さでループ\n",
        "        for i in range(self.b_size):\n",
        "            # ずらしながら上書き\n",
        "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\n",
        "        # 重みとバイアスを考慮して計算\n",
        "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\n",
        "        return A\n",
        "    \n",
        "    def backward(self, dA):\n",
        "        \"\"\"逆伝播\n",
        "        dA : 逆伝播してきた配列\n",
        "        \"\"\"\n",
        "        # 重みの勾配\n",
        "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\n",
        "        # バイアスの勾配\n",
        "        self.dB = np.sum(dA, axis=(0, -1))\n",
        "        # 逆伝播の値計算のためにdAを変形\n",
        "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\n",
        "        # 出力配列（dX）の計算のためゼロ配列dA1を用意する\n",
        "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\n",
        "        # 重みの長さでループ\n",
        "        for i in range(self.b_size):\n",
        "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\n",
        "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\n",
        "        # 重みとバイアスの更新\n",
        "        self.optimizer.update(self)\n",
        "        return dX"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLFcHOy8LBl_"
      },
      "source": [
        "# インスタンス化\n",
        "conc_1d = Conv1d_Arbitrary_Strides(b_size=3, initializer=SimpleInitializer(0.01), optimizer=SGD(0.01), n_in_channels=2, n_out_channels=3, pa=0)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZyES7DfLFDm"
      },
      "source": [
        "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]])\n",
        "# コンストラクタ内で初期化しているけど確認のため再代入\n",
        "conc_1d.W = np.ones((3, 2, 3), dtype=float)\n",
        "conc_1d.B = np.array([1, 2, 3], dtype=float)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "eOyGthojjSBe",
        "outputId": "b7458b6e-4e0c-4599-a091-9d4540106597"
      },
      "source": [
        "conc_1d.forward(x)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-36bcaccd3093>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconc_1d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-246db55e1cc5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# 計算のために逆転させる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# 0埋め実施\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 8 into shape (2,2,4)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3OY4QV8IFNZ"
      },
      "source": [
        "**【問題5】（アドバンス課題）パディングの実装**\n",
        "\n",
        "畳み込み層にパディングの機能を加えてください。1次元配列の場合、前後にn個特徴量を増やせるようにしてください。\n",
        "\n",
        "\n",
        "最も単純なパディングはすべて0で埋める ゼロパディング であり、CNNでは一般的です。他に端の値を繰り返す方法などもあります。\n",
        "\n",
        "\n",
        "フレームワークによっては、元の入力のサイズを保つようにという指定をすることができます。この機能も持たせておくと便利です。なお、NumPyにはパディングの関数が存在します。\n",
        "\n",
        "\n",
        "numpy.pad — NumPy v1.17 Manual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fS8hvBWIM9e"
      },
      "source": [
        "**【問題6】（アドバンス課題）ミニバッチへの対応**\n",
        "\n",
        "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mf9poRnDIRmo"
      },
      "source": [
        "**【問題7】（アドバンス課題）任意のストライド数**\n",
        "\n",
        "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOWvgXbtIY6D"
      },
      "source": [
        "# 3.検証"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wUomVW_Iheh"
      },
      "source": [
        "**【問題8】学習と推定**\n",
        "\n",
        "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。\n",
        "\n",
        "\n",
        "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。\n",
        "\n",
        "\n",
        "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpPwxieLLMZK"
      },
      "source": [
        "# データの準備\n",
        "# データ読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 画像データ→行データに\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# 正規化\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# onehotベクトル化\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "\n",
        "# 訓練データと評価データに\n",
        "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xlbi9jgLSAw"
      },
      "source": [
        "class ScratchCNNClassifier:\n",
        "    \n",
        "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        -----------\n",
        "        num_epoch : 学習回数\n",
        "        lr : 学習率\n",
        "        batch_size : バッチサイズ\n",
        "        n_features : 特徴量数\n",
        "        n_nodes1 : 1層目のノード数\n",
        "        n_nodes2 : 2層目のノード数\n",
        "        n_output : 出力層の数\n",
        "        verbose : 仮定出力するか否か\n",
        "        Activater : 活性化関数\n",
        "        Optimizer : 最適化手法\n",
        "        \"\"\"\n",
        "        self.num_epoch = num_epoch\n",
        "        self.lr = lr\n",
        "        self.verbose = verbose  \n",
        "        self.batch_size = batch_size \n",
        "        self.n_features = n_features \n",
        "        self.n_nodes2 = n_nodes2 \n",
        "        self.n_output = n_output \n",
        "        self.Activater = Activater\n",
        "        if Activater == Sigmoid or Activater == Tanh:\n",
        "            self.Initializer = XavierInitializer\n",
        "        elif Activater == ReLU:\n",
        "            self.Initializer = HeInitializer\n",
        "        self.Optimizer = Optimizer\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"学習\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 訓練データの説明変数\n",
        "        y : 訓練データの目的変数\n",
        "        X_val : 評価データの説明変数\n",
        "        y_val : 評価データの目的変数\n",
        "        \"\"\"        \n",
        "        # レイヤー初期化\n",
        "        self.Conv1d_Arbitrary_Strides = Conv1d_Arbitrary_Strides(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\n",
        "        self.Conv1d_Arbitrary_Strides.n_out = output_size_calculation(X.shape[-1], self.Conv1d_Arbitrary_Strides.b_size, self.Conv1d_Arbitrary_Strides.pa, self.Conv1d_Arbitrary_Strides.stride)\n",
        "        self.activation1 = self.Activater()\n",
        "        self.FC2 = FC(1*self.Conv1d_Arbitrary_Strides.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\n",
        "        self.activation2 = self.Activater()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\n",
        "        self.activation3 = Softmax()\n",
        "        \n",
        "        # loss配列定義と初期値格納（loss:ミニバッチごとの損失格納  loss_epoch:ミニバッチ学習終了後の全体損失）\n",
        "        self.loss = []\n",
        "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\n",
        "        \n",
        "        # 学習回数分ループ\n",
        "        for _ in range(self.num_epoch):\n",
        "            # ミニバッチイテレータ生成\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\n",
        "            # イテレータ呼び出し\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                # 順伝播\n",
        "                self.forward_propagation(mini_X)\n",
        "                # 逆伝播\n",
        "                self.back_propagation(mini_X, mini_y)\n",
        "                # 損失記録\n",
        "                self.loss.append(self.activation3.loss)\n",
        "            # 損失記録\n",
        "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"予測値出力\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 説明変数\n",
        "        \"\"\"\n",
        "        return np.argmax(self.forward_propagation(X), axis=1)\n",
        "    \n",
        "    def forward_propagation(self, X):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 訓練データの説明変数\n",
        "        \"\"\"\n",
        "        A1 = self.Conv1d_Arbitrary_Strides.forward(X)\n",
        "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        return Z3\n",
        "        \n",
        "    def back_propagation(self, X, y_true):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 訓練データの説明変数\n",
        "        y_true : 正解データ\n",
        "        \"\"\"\n",
        "        dA3 = self.activation3.backward(y_true) \n",
        "        dZ2 = self.FC3.backward(dA3)\n",
        "        dA2 = self.activation2.backward(dZ2)\n",
        "        dZ1 = self.FC2.backward(dA2)\n",
        "        dA1 = self.activation1.backward(dZ1)\n",
        "        dA1 = dA1[:, np.newaxis]\n",
        "        dZ0 = self.Conv1d_Arbitrary_Strides.backward(dA1) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "GYP7ZW1FLbfu",
        "outputId": "7352c8fa-44c0-4b38-a817-59beb4811222"
      },
      "source": [
        "cnn = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\n",
        "cnn.fit(X_train_[:1000], y_train_[:1000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-7672c2a2f9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchCNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nodes1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_nodes2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivater\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTanh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-18-e14308e65de8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# loss配列定義と初期値格納（loss:ミニバッチごとの損失格納  loss_epoch:ミニバッチ学習終了後の全体損失）\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# 学習回数分ループ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e14308e65de8>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0m訓練データの説明変数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1d_Arbitrary_Strides\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b90792c94eb0>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size_calculation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# 計算のために逆転させる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# 0埋め実施\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 784000 into shape (1,784)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0AUtyKlLdeF"
      },
      "source": [
        "y_pred = cnn.predict(X_test)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}