{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ディープニューラルネットワーク.ipynb",
      "provenance": [],
      "mount_file_id": "19AaF3Yjrhb9dA780zlfCK1NwJPfFSD2u",
      "authorship_tag": "ABX9TyND07dU4CWhY/touqgBD1kl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinmiura/diveintocode-ml/blob/master/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWedC1-SPWca"
      },
      "source": [
        "**Sprintの目的**\n",
        "\n",
        "スクラッチを通してニューラルネットワークの発展的内容を理解する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rmoTy_zPVZ_"
      },
      "source": [
        "**どのように学ぶか**\n",
        "\n",
        "スクラッチで作成したニューラルネットワークの実装を拡張していきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAg8sxAWUyM_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJv5V0b0WplF",
        "outputId": "220bf7eb-8b0f-4e29-ee21-5a5f5b475892"
      },
      "source": [
        "# 読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 画像データ→行データ\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# 正規化\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# 分割(訓練データ・評価データ)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "# one-hotベクトル化\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_val[:, np.newaxis])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4ZW02ooPl4d"
      },
      "source": [
        "# 2.ディープニューラルネットワークスクラッチ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3kNexWCPvbD"
      },
      "source": [
        "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
        "\n",
        "\n",
        "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
        "\n",
        "\n",
        "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifYccD8WPzp2"
      },
      "source": [
        "**層などのクラス化**\n",
        "\n",
        "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
        "\n",
        "\n",
        "***手を加える箇所***\n",
        "\n",
        "\n",
        "層の数\n",
        "層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
        "活性化関数の種類\n",
        "重みやバイアスの初期化方法\n",
        "最適化手法\n",
        "\n",
        "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
        "\n",
        "\n",
        "実装方法は自由ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
        "\n",
        "《サンプルコード1》\n",
        "\n",
        "\n",
        "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "cJDooMb4OSKw",
        "outputId": "1c0285b5-e8da-4347-e609-7a4a5df9f32f"
      },
      "source": [
        "# self.sigma : ガウス分布の標準偏差\n",
        "# self.lr : 学習率\n",
        "# self.n_nodes1 : 1層目のノード数\n",
        "# self.n_nodes2 : 2層目のノード数\n",
        "# self.n_output : 出力層のノード数\n",
        "\n",
        "optimizer = SGD(self.lr)\n",
        "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation1 = Tanh()\n",
        "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation2 = Tanh()\n",
        "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation3 = Softmax()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a204247732d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# self.n_output : 出力層のノード数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_nodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleInitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcwGKixgQJeR"
      },
      "source": [
        "《サンプルコード2》\n",
        "\n",
        "\n",
        "イテレーションごとのフォワード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh3dgEvNQQgI"
      },
      "source": [
        "A1 = self.FC1.forward(X)\n",
        "Z1 = self.activation1.forward(A1)\n",
        "A2 = self.FC2.forward(Z1)\n",
        "Z2 = self.activation2.forward(A2)\n",
        "A3 = self.FC3.forward(Z2)\n",
        "Z3 = self.activation3.forward(A3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-MQrCtEQS4B"
      },
      "source": [
        "《サンプルコード3》\n",
        "\n",
        "\n",
        "イテレーションごとのバックワード\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDlC-tuUQW1D"
      },
      "source": [
        "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "dZ2 = self.FC3.backward(dA3)\n",
        "dA2 = self.activation2.backward(dZ2)\n",
        "dZ1 = self.FC2.backward(dA2)\n",
        "dA1 = self.activation1.backward(dZ1)\n",
        "dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ZfdKDgQe9W"
      },
      "source": [
        "**【問題1】全結合層のクラス化**\n",
        "\n",
        "全結合層のクラス化を行なってください。\n",
        "\n",
        "\n",
        "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
        "\n",
        "\n",
        "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
        "\n",
        "\n",
        "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、すべて全結合層が持つインスタンス変数にすることができます。\n",
        "\n",
        "\n",
        "初期化方法と最適化手法のクラスについては後述します。\n",
        "\n",
        "\n",
        "《雛形》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urilqsyxQwTs"
      },
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        initializer : 初期化方法のインスタンス\n",
        "        optimizer : 最適化手法のインスタンス\n",
        "        \"\"\"\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # 初期化インスタンスの関数実行\n",
        "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = initializer.B(self.n_nodes2)\n",
        "        # 最適化インスタンス\n",
        "        self.optimizer = optimizer\n",
        "        # 勾配更新の際に使用(AdaGradのみ)\n",
        "        self.HW = 0\n",
        "        self.HB = 0\n",
        "  \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"        \n",
        "        # 逆伝播時に使用\n",
        "        self.Z = X\n",
        "        # 順伝播計算部分本体\n",
        "        self.A = X @ self.W + self.B\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        dA : 前の層から逆伝播してきた値（活性化関数の逆伝播の値が入ってくる）\n",
        "        \n",
        "        Overview\n",
        "        ----------\n",
        "        前回のSprint9 ニューラルネットワークでは下記のような逆伝播処理になっていた\n",
        "            0 ## 2層目\n",
        "            1 dZ2 = dA3 @ self.W3.T\n",
        "            2 dA2 = dZ2 * (1 - self.tanh_function(self.A2)**2)\n",
        "            3 dW2 = self.Z1.T @ dA2\n",
        "            4 dB2 = np.sum(dA2, axis=0)\n",
        "            5 ## 1層目\n",
        "            6 dZ1 = dA2 @ self.W2.T\n",
        "            7 dA1 = dZ1 * (1 - self.tanh_function(self.A1)**2)\n",
        "            8 dW1 = X.T @ dA1\n",
        "            9 dB1 = np.sum(dA1, axis=0)\n",
        "        勾配の計算\n",
        "            ここでは、活性化関数の逆伝播は別で実装し、その値をこの関数の引数として受け取っているので、\n",
        "            この関数の  dA  は、Sprint9の上記の  dA2  に該当する\n",
        "            よって、上記3,4に該当する処理を書いていけばいい\n",
        "        逆伝播の値の計算\n",
        "            活性化関数の逆伝播に渡してやる値、つまりSprint9の上記の  dZ1  に該当する\n",
        "        重み更新\n",
        "            勾配dB,dWが計算されているので、このインスタンス自身をoptimizerインスタンスのupdate関数に渡してやる\n",
        "            optimizerインスタンスのupdate関数の引数は、layerとなっているので、update関数内では、layer.変数名で\n",
        "            このインスタンスの各種メンバ変数にアクセスできる\n",
        "        \"\"\"\n",
        "        # バイアス項の勾配\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        # バイアス項以外の勾配\n",
        "        self.dW = self.Z.T @ dA\n",
        "        # 逆伝播させる値\n",
        "        self.dZ = dA @ self.W.T\n",
        "        # 重み更新\n",
        "        self = self.optimizer.update(self)\n",
        "        return self.dZ"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_bHJMzKQ06V"
      },
      "source": [
        "**【問題2】初期化方法のクラス化**\n",
        "\n",
        "初期化を行うコードをクラス化してください。\n",
        "\n",
        "\n",
        "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
        "\n",
        "\n",
        "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
        "《雛形》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlKvWXCeQ_Rz"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ------------\n",
        "        sigma : \n",
        "        　　　　重みの初期化の際のガウス分布の標準偏差\n",
        "        \"\"\"\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvgiPsN8RDgY"
      },
      "source": [
        "**【問題3】最適化手法のクラス化**\n",
        "\n",
        "最適化手法のクラス化を行なってください。\n",
        "\n",
        "\n",
        "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
        "\n",
        "\n",
        "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29S41GUIRLju"
      },
      "source": [
        "# 雛形\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "\n",
        "        Overview\n",
        "        FCクラス内のbackward関数内で下記の要に実行されている\n",
        "            0 self.optimizer.update(self)\n",
        "        引数に「FCクラスのインスタンス自身」が入っていることを考えると\n",
        "        layer.dW , layer.dB , layer.Z は「FCクラスのインスタンスのメンバ変数」にアクセスしていると考えられる              \n",
        "        \"\"\"\n",
        "        layer.W -= self.lr * layer.dW / len(layer.Z)\n",
        "        layer.B -= self.lr * layer.dB / len(layer.Z)\n",
        "        return layer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifAPkW3iRWdH"
      },
      "source": [
        "**【問題4】活性化関数のクラス化**\n",
        "\n",
        "活性化関数のクラス化を行なってください。\n",
        "\n",
        "\n",
        "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLdtJcDUoqP-"
      },
      "source": [
        "class Sigmoid:\n",
        "    \"\"\"シグモイド関数\"\"\"\n",
        "    def forward(self, A):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        Z = 1 / (1 + np.exp(-self.A))\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 逆伝播されてきた値\n",
        "        \"\"\"\n",
        "        dA = dZ * ((1 / (1 + np.exp(-self.A))) - (1 / (1 + np.exp(-self.A)))**2)\n",
        "        return dA"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFpVgmH-o9QB"
      },
      "source": [
        "class Tanh:\n",
        "    \"\"\"tanh関数\"\"\"\n",
        "    def forward(self, A):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        Z = np.tanh(self.A)\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 逆伝播されてきた値\n",
        "        \"\"\"\n",
        "        dA = dZ * (1 - np.tanh(self.A)**2)\n",
        "        return dA"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efv8Dc2JpAa7"
      },
      "source": [
        "class Softmax:\n",
        "    \"\"\"Softmax関数\"\"\"\n",
        "    def forward(self, A): \n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
        "        return Z\n",
        "        \n",
        "    def backward(self, Z, y):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : 出力値\n",
        "        y : 正解データ\n",
        "        \"\"\"\n",
        "        # 逆伝播の値\n",
        "        dA = Z - y\n",
        "        # 損失\n",
        "        loss = - np.sum(y * np.log(Z)) / len(y)\n",
        "        return dA, loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7GlxB2cTJAp"
      },
      "source": [
        "**発展的要素**\n",
        "\n",
        "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC0pEx6cTQL_"
      },
      "source": [
        "**【問題5】ReLUクラスの作成**\n",
        "\n",
        "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
        "\n",
        "\n",
        "ReLUは以下の数式です。\n",
        "\n",
        "$$\n",
        "f(x) = ReLU(x) = \\begin{cases}\n",
        "x  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$x$ : ある特徴量。スカラー\n",
        "\n",
        "\n",
        "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
        "\n",
        "\n",
        "numpy.maximum — NumPy v1.15 Manual\n",
        "\n",
        "\n",
        "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
        "1  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
        "\n",
        "\n",
        "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkvATh4YpKq5"
      },
      "source": [
        "class ReLU:\n",
        "    \"\"\"ReLU関数\"\"\"\n",
        "    def forward(self, A):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 逆伝播されてきた値\n",
        "        \"\"\"\n",
        "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
        "        return dA"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3YduTcTT9DJ"
      },
      "source": [
        "**【問題6】重みの初期値**\n",
        "\n",
        "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
        "\n",
        "\n",
        "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taks2WXmUCDh"
      },
      "source": [
        "**Xavierの初期値**\n",
        "\n",
        "Xavierの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
        "$$\n",
        "\\sigma = \\frac{1}{\\sqrt{n}}\n",
        "$$\n",
        "\n",
        "$n$ : 前の層のノード数\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky2iNjrHUqA1"
      },
      "source": [
        "Heの初期値\n",
        "\n",
        "Heの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
        "$$\n",
        "\\sigma = \\sqrt{\\frac{2}{n}}\n",
        "$$\n",
        "\n",
        "$n$ : 前の層のノード数\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVKDIw-6pSX6"
      },
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"Xavierの初期化クラス\"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        sigma : 使用されていない\n",
        "        \n",
        "        Overview\n",
        "        ----------\n",
        "        なぜ使用されていないのに、引数として受け取っているのか\n",
        "        \n",
        "        初期化クラスは、概略すると、下記のように使用されている\n",
        "        \n",
        "        呼び出しの大元\n",
        "            dnn = ScratchDeepNeuralNetrowkClassifier(initializer=SGD or XavierInitializer or HeInitializer) \n",
        "            \n",
        "        定義部分(ScratchDeepNeuralNetrowkClassifier)\n",
        "            class ScratchDeepNeuralNetrowkClassifier:\n",
        "                def __init__(self,xxx,xxx,xxx,initializer):\n",
        "                    .....\n",
        "                    self.initializer = initializer\n",
        "                    .....\n",
        "                def fit(self,xxx,xxx,xxx):\n",
        "                    self.initializer(self.sigma)\n",
        "        \n",
        "        つまり、\n",
        "        「呼び出しの大元」で、どの初期化クラスが渡されるかわからないので、\n",
        "        初期化クラスによっては、sigmaが必要なものと、別途計算が必要なものがあるので、\n",
        "        同じ呼び出し方をしてやるために、この初期かクラスのコンストラクタでも引数として、sigmaを受け取っている\n",
        "        \n",
        "        同じ呼び出し方をしてやらないのであれば、上記fitは、下記のような書き方でも可能\n",
        "            if initializerのクラス名 == \"SGD\":\n",
        "                self.initializer(self.sigma)\n",
        "            else initializerのクラス名 != \"SGD\":\n",
        "                self.initializer()\n",
        "        \"\"\"\n",
        "        _ = sigma\n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : 前の層のノード数\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwRlQfepU2Z"
      },
      "source": [
        "class HeInitializer:\n",
        "    \"\"\"Heの初期化クラス\"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        _ = sigma\n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : 前の層のノード数\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        self.sigma = np.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUtrUna4U76E"
      },
      "source": [
        "**【問題7】最適化手法**\n",
        "\n",
        "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
        "\n",
        "\n",
        "まず、これまで使ってきたSGDを確認します。\n",
        "\n",
        "$$\n",
        "W_i^{\\prime} = W_i - \\alpha \\frac{\\partial L}{\\partial W_i} \\\\\n",
        "B_i^{\\prime} = B_i - \\alpha \\frac{\\partial L}{\\partial B_i}\n",
        "$$\n",
        "\n",
        "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的にはすべて同じとする）\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
        "\n",
        "\n",
        "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
        "\n",
        "\n",
        "学習率は重み一つひとつに対して異なることになります。\n",
        "\n",
        "\n",
        "$$\n",
        "H_i^{\\prime} = H_i + \\frac{\\partial L}{\\partial W_i} \\odot \\frac{\\partial L}{\\partial W_i} \\\\\n",
        "W_i^{\\prime} = W_i - \\alpha \\left( \\frac{1}{\\sqrt{H_i^{\\prime}}} \\odot \\frac{\\partial L}{\\partial W_i} \\right)\n",
        "$$\n",
        "\n",
        "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
        "\n",
        "\n",
        "$H_i^{\\prime}$ : 更新した $H_i$\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOF2Zot8pek6"
      },
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"最適化手法（AdaGrad）\"\"\"\n",
        "    def __init__(self, lr):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        lr : 学習率\n",
        "        \"\"\"\n",
        "        self.lr = lr \n",
        "    \n",
        "    def update(self, layer):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : layerインスタンス\n",
        "        \"\"\"\n",
        "        layer.HW += layer.dW * layer.dW\n",
        "        layer.HB += layer.dB * layer.dB\n",
        "        delta = 1e-7 # 0割エラー防止のため\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + delta) / len(layer.Z)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.HB) + delta) / len(layer.Z)\n",
        "        return layer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3beIDX9Vjfn"
      },
      "source": [
        "**【問題8】クラスの完成**\n",
        "\n",
        "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjE-agETpjMr"
      },
      "source": [
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        \"\"\"通常のコンストラクタと同様の働き\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 説明変数（画像の1次元データ）\n",
        "        y : 目的変数（ラベル）\n",
        "        batch_size : 必要なミニバッチのデータ数\n",
        "        seed : ランダムシード固定\n",
        "        \"\"\"\n",
        "        # ランダムシードの固定（学習ごとに同じ生成順）\n",
        "        np.random.seed(seed)\n",
        "        # バッチ数のメンバ変数\n",
        "        self.batch_size = batch_size\n",
        "        # データ全体の長さ分のインデックスをランダムに並べ替え\n",
        "        # np.random.permutation:配列をランダムに並べ替え\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        # 並べ替えたインデックスと同じ順番で説明変数と目的変数を並べ替え\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        # データ数をバッチ数で割って、何回呼び出せば、全データを学習したことになるかの判定\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    \n",
        "    def __iter__(self):\n",
        "        # 何回目の呼び出しか\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # 全データを学習すればストップ\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        # 並び変えた_X,_yの何番目のインデックスを採用するか\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        # returnする前にカウンタに+1しておく\n",
        "        self._counter += 1\n",
        "        # 説明変数と目的変数を返す\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAdJkCDipnqb"
      },
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "\n",
        "    def __init__(self,batch_size=20,n_features=784,n_nodes1 =400,n_nodes2 = 200,n_output =10,lr =0.005,epoch=10,sigma=0.02,optimizer=SGD, initializer=HeInitializer,activater=ReLU,output_activater=Softmax,verbose=True):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : バッチサイズ（default:20)\n",
        "        n_features : 説明変数の数（default:784)\n",
        "        n_nodes1 : 前の層のノード数（default:400)\n",
        "        n_nodes2 : 当該層のノード数（default:200)\n",
        "        n_output : 出力層のノード数（default:10)\n",
        "        sigma : 初期化時のパラメータ（default:0.02)\n",
        "        lr : 学習率（default:0.005)\n",
        "        verbose : 計算過程の出力（default:True)\n",
        "        epoch : 学習回数（default:10)\n",
        "        optimizer : 最適化手法（default:SGD)\n",
        "        initializer : 初期化方法（default:HeInitializer）\n",
        "        activater : 活性化関数（default:ReLU）\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.n_features = n_features \n",
        "        self.n_nodes1 = n_nodes1  \n",
        "        self.n_nodes2 = n_nodes2 \n",
        "        self.n_output = n_output\n",
        "        self.lr = lr\n",
        "        self.epoch = epoch\n",
        "        self.optimizer = optimizer \n",
        "        self.sigma = sigma\n",
        "        self.initializer = initializer \n",
        "        self.activater = activater\n",
        "        self.output_activater = output_activater \n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"学習\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 訓練データの説明変数\n",
        "        y : 訓練データの目的変数\n",
        "        X_val : 評価データの説明変数\n",
        "        y_val : 評価データの目的変数\n",
        "        \"\"\"\n",
        "        # lossの記録用配列\n",
        "        self.loss_train = [] \n",
        "        self.loss_val = [] \n",
        "        # 最適化手法の初期化\n",
        "        optimizer = self.optimizer(self.lr)\n",
        "        # 各層の初期化\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
        "        self.activation1 = self.activater()\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
        "        self.activation2 = self.activater()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer(self.sigma), optimizer)\n",
        "        self.activation3 = self.output_activater()\n",
        "        \n",
        "        # 学習回数分ループ\n",
        "        for i in range(self.epoch):\n",
        "            # ミニバッチイテレータ生成\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "            # ミニバッチイテレータループ\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                ## 順伝播\n",
        "                # 1層目\n",
        "                A1 = self.FC1.forward(mini_X)\n",
        "                Z1 = self.activation1.forward(A1)\n",
        "                # 2層目\n",
        "                A2 = self.FC2.forward(Z1)\n",
        "                Z2 = self.activation2.forward(A2)\n",
        "                # 3層目\n",
        "                A3 = self.FC3.forward(Z2)\n",
        "                Z3 = self.activation3.forward(A3)\n",
        "                \n",
        "                ## 逆伝播\n",
        "                dA3, loss = self.activation3.backward(Z3, mini_y)\n",
        "                dZ2 = self.FC3.backward(dA3)\n",
        "                dA2 = self.FC2.backward(dZ2)\n",
        "                dZ1 = self.FC2.backward(dA2)\n",
        "                dA1 = self.activation1.backward(dZ1)\n",
        "                dZ0 = self.FC1.backward(dA1) \n",
        "                \n",
        "            # 過程出力\n",
        "            if self.verbose:\n",
        "                ## 順伝播\n",
        "                # 1層目\n",
        "                A1 = self.FC1.forward(X)\n",
        "                Z1 = self.activation1.forward(A1)\n",
        "                # 2層目\n",
        "                A2 = self.FC2.forward(Z1)\n",
        "                Z2 = self.activation2.forward(A2)\n",
        "                # 3層目\n",
        "                A3 = self.FC3.forward(Z2)\n",
        "                Z3 = self.activation3.forward(A3)      \n",
        "                # 損失計算と記録\n",
        "                loss = self.activation3.backward(Z3, y)[1]\n",
        "                self.loss_train.append(loss)\n",
        "                print('epoch:%d train_loss:%f'%(i,loss))\n",
        "                # 評価データ見る\n",
        "                if X_val is not None:\n",
        "                    ## 順伝播\n",
        "                    # 1層目\n",
        "                    A1 = self.FC1.forward(X_val)\n",
        "                    Z1 = self.activation1.forward(A1)\n",
        "                    # 2層目\n",
        "                    A2 = self.FC2.forward(Z1)\n",
        "                    Z2 = self.activation2.forward(A2)\n",
        "                    # 3層目\n",
        "                    A3 = self.FC3.forward(Z2)\n",
        "                    Z3 = self.activation3.forward(A3)\n",
        "                    # 損失計算と記録\n",
        "                    self.loss_val.append(self.activation3.backward(Z3, y_val)[1])\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"予測\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 入力配列\n",
        "        \"\"\"\n",
        "        ## 順伝播\n",
        "        # 1層目\n",
        "        A1 = self.FC1.forward(X)\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        # 2層目\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        # 3層目\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        # 最も大きいインデックスを採用\n",
        "        return np.argmax(Z3, axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIYf8J9VoWd"
      },
      "source": [
        "# 3.検証\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gta8ReeRVsP3"
      },
      "source": [
        "**【問題9】学習と推定**\n",
        "\n",
        "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "5yGnw1rhpoy9",
        "outputId": "17d8073c-fb60-4b31-a29e-04b43c61d813"
      },
      "source": [
        "dnn = ScratchDeepNeuralNetrowkClassifier(epoch=100) \n",
        "\n",
        "dnn.fit(X_train[:4000], y_train_one_hot[:4000], X_val[:2000], y_test_one_hot[:2000])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-0e7fc08d41d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScratchDeepNeuralNetrowkClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_one_hot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-d940d25dace0>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mdZ2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mdA2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mdZ1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mdA1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mdZ0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-184f989fe601>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dA)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mdA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# 逆伝播させる値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdA\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;31m# 重み更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 200 is different from 400)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46jTDprqpv3Y"
      },
      "source": [
        "pred = dnn.predict(X_val)\n",
        "accuracy_score(y_val, pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TIUp5PEp0CW"
      },
      "source": [
        "plt.plot(list(range(1, dnn.epoch+1)), dnn.loss_train, label='train')\n",
        "plt.plot(list(range(1, dnn.epoch+1)), dnn.loss_val, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(list(range(1, dnn.epoch+1)));"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}