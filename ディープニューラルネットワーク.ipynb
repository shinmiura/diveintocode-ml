{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ディープニューラルネットワーク.ipynb",
      "provenance": [],
      "mount_file_id": "19AaF3Yjrhb9dA780zlfCK1NwJPfFSD2u",
      "authorship_tag": "ABX9TyPO1T4KX5AtDuxZuxfonNb1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinmiura/diveintocode-ml/blob/master/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWedC1-SPWca"
      },
      "source": [
        "**Sprintの目的**\n",
        "\n",
        "スクラッチを通してニューラルネットワークの発展的内容を理解する"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rmoTy_zPVZ_"
      },
      "source": [
        "**どのように学ぶか**\n",
        "\n",
        "スクラッチで作成したニューラルネットワークの実装を拡張していきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAg8sxAWUyM_"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJv5V0b0WplF",
        "outputId": "0cb85850-83ab-49e5-cb18-cf50d57663a4"
      },
      "source": [
        "# 読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 画像データ→行データ\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# 正規化\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# 分割(訓練データ・評価データ)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "\n",
        "# one-hotベクトル化\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_val[:, np.newaxis])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4ZW02ooPl4d"
      },
      "source": [
        "# 2.ディープニューラルネットワークスクラッチ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3kNexWCPvbD"
      },
      "source": [
        "前回は3層のニューラルネットワークを作成しましたが、今回はこれを任意の層数に拡張しやすいものに書き換えていきます。その上で、活性化関数や初期値、最適化手法について発展的なものを扱えるようにしていきます。\n",
        "\n",
        "\n",
        "このようなスクラッチを行うことで、今後各種フレームワークを利用していくにあたり、内部の動きが想像できることを目指します。\n",
        "\n",
        "\n",
        "名前は新しくScratchDeepNeuralNetrowkClassifierクラスとしてください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifYccD8WPzp2"
      },
      "source": [
        "**層などのクラス化**\n",
        "\n",
        "クラスにまとめて行くことで、構成を変更しやすい実装にしていきます。\n",
        "\n",
        "\n",
        "***手を加える箇所***\n",
        "\n",
        "\n",
        "層の数\n",
        "層の種類（今後畳み込み層など他のタイプの層が登場する）\n",
        "活性化関数の種類\n",
        "重みやバイアスの初期化方法\n",
        "最適化手法\n",
        "\n",
        "そのために、全結合層、各種活性化関数、重みやバイアスの初期化、最適化手法それぞれのクラスを作成します。\n",
        "\n",
        "\n",
        "実装方法は自由ですが、簡単な例を紹介します。サンプルコード1のように全結合層と活性化関数のインスタンスを作成し、サンプルコード2,3のようにして使用します。それぞれのクラスについてはこのあと解説します。\n",
        "\n",
        "《サンプルコード1》\n",
        "\n",
        "\n",
        "ScratchDeepNeuralNetrowkClassifierのfitメソッド内\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "cJDooMb4OSKw",
        "outputId": "1c0285b5-e8da-4347-e609-7a4a5df9f32f"
      },
      "source": [
        "# self.sigma : ガウス分布の標準偏差\n",
        "# self.lr : 学習率\n",
        "# self.n_nodes1 : 1層目のノード数\n",
        "# self.n_nodes2 : 2層目のノード数\n",
        "# self.n_output : 出力層のノード数\n",
        "\n",
        "optimizer = SGD(self.lr)\n",
        "self.FC1 = FC(self.n_features, self.n_nodes1, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation1 = Tanh()\n",
        "self.FC2 = FC(self.n_nodes1, self.n_nodes2, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation2 = Tanh()\n",
        "self.FC3 = FC(self.n_nodes2, self.n_output, SimpleInitializer(self.sigma), optimizer)\n",
        "self.activation3 = Softmax()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-a204247732d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# self.n_output : 出力層のノード数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFC1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_nodes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSimpleInitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'SGD' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcwGKixgQJeR"
      },
      "source": [
        "《サンプルコード2》\n",
        "\n",
        "\n",
        "イテレーションごとのフォワード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh3dgEvNQQgI"
      },
      "source": [
        "A1 = self.FC1.forward(X)\n",
        "Z1 = self.activation1.forward(A1)\n",
        "A2 = self.FC2.forward(Z1)\n",
        "Z2 = self.activation2.forward(A2)\n",
        "A3 = self.FC3.forward(Z2)\n",
        "Z3 = self.activation3.forward(A3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-MQrCtEQS4B"
      },
      "source": [
        "《サンプルコード3》\n",
        "\n",
        "\n",
        "イテレーションごとのバックワード\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDlC-tuUQW1D"
      },
      "source": [
        "dA3 = self.activation3.backward(Z3, Y) # 交差エントロピー誤差とソフトマックスを合わせている\n",
        "dZ2 = self.FC3.backward(dA3)\n",
        "dA2 = self.activation2.backward(dZ2)\n",
        "dZ1 = self.FC2.backward(dA2)\n",
        "dA1 = self.activation1.backward(dZ1)\n",
        "dZ0 = self.FC1.backward(dA1) # dZ0は使用しない"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_ZfdKDgQe9W"
      },
      "source": [
        "**【問題1】全結合層のクラス化**\n",
        "\n",
        "全結合層のクラス化を行なってください。\n",
        "\n",
        "\n",
        "以下に雛形を載せました。コンストラクタで重みやバイアスの初期化をして、あとはフォワードとバックワードのメソッドを用意します。重みW、バイアスB、およびフォワード時の入力Xをインスタンス変数として保持しておくことで、煩雑な入出力は不要になります。\n",
        "\n",
        "\n",
        "なお、インスタンスも引数として渡すことができます。そのため、初期化方法のインスタンスinitializerをコンストラクタで受け取れば、それにより初期化が行われます。渡すインスタンスを変えれば、初期化方法が変えられます。\n",
        "\n",
        "\n",
        "また、引数として自身のインスタンスselfを渡すこともできます。これを利用してself.optimizer.update(self)という風に層の重みの更新が可能です。更新に必要な値は複数ありますが、すべて全結合層が持つインスタンス変数にすることができます。\n",
        "\n",
        "\n",
        "初期化方法と最適化手法のクラスについては後述します。\n",
        "\n",
        "\n",
        "《雛形》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urilqsyxQwTs"
      },
      "source": [
        "class FC:\n",
        "    \"\"\"\n",
        "    ノード数n_nodes1からn_nodes2への全結合層\n",
        "    \"\"\"\n",
        "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        initializer : 初期化方法のインスタンス\n",
        "        optimizer : 最適化手法のインスタンス\n",
        "        \"\"\"\n",
        "        self.n_nodes1 = n_nodes1\n",
        "        self.n_nodes2 = n_nodes2\n",
        "        # 初期化インスタンスの関数実行\n",
        "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
        "        self.B = initializer.B(self.n_nodes2)\n",
        "        # 最適化インスタンス\n",
        "        self.optimizer = optimizer\n",
        "        # 勾配更新の際に使用(AdaGradのみ)\n",
        "        self.HW = 0\n",
        "        self.HB = 0\n",
        "  \n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        フォワード\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
        "            入力\n",
        "        Returns\n",
        "        ----------\n",
        "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
        "            出力\n",
        "        \"\"\"        \n",
        "        # 逆伝播時に使用\n",
        "        self.Z = X\n",
        "        # 順伝播計算部分本体\n",
        "        self.A = X @ self.W + self.B\n",
        "        return self.A\n",
        "\n",
        "    def backward(self, dA):\n",
        "        \"\"\"\n",
        "        dA : 前の層から逆伝播してきた値（活性化関数の逆伝播の値が入ってくる）\n",
        "        \n",
        "        Overview\n",
        "        ----------\n",
        "        前回のSprint9 ニューラルネットワークでは下記のような逆伝播処理になっていた\n",
        "            0 ## 2層目\n",
        "            1 dZ2 = dA3 @ self.W3.T\n",
        "            2 dA2 = dZ2 * (1 - self.tanh_function(self.A2)**2)\n",
        "            3 dW2 = self.Z1.T @ dA2\n",
        "            4 dB2 = np.sum(dA2, axis=0)\n",
        "            5 ## 1層目\n",
        "            6 dZ1 = dA2 @ self.W2.T\n",
        "            7 dA1 = dZ1 * (1 - self.tanh_function(self.A1)**2)\n",
        "            8 dW1 = X.T @ dA1\n",
        "            9 dB1 = np.sum(dA1, axis=0)\n",
        "        勾配の計算\n",
        "            ここでは、活性化関数の逆伝播は別で実装し、その値をこの関数の引数として受け取っているので、\n",
        "            この関数の  dA  は、Sprint9の上記の  dA2  に該当する\n",
        "            よって、上記3,4に該当する処理を書いていけばいい\n",
        "        逆伝播の値の計算\n",
        "            活性化関数の逆伝播に渡してやる値、つまりSprint9の上記の  dZ1  に該当する\n",
        "        重み更新\n",
        "            勾配dB,dWが計算されているので、このインスタンス自身をoptimizerインスタンスのupdate関数に渡してやる\n",
        "            optimizerインスタンスのupdate関数の引数は、layerとなっているので、update関数内では、layer.変数名で\n",
        "            このインスタンスの各種メンバ変数にアクセスできる\n",
        "        \"\"\"\n",
        "        # バイアス項の勾配\n",
        "        self.dB = np.sum(dA, axis=0)\n",
        "        # バイアス項以外の勾配\n",
        "        self.dW = self.Z.T @ dA\n",
        "        # 逆伝播させる値\n",
        "        self.dZ = dA @ self.W.T\n",
        "        # 重み更新\n",
        "        self = self.optimizer.update(self)\n",
        "        return self.dZ"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_bHJMzKQ06V"
      },
      "source": [
        "**【問題2】初期化方法のクラス化**\n",
        "\n",
        "初期化を行うコードをクラス化してください。\n",
        "\n",
        "\n",
        "前述のように、全結合層のコンストラクタに初期化方法のインスタンスを渡せるようにします。以下の雛形に必要なコードを書き加えていってください。標準偏差の値（sigma）はコンストラクタで受け取るようにすることで、全結合層のクラス内にこの値（sigma）を渡さなくてすむようになります。\n",
        "\n",
        "\n",
        "これまで扱ってきた初期化方法はSimpleInitializerクラスと名付けることにします。\n",
        "《雛形》"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlKvWXCeQ_Rz"
      },
      "source": [
        "class SimpleInitializer:\n",
        "    \"\"\"\n",
        "    ガウス分布によるシンプルな初期化\n",
        "    \"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ------------\n",
        "        sigma : \n",
        "        　　　　重みの初期化の際のガウス分布の標準偏差\n",
        "        \"\"\"\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : int\n",
        "          前の層のノード数\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        \"\"\"\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "\n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"\n",
        "        コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : int\n",
        "          後の層のノード数\n",
        "        Returns\n",
        "        ----------\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvgiPsN8RDgY"
      },
      "source": [
        "**【問題3】最適化手法のクラス化**\n",
        "\n",
        "最適化手法のクラス化を行なってください。\n",
        "\n",
        "\n",
        "最適化手法に関しても初期化方法同様に全結合層にインスタンスとして渡します。バックワードのときにself.optimizer.update(self)のように更新できるようにします。以下の雛形に必要なコードを書き加えていってください。\n",
        "\n",
        "\n",
        "これまで扱ってきた最適化手法はSGDクラス（Stochastic Gradient Descent、確率的勾配降下法）として作成します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29S41GUIRLju"
      },
      "source": [
        "# 雛形\n",
        "class SGD:\n",
        "    \"\"\"\n",
        "    確率的勾配降下法\n",
        "    Parameters\n",
        "    ----------\n",
        "    lr : 学習率\n",
        "    \"\"\"\n",
        "    def __init__(self, lr):\n",
        "        self.lr = lr\n",
        "\n",
        "    def update(self, layer):\n",
        "        \"\"\"\n",
        "        ある層の重みやバイアスの更新\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : 更新前の層のインスタンス\n",
        "\n",
        "        Overview\n",
        "        FCクラス内のbackward関数内で下記の要に実行されている\n",
        "            0 self.optimizer.update(self)\n",
        "        引数に「FCクラスのインスタンス自身」が入っていることを考えると\n",
        "        layer.dW , layer.dB , layer.Z は「FCクラスのインスタンスのメンバ変数」にアクセスしていると考えられる              \n",
        "        \"\"\"\n",
        "        layer.W -= self.lr * layer.dW / len(layer.Z)\n",
        "        layer.B -= self.lr * layer.dB / len(layer.Z)\n",
        "        return layer"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifAPkW3iRWdH"
      },
      "source": [
        "**【問題4】活性化関数のクラス化**\n",
        "\n",
        "活性化関数のクラス化を行なってください。\n",
        "\n",
        "\n",
        "ソフトマックス関数のバックプロパゲーションには交差エントロピー誤差の計算も含む実装を行うことで計算が簡略化されます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLdtJcDUoqP-"
      },
      "source": [
        "class Sigmoid:\n",
        "    \"\"\"シグモイド関数\"\"\"\n",
        "    def forward(self, A):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        Z = 1 / (1 + np.exp(-self.A))\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 逆伝播されてきた値\n",
        "        \"\"\"\n",
        "        dA = dZ * ((1 / (1 + np.exp(-self.A))) - (1 / (1 + np.exp(-self.A)))**2)\n",
        "        return dA"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFpVgmH-o9QB"
      },
      "source": [
        "class Tanh:\n",
        "    \"\"\"tanh関数\"\"\"\n",
        "    def forward(self, A):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        Z = np.tanh(self.A)\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 逆伝播されてきた値\n",
        "        \"\"\"\n",
        "        dA = dZ * (1 - np.tanh(self.A)**2)\n",
        "        return dA"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efv8Dc2JpAa7"
      },
      "source": [
        "class Softmax:\n",
        "    \"\"\"Softmax関数\"\"\"\n",
        "    def forward(self, A): \n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n",
        "        return Z\n",
        "        \n",
        "    def backward(self, Z, y):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        Z : 出力値\n",
        "        y : 正解データ\n",
        "        \"\"\"\n",
        "        # 逆伝播の値\n",
        "        dA = Z - y\n",
        "        # 損失\n",
        "        loss = - np.sum(y * np.log(Z)) / len(y)\n",
        "        return dA, loss"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7GlxB2cTJAp"
      },
      "source": [
        "**発展的要素**\n",
        "\n",
        "活性化関数や重みの初期値、最適化手法に関してこれまで見てきた以外のものを実装していきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC0pEx6cTQL_"
      },
      "source": [
        "**【問題5】ReLUクラスの作成**\n",
        "\n",
        "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。\n",
        "\n",
        "\n",
        "ReLUは以下の数式です。\n",
        "\n",
        "$$\n",
        "f(x) = ReLU(x) = \\begin{cases}\n",
        "x  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$x$ : ある特徴量。スカラー\n",
        "\n",
        "\n",
        "実装上はnp.maximumを使い配列に対してまとめて計算が可能です。\n",
        "\n",
        "\n",
        "numpy.maximum — NumPy v1.15 Manual\n",
        "\n",
        "\n",
        "一方、バックプロパゲーションのための $x$ に関する $f(x)$ の微分は以下のようになります。\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f(x)}{\\partial x} = \\begin{cases}\n",
        "1  & \\text{if $x>0$,}\\\\\n",
        "0 & \\text{if $x\\leqq0$.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "数学的には微分可能ではないですが、 $x=0$ のとき $0$ とすることで対応しています。\n",
        "\n",
        "\n",
        "フォワード時の $x$ の正負により、勾配を逆伝播するかどうかが決まるということになります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkvATh4YpKq5"
      },
      "source": [
        "class ReLU:\n",
        "    \"\"\"ReLU関数\"\"\"\n",
        "    def forward(self, A):\n",
        "        \"\"\"順伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        A : 順伝播されてきた値\n",
        "        \"\"\"\n",
        "        self.A = A\n",
        "        Z = np.maximum(0, A)\n",
        "        return Z\n",
        "    \n",
        "    def backward(self, dZ):\n",
        "        \"\"\"逆伝播\n",
        "        Parameters\n",
        "        ----------\n",
        "        dZ : 逆伝播されてきた値\n",
        "        \"\"\"\n",
        "        dA = dZ * np.where(self.A > 0, 1, 0)\n",
        "        return dA"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3YduTcTT9DJ"
      },
      "source": [
        "**【問題6】重みの初期値**\n",
        "\n",
        "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。\n",
        "\n",
        "\n",
        "XavierInitializerクラスと、HeInitializerクラスを作成してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taks2WXmUCDh"
      },
      "source": [
        "**Xavierの初期値**\n",
        "\n",
        "Xavierの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
        "$$\n",
        "\\sigma = \\frac{1}{\\sqrt{n}}\n",
        "$$\n",
        "\n",
        "$n$ : 前の層のノード数\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "Glorot, X., & Bengio, Y. (n.d.). Understanding the difficulty of training deep feedforward neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ky2iNjrHUqA1"
      },
      "source": [
        "Heの初期値\n",
        "\n",
        "Heの初期値における標準偏差 $\\sigma$ は次の式で求められます。\n",
        "$$\n",
        "\\sigma = \\sqrt{\\frac{2}{n}}\n",
        "$$\n",
        "\n",
        "$n$ : 前の層のノード数\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVKDIw-6pSX6"
      },
      "source": [
        "class XavierInitializer:\n",
        "    \"\"\"Xavierの初期化クラス\"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        sigma : 使用されていない\n",
        "        \n",
        "        Overview\n",
        "        ----------\n",
        "        なぜ使用されていないのに、引数として受け取っているのか\n",
        "        \n",
        "        初期化クラスは、概略すると、下記のように使用されている\n",
        "        \n",
        "        呼び出しの大元\n",
        "            dnn = ScratchDeepNeuralNetrowkClassifier(initializer=SGD or XavierInitializer or HeInitializer) \n",
        "            \n",
        "        定義部分(ScratchDeepNeuralNetrowkClassifier)\n",
        "            class ScratchDeepNeuralNetrowkClassifier:\n",
        "                def __init__(self,xxx,xxx,xxx,initializer):\n",
        "                    .....\n",
        "                    self.initializer = initializer\n",
        "                    .....\n",
        "                def fit(self,xxx,xxx,xxx):\n",
        "                    self.initializer(self.sigma)\n",
        "        \n",
        "        つまり、\n",
        "        「呼び出しの大元」で、どの初期化クラスが渡されるかわからないので、\n",
        "        初期化クラスによっては、sigmaが必要なものと、別途計算が必要なものがあるので、\n",
        "        同じ呼び出し方をしてやるために、この初期かクラスのコンストラクタでも引数として、sigmaを受け取っている\n",
        "        \n",
        "        同じ呼び出し方をしてやらないのであれば、上記fitは、下記のような書き方でも可能\n",
        "            if initializerのクラス名 == \"SGD\":\n",
        "                self.initializer(self.sigma)\n",
        "            else initializerのクラス名 != \"SGD\":\n",
        "                self.initializer()\n",
        "        \"\"\"\n",
        "        _ = sigma\n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : 前の層のノード数\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        self.sigma = 1 / np.sqrt(n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "        \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwRlQfepU2Z"
      },
      "source": [
        "class HeInitializer:\n",
        "    \"\"\"Heの初期化クラス\"\"\"\n",
        "    def __init__(self, sigma):\n",
        "        _ = sigma\n",
        "        \n",
        "    def W(self, n_nodes1, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes1 : 前の層のノード数\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        self.sigma = np.sqrt(2 / n_nodes1)\n",
        "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
        "        return W\n",
        "    \n",
        "    def B(self, n_nodes2):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_nodes2 : 当該層のノード数\n",
        "        \"\"\"\n",
        "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
        "        return B"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUtrUna4U76E"
      },
      "source": [
        "**【問題7】最適化手法**\n",
        "\n",
        "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。\n",
        "\n",
        "\n",
        "まず、これまで使ってきたSGDを確認します。\n",
        "\n",
        "$$\n",
        "W_i^{\\prime} = W_i - \\alpha \\frac{\\partial L}{\\partial W_i} \\\\\n",
        "B_i^{\\prime} = B_i - \\alpha \\frac{\\partial L}{\\partial B_i}\n",
        "$$\n",
        "\n",
        "$\\alpha$ : 学習率（層ごとに変えることも可能だが、基本的にはすべて同じとする）\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W_i}$ : $W_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "$\\frac{\\partial L}{\\partial B_i}$ : $B_i$ に関する損失 $L$ の勾配\n",
        "\n",
        "\n",
        "続いて、AdaGradです。バイアスの数式は省略しますが、重みと同様のことをします。\n",
        "\n",
        "\n",
        "更新された分だけその重みに対する学習率を徐々に下げていきます。イテレーションごとの勾配の二乗和 $H$ を保存しておき、その分だけ学習率を小さくします。\n",
        "\n",
        "\n",
        "学習率は重み一つひとつに対して異なることになります。\n",
        "\n",
        "\n",
        "$$\n",
        "H_i^{\\prime} = H_i + \\frac{\\partial L}{\\partial W_i} \\odot \\frac{\\partial L}{\\partial W_i} \\\\\n",
        "W_i^{\\prime} = W_i - \\alpha \\left( \\frac{1}{\\sqrt{H_i^{\\prime}}} \\odot \\frac{\\partial L}{\\partial W_i} \\right)\n",
        "$$\n",
        "\n",
        "$H_i$ : i層目に関して、前のイテレーションまでの勾配の二乗和（初期値は0）\n",
        "\n",
        "\n",
        "$H_i^{\\prime}$ : 更新した $H_i$\n",
        "\n",
        "\n",
        "《論文》\n",
        "\n",
        "\n",
        "Duchi JDUCHI, J., & Singer, Y. (2011). Adaptive Subgradient Methods for Online Learning and Stochastic Optimization * Elad Hazan. Journal of Machine Learning Research (Vol. 12)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOF2Zot8pek6"
      },
      "source": [
        "class AdaGrad:\n",
        "    \"\"\"最適化手法（AdaGrad）\"\"\"\n",
        "    def __init__(self, lr):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        lr : 学習率\n",
        "        \"\"\"\n",
        "        self.lr = lr \n",
        "    \n",
        "    def update(self, layer):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        layer : layerインスタンス\n",
        "        \"\"\"\n",
        "        layer.HW += layer.dW * layer.dW\n",
        "        layer.HB += layer.dB * layer.dB\n",
        "        delta = 1e-7 # 0割エラー防止のため\n",
        "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + delta) / len(layer.Z)\n",
        "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.HB) + delta) / len(layer.Z)\n",
        "        return layer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3beIDX9Vjfn"
      },
      "source": [
        "**【問題8】クラスの完成**\n",
        "\n",
        "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjE-agETpjMr"
      },
      "source": [
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        \"\"\"通常のコンストラクタと同様の働き\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 説明変数（画像の1次元データ）\n",
        "        y : 目的変数（ラベル）\n",
        "        batch_size : 必要なミニバッチのデータ数\n",
        "        seed : ランダムシード固定\n",
        "        \"\"\"\n",
        "        # ランダムシードの固定（学習ごとに同じ生成順）\n",
        "        np.random.seed(seed)\n",
        "        # バッチ数のメンバ変数\n",
        "        self.batch_size = batch_size\n",
        "        # データ全体の長さ分のインデックスをランダムに並べ替え\n",
        "        # np.random.permutation:配列をランダムに並べ替え\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        # 並べ替えたインデックスと同じ順番で説明変数と目的変数を並べ替え\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        # データ数をバッチ数で割って、何回呼び出せば、全データを学習したことになるかの判定\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    \n",
        "    def __iter__(self):\n",
        "        # 何回目の呼び出しか\n",
        "        self._counter = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # 全データを学習すればストップ\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        # 並び変えた_X,_yの何番目のインデックスを採用するか\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        # returnする前にカウンタに+1しておく\n",
        "        self._counter += 1\n",
        "        # 説明変数と目的変数を返す\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAdJkCDipnqb"
      },
      "source": [
        "class ScratchDeepNeuralNetrowkClassifier():\n",
        "\n",
        "    def __init__(self,batch_size=20,n_features=784,n_nodes1 =400,n_nodes2 = 200,n_output =10,lr =0.005,epoch=10,sigma=0.02,optimizer=SGD, initializer=HeInitializer,activater=ReLU,output_activater=Softmax,verbose=True):\n",
        "        \"\"\"コンストラクタ\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : バッチサイズ（default:20)\n",
        "        n_features : 説明変数の数（default:784)\n",
        "        n_nodes1 : 前の層のノード数（default:400)\n",
        "        n_nodes2 : 当該層のノード数（default:200)\n",
        "        n_output : 出力層のノード数（default:10)\n",
        "        sigma : 初期化時のパラメータ（default:0.02)\n",
        "        lr : 学習率（default:0.005)\n",
        "        verbose : 計算過程の出力（default:True)\n",
        "        epoch : 学習回数（default:10)\n",
        "        optimizer : 最適化手法（default:SGD)\n",
        "        initializer : 初期化方法（default:HeInitializer）\n",
        "        activater : 活性化関数（default:ReLU）\n",
        "        \"\"\"\n",
        "        self.batch_size = batch_size\n",
        "        self.n_features = n_features \n",
        "        self.n_nodes1 = n_nodes1  \n",
        "        self.n_nodes2 = n_nodes2 \n",
        "        self.n_output = n_output\n",
        "        self.lr = lr\n",
        "        self.epoch = epoch\n",
        "        self.optimizer = optimizer \n",
        "        self.sigma = sigma\n",
        "        self.initializer = initializer \n",
        "        self.activater = activater\n",
        "        self.output_activater = output_activater \n",
        "        self.verbose = verbose\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"学習\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 訓練データの説明変数\n",
        "        y : 訓練データの目的変数\n",
        "        X_val : 評価データの説明変数\n",
        "        y_val : 評価データの目的変数\n",
        "        \"\"\"\n",
        "        # lossの記録用配列\n",
        "        self.loss_train = [] \n",
        "        self.loss_val = [] \n",
        "        # 最適化手法の初期化\n",
        "        optimizer = self.optimizer(self.lr)\n",
        "        # 各層の初期化\n",
        "        self.FC1 = FC(self.n_features, self.n_nodes1, self.initializer(self.sigma), optimizer)\n",
        "        self.activation1 = self.activater()\n",
        "        self.FC2 = FC(self.n_nodes1, self.n_nodes2, self.initializer(self.sigma), optimizer)\n",
        "        self.activation2 = self.activater()\n",
        "        self.FC3 = FC(self.n_nodes2, self.n_output, self.initializer(self.sigma), optimizer)\n",
        "        self.activation3 = self.output_activater()\n",
        "        \n",
        "        # 学習回数分ループ\n",
        "        for i in range(self.epoch):\n",
        "            # ミニバッチイテレータ生成\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=i)\n",
        "            # ミニバッチイテレータループ\n",
        "            for mini_X, mini_y in get_mini_batch:\n",
        "                ## 順伝播\n",
        "                # 1層目\n",
        "                A1 = self.FC1.forward(mini_X)\n",
        "                Z1 = self.activation1.forward(A1)\n",
        "                # 2層目\n",
        "                A2 = self.FC2.forward(Z1)\n",
        "                Z2 = self.activation2.forward(A2)\n",
        "                # 3層目\n",
        "                A3 = self.FC3.forward(Z2)\n",
        "                Z3 = self.activation3.forward(A3)\n",
        "                \n",
        "                ## 逆伝播\n",
        "                dA3, loss = self.activation3.backward(Z3, mini_y)\n",
        "                dZ2 = self.FC3.backward(dA3)\n",
        "                dA2 = self.activation2.backward(dZ2)\n",
        "                dZ1 = self.FC2.backward(dA2)\n",
        "                dA1 = self.activation1.backward(dZ1)\n",
        "                dZ0 = self.FC1.backward(dA1) \n",
        "                \n",
        "            # 過程出力\n",
        "            if self.verbose:\n",
        "                ## 順伝播\n",
        "                # 1層目\n",
        "                A1 = self.FC1.forward(X)\n",
        "                Z1 = self.activation1.forward(A1)\n",
        "                # 2層目\n",
        "                A2 = self.FC2.forward(Z1)\n",
        "                Z2 = self.activation2.forward(A2)\n",
        "                # 3層目\n",
        "                A3 = self.FC3.forward(Z2)\n",
        "                Z3 = self.activation3.forward(A3)      \n",
        "                # 損失計算と記録\n",
        "                loss = self.activation3.backward(Z3, y)[1]\n",
        "                self.loss_train.append(loss)\n",
        "                print('epoch:%d train_loss:%f'%(i,loss))\n",
        "                # 評価データ見る\n",
        "                if X_val is not None:\n",
        "                    ## 順伝播\n",
        "                    # 1層目\n",
        "                    A1 = self.FC1.forward(X_val)\n",
        "                    Z1 = self.activation1.forward(A1)\n",
        "                    # 2層目\n",
        "                    A2 = self.FC2.forward(Z1)\n",
        "                    Z2 = self.activation2.forward(A2)\n",
        "                    # 3層目\n",
        "                    A3 = self.FC3.forward(Z2)\n",
        "                    Z3 = self.activation3.forward(A3)\n",
        "                    # 損失計算と記録\n",
        "                    self.loss_val.append(self.activation3.backward(Z3, y_val)[1])\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"予測\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 入力配列\n",
        "        \"\"\"\n",
        "        ## 順伝播\n",
        "        # 1層目\n",
        "        A1 = self.FC1.forward(X)\n",
        "        Z1 = self.activation1.forward(A1)\n",
        "        # 2層目\n",
        "        A2 = self.FC2.forward(Z1)\n",
        "        Z2 = self.activation2.forward(A2)\n",
        "        # 3層目\n",
        "        A3 = self.FC3.forward(Z2)\n",
        "        Z3 = self.activation3.forward(A3)\n",
        "        # 最も大きいインデックスを採用\n",
        "        return np.argmax(Z3, axis=1)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omIYf8J9VoWd"
      },
      "source": [
        "# 3.検証\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gta8ReeRVsP3"
      },
      "source": [
        "**【問題9】学習と推定**\n",
        "\n",
        "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yGnw1rhpoy9",
        "outputId": "f81599ac-b183-4ed3-8306-2bc06b462016"
      },
      "source": [
        "dnn = ScratchDeepNeuralNetrowkClassifier(epoch=100) \n",
        "\n",
        "dnn.fit(X_train[:4000], y_train_one_hot[:4000], X_val[:2000], y_test_one_hot[:2000])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 train_loss:1.355044\n",
            "epoch:1 train_loss:0.833435\n",
            "epoch:2 train_loss:0.617198\n",
            "epoch:3 train_loss:0.505132\n",
            "epoch:4 train_loss:0.440820\n",
            "epoch:5 train_loss:0.398909\n",
            "epoch:6 train_loss:0.368104\n",
            "epoch:7 train_loss:0.338458\n",
            "epoch:8 train_loss:0.318811\n",
            "epoch:9 train_loss:0.301731\n",
            "epoch:10 train_loss:0.286110\n",
            "epoch:11 train_loss:0.275386\n",
            "epoch:12 train_loss:0.265225\n",
            "epoch:13 train_loss:0.257080\n",
            "epoch:14 train_loss:0.241316\n",
            "epoch:15 train_loss:0.232369\n",
            "epoch:16 train_loss:0.225370\n",
            "epoch:17 train_loss:0.217288\n",
            "epoch:18 train_loss:0.209554\n",
            "epoch:19 train_loss:0.202682\n",
            "epoch:20 train_loss:0.198153\n",
            "epoch:21 train_loss:0.189635\n",
            "epoch:22 train_loss:0.186034\n",
            "epoch:23 train_loss:0.179473\n",
            "epoch:24 train_loss:0.175927\n",
            "epoch:25 train_loss:0.167212\n",
            "epoch:26 train_loss:0.162480\n",
            "epoch:27 train_loss:0.158116\n",
            "epoch:28 train_loss:0.152473\n",
            "epoch:29 train_loss:0.149652\n",
            "epoch:30 train_loss:0.146539\n",
            "epoch:31 train_loss:0.142614\n",
            "epoch:32 train_loss:0.137945\n",
            "epoch:33 train_loss:0.133693\n",
            "epoch:34 train_loss:0.132022\n",
            "epoch:35 train_loss:0.125507\n",
            "epoch:36 train_loss:0.122558\n",
            "epoch:37 train_loss:0.117954\n",
            "epoch:38 train_loss:0.116274\n",
            "epoch:39 train_loss:0.112597\n",
            "epoch:40 train_loss:0.109020\n",
            "epoch:41 train_loss:0.106271\n",
            "epoch:42 train_loss:0.105043\n",
            "epoch:43 train_loss:0.099996\n",
            "epoch:44 train_loss:0.097860\n",
            "epoch:45 train_loss:0.095014\n",
            "epoch:46 train_loss:0.092891\n",
            "epoch:47 train_loss:0.090115\n",
            "epoch:48 train_loss:0.088944\n",
            "epoch:49 train_loss:0.086252\n",
            "epoch:50 train_loss:0.083937\n",
            "epoch:51 train_loss:0.081819\n",
            "epoch:52 train_loss:0.078502\n",
            "epoch:53 train_loss:0.077048\n",
            "epoch:54 train_loss:0.075828\n",
            "epoch:55 train_loss:0.073222\n",
            "epoch:56 train_loss:0.072479\n",
            "epoch:57 train_loss:0.070031\n",
            "epoch:58 train_loss:0.068301\n",
            "epoch:59 train_loss:0.066299\n",
            "epoch:60 train_loss:0.064643\n",
            "epoch:61 train_loss:0.062086\n",
            "epoch:62 train_loss:0.060677\n",
            "epoch:63 train_loss:0.059765\n",
            "epoch:64 train_loss:0.059322\n",
            "epoch:65 train_loss:0.056075\n",
            "epoch:66 train_loss:0.055361\n",
            "epoch:67 train_loss:0.053297\n",
            "epoch:68 train_loss:0.052511\n",
            "epoch:69 train_loss:0.050801\n",
            "epoch:70 train_loss:0.049707\n",
            "epoch:71 train_loss:0.048418\n",
            "epoch:72 train_loss:0.047386\n",
            "epoch:73 train_loss:0.046396\n",
            "epoch:74 train_loss:0.044875\n",
            "epoch:75 train_loss:0.044120\n",
            "epoch:76 train_loss:0.042639\n",
            "epoch:77 train_loss:0.042409\n",
            "epoch:78 train_loss:0.040627\n",
            "epoch:79 train_loss:0.040285\n",
            "epoch:80 train_loss:0.039862\n",
            "epoch:81 train_loss:0.038374\n",
            "epoch:82 train_loss:0.037236\n",
            "epoch:83 train_loss:0.036680\n",
            "epoch:84 train_loss:0.035527\n",
            "epoch:85 train_loss:0.035150\n",
            "epoch:86 train_loss:0.034397\n",
            "epoch:87 train_loss:0.033396\n",
            "epoch:88 train_loss:0.032605\n",
            "epoch:89 train_loss:0.032173\n",
            "epoch:90 train_loss:0.032041\n",
            "epoch:91 train_loss:0.030678\n",
            "epoch:92 train_loss:0.029991\n",
            "epoch:93 train_loss:0.030340\n",
            "epoch:94 train_loss:0.028850\n",
            "epoch:95 train_loss:0.028571\n",
            "epoch:96 train_loss:0.027781\n",
            "epoch:97 train_loss:0.027098\n",
            "epoch:98 train_loss:0.026707\n",
            "epoch:99 train_loss:0.026760\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46jTDprqpv3Y",
        "outputId": "4ab11c8a-0547-42aa-c12e-d16e40dd1636"
      },
      "source": [
        "pred = dnn.predict(X_val)\n",
        "accuracy_score(y_val, pred)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9281666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "9TIUp5PEp0CW",
        "outputId": "2339778b-3a77-4f39-b879-ce534c9850ff"
      },
      "source": [
        "plt.plot(list(range(1, dnn.epoch+1)), dnn.loss_train, label='train')\n",
        "plt.plot(list(range(1, dnn.epoch+1)), dnn.loss_val, label='test')\n",
        "plt.legend()\n",
        "plt.xticks(list(range(1, dnn.epoch+1)));"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}