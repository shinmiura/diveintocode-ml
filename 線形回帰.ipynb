{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "線形回帰.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "11qWNCja6i975mlyamtqVt3J0vDCJjLnY",
      "authorship_tag": "ABX9TyMNX9PD3R2e5CM8wSszyJb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinmiura/diveintocode-ml/blob/master/%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7C0KBIglTfh"
      },
      "source": [
        "# 1.このSprintについて\n",
        "\n",
        "**Sprintの目的**\n",
        "\n",
        "・スクラッチを通して線形回帰を理解する\n",
        "\n",
        "・オブジェクト指向を意識した実装に慣れる\n",
        "\n",
        "・数式をコードに落とし込めるようにする\n",
        "\n",
        "**どのように学ぶか**\n",
        "\n",
        "スクラッチで線形回帰を実装した後、学習と検証を行なっていきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTzpfKsEl6yc"
      },
      "source": [
        "# 2.線形回帰スクラッチ\n",
        "\n",
        "線形回帰のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
        "\n",
        "以下に雛形を用意してあります。このScratchLinearRegressionクラスにコードを書き加えていってください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMN1MoAllKrN"
      },
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    線形回帰のスクラッチ実装\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    num_iter : int\n",
        "      イテレーション数\n",
        "    lr : float\n",
        "      学習率\n",
        "    no_bias : bool\n",
        "      バイアス項を入れない場合はTrue\n",
        "    verbose : bool\n",
        "      学習過程を出力する場合はTrue\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
        "      パラメータ\n",
        "    self.loss : 次の形のndarray, shape (self.iter,)\n",
        "      訓練データに対する損失の記録\n",
        "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
        "      検証データに対する損失の記録\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):\n",
        "        # ハイパーパラメータを属性として記録\n",
        "        self.iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "        self.loss = np.zeros(self.iter)\n",
        "        self.val_loss = np.zeros(self.iter)\n",
        "        \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証データの正解値\n",
        "        \"\"\"\n",
        "        if self.verbose:\n",
        "            #verboseをTrueにした際は学習過程を出力\n",
        "            print()\n",
        "        pass\n",
        "        \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        線形回帰を使い推定する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "        Returns\n",
        "        -------\n",
        "            次の形のndarray, shape (n_samples, 1)\n",
        "            線形回帰による推定結果\n",
        "        \"\"\"\n",
        "        a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "        X = np.hstack([a, X])\n",
        "\n",
        "        pred_y = _linear_hypothesis(X)\n",
        "        return pred_y\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QETW5jWknOmX"
      },
      "source": [
        "【問題1】仮定関数\n",
        "\n",
        "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。\n",
        "\n",
        "$$\n",
        "h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... +\\theta_n x_n  (x_0 = 1)\\\\\n",
        "$$\n",
        "\n",
        "\n",
        "$x$ : 特徴量ベクトル\n",
        "\n",
        "\n",
        "$\\theta$ : パラメータベクトル\n",
        "\n",
        "\n",
        "$n$ : 特徴量の数\n",
        "\n",
        "\n",
        "$x_j$ : j番目の特徴量\n",
        "\n",
        "\n",
        "$\\theta_j$ : j番目のパラメータ（重み）\n",
        "\n",
        "\n",
        "特徴量の数$n$は任意の値に対応できる実装にしてください。\n",
        "\n",
        "\n",
        "なお、ベクトル形式で表すと以下のようになります。\n",
        "\n",
        "\n",
        "$$\n",
        "h_\\theta(x) = \\theta^T \\cdot x.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpZkbCtFrWCA"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "from numpy.random import *\n",
        "import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xb2MSudKk-U0"
      },
      "source": [
        "# 問題６の箇所に解答しました。以下は、検算などのための記録です。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDJzLXptpH-j"
      },
      "source": [
        "# 雛形\n",
        "# クラスの外から呼び出すことがないメソッドのため、Pythonの慣例としてアンダースコアを先頭にひとつつけています。\n",
        "def _linear_hypothesis(self, X):\n",
        "    \"\"\"\n",
        "    線形の仮定関数を計算する\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    Returns\n",
        "    -------\n",
        "      次の形のndarray, shape (n_samples, 1)\n",
        "      線形の仮定関数による推定結果\n",
        "    \"\"\"\n",
        "    # x_{0} = 1のため、行列の積の計算において考慮すべく全て\n",
        "    a = np.ones(X.shape[0]).reshape(X.shape[0], 1) #１次元配列なので、reshape(Xの列数＋１（列が全て１の列）,1)により２次元配列化している\n",
        "    X = np.hstack([a, X])\n",
        "    self.Theta = np.random.ramdom_sample((X.shape[0] + 1, 1))\n",
        "    self.y = self.Theta @ X.T\n",
        "    return self.y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLL2-z02RlH3",
        "outputId": "8cba7df0-7a40-43eb-b2d4-296095509b24"
      },
      "source": [
        "X = np.arange(10).reshape(2, -1)\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 2, 3, 4],\n",
              "       [5, 6, 7, 8, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z74ezulsSV2M",
        "outputId": "72108eed-cac4-40cd-9aea-f7c9ebc31529"
      },
      "source": [
        "a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcd1uutfS0rl",
        "outputId": "765feec8-3944-478c-bfc7-ae72c0b9c350"
      },
      "source": [
        "X = np.hstack([a, X])\n",
        "X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 1., 2., 3., 4.],\n",
              "       [1., 5., 6., 7., 8., 9.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MewHxaRKwHwZ",
        "outputId": "870c7115-ff49-484b-f8e9-36e678afc3ad"
      },
      "source": [
        "theta = np.zeros(X.shape[1])\n",
        "print(theta)\n",
        "theta = theta.reshape(X.shape[1], 1)\n",
        "print(theta)\n",
        "# self.theta2 = np.zeros(X_val.shape[1])\n",
        "# self.theta2 = self.theta2.reshape(1, X_val.shpae[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0.]\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rc08XDlXYfCG",
        "outputId": "4ba7ebd4-6ef8-4299-e817-f23b38305db1"
      },
      "source": [
        "theta = np.array([])\n",
        "theta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([], dtype=float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdsegNVMpHSX"
      },
      "source": [
        "【問題2】最急降下法\n",
        "\n",
        "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、fitメソッドから呼び出すようにしてください。\n",
        "\n",
        "$$\n",
        "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
        "$$\n",
        "\n",
        "\n",
        "$\\alpha$ : 学習率\n",
        "\n",
        "\n",
        "$i$ : サンプルのインデックス\n",
        "\n",
        "\n",
        "$j$ : 特徴量のインデックス\n",
        " \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9G6uKb6llGlL"
      },
      "source": [
        "# 問題６の箇所に解答しました。以下は、検算などのための記録です。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG5n_AGF2THg"
      },
      "source": [
        "# 雛形\n",
        "# ScratchLinearRegressionクラスへ以下のメソッドを追加してください。コメントアウト部分の説明も記述してください。\n",
        "\n",
        "def _gradient_descent(self, X, error):\n",
        "    \"\"\"\n",
        "    最急降下法によるパラメータの更新\n",
        "    適当な重みを掛けてθが最小値の方に動いていくようにする\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    Returns\n",
        "    -------\n",
        "    Theta : 次の形のndarray, shape (n_samples, 1)\n",
        "      線形の仮定関数による推定結果\n",
        "    \"\"\"\n",
        "    self.Theta = self.Theta - 0.01 * ((self.error * X).sum()) / X.shape[0]\n",
        "    return Theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzp8sJTbTN6N",
        "outputId": "9c040130-6496-4b30-b0d4-77888a5d4eb5"
      },
      "source": [
        "x = np.linspace(1,6,5)\n",
        "X = np.c_[np.ones(5),x]#入力データX\n",
        "print(x)\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.   2.25 3.5  4.75 6.  ]\n",
            "[[1.   1.  ]\n",
            " [1.   2.25]\n",
            " [1.   3.5 ]\n",
            " [1.   4.75]\n",
            " [1.   6.  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC9sZEQgURIG",
        "outputId": "fd8079c4-5084-4c4d-8f42-adfa170af5fc"
      },
      "source": [
        "y = 2*x + 1#適当な真のモデル\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3. ,  5.5,  8. , 10.5, 13. ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ajGgR-aUR-K"
      },
      "source": [
        "theta = [0,0]#仮定関数の係数の初期値\n",
        "y_pred = X @ theta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yph-uI2PUhiM"
      },
      "source": [
        "error = y_pred - y#入力データerror"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afxinRz2S1D9",
        "outputId": "66a0b14f-8297-4ed8-853d-75886ad671ca"
      },
      "source": [
        "h = X @ theta\n",
        "lr = 0.05\n",
        "m = X.shape[0]\n",
        "n = X.shape[1]\n",
        "# 特徴量の数だけfor文を回す\n",
        "for j in range(n):\n",
        "  # gradientの初期値を設定\n",
        "  gradient = 0\n",
        "  # サンプル数分だけfor文を回す\n",
        "  for i in range(m):\n",
        "    gradient += (h[i] - y[i]) * X[i, j]\n",
        "    theta[j] = theta[j] - lr * (gradient / m)\n",
        "print(theta)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6661594085922242, 2.0351678661450143]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQNeaD-gq3TB"
      },
      "source": [
        "雛形として用意されたメソッドや関数以外でも必要があれば各自作成して完成させてください。雛形を外れても問題ありません。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYW1EPTKq8vg"
      },
      "source": [
        "【問題3】推定\n",
        "\n",
        "推定する仕組みを実装してください。ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。\n",
        "\n",
        "\n",
        "仮定関数 $h_\\theta(x)$ の出力が推定結果です。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMo2xQFokpdR"
      },
      "source": [
        "# 問題６の箇所に解答しました。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtugnHdHr-w4"
      },
      "source": [
        "【問題4】平均二乗誤差\n",
        "\n",
        "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。\n",
        "\n",
        "\n",
        "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。雛形を用意してあります。\n",
        "\n",
        "\n",
        "平均二乗誤差は以下の数式で表されます。\n",
        "\n",
        "$\n",
        "L(\\theta)=  \\frac{1}{m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
        "$\n",
        "\n",
        "\n",
        "\n",
        "$m$ : 入力されるデータの数\n",
        "\n",
        "\n",
        "$h_\\theta()$ : 仮定関数\n",
        "\n",
        "\n",
        "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
        "\n",
        "\n",
        "$y^{(i)}$ : i番目のサンプルの正解値\n",
        "\n",
        "\n",
        "なお、最急降下法のための目的関数（損失関数）としては、これを2で割ったものを使用します。（問題5, 9）\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRLMs0VDZZFC"
      },
      "source": [
        "〈計算順序（ヒント集より）〉\n",
        "\n",
        "①推定結果を計算\n",
        "$$\n",
        "h_\\theta(x_i) = \\theta^T \\cdot x_i\n",
        "$$\n",
        "\n",
        "②実測値との差を計算\n",
        "$$\n",
        "error_i = h_\\theta(x_i) - y_i\n",
        "$$\n",
        "\n",
        "③上記②の二乗を計算\n",
        "$$\n",
        "squared error_i = error_i^2\n",
        "$$\n",
        "\n",
        "④上記③の合計値を計算\n",
        "$$\n",
        "sum squared error = \\sum_{i=1}^{m} squared error_i\n",
        "$$\n",
        "\n",
        "⑤データの長さで割って４の平均値を計算\n",
        "$$\n",
        "mean squared error = \\sum_{i=1}^{m} squared error_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGWdCLEKs9mC"
      },
      "source": [
        "# 問題4解答（他の問題は問題6に含めて解答していますが、問題4のみこちらに記載しています。)\n",
        "def MSE(y_pred, y):\n",
        "    \"\"\"\n",
        "    平均二乗誤差の計算\n",
        "    Parameters\n",
        "    ----------\n",
        "    y_pred : 次の形のndarray, shape (n_samples,)\n",
        "      推定した値\n",
        "    y : 次の形のndarray, shape (n_samples,)\n",
        "      正解値\n",
        "    Returns\n",
        "    ----------\n",
        "    mse : numpy.float\n",
        "      平均二乗誤差\n",
        "    \"\"\"\n",
        "    error = 0\n",
        "    for i in range(len(y_pred)):\n",
        "      error += ((y_pred[i] - y[i]) ** 2)\n",
        "    # データの長さで割る(ヒント集より。合計してから割るのだからインデントはfor文と同じところまで戻す)\n",
        "    mse = error / len(y_pred)\n",
        "    return mse"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42n3QVgTzHqj",
        "outputId": "e91d3b34-bf38-4cdd-d03e-1294aa2e7e78"
      },
      "source": [
        "# 作成した関数に、下記の変数を引数として与えてみる(期待される出力は、戻り値15.166666666666666。ヒント集の数値は間違いなので気を付ける！)。\n",
        "y_pred = np.array([0,1,2,3,4,5])\n",
        "y = np.array([1,3,5,7,9,11])\n",
        "\n",
        "print('MSEは、{}'.format(MSE(y_pred, y)))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSEは、15.166666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJDLySMcie3d",
        "outputId": "4290b292-ceed-484d-9943-58c13e92379b"
      },
      "source": [
        "# 検算用\n",
        "from sklearn.metrics import mean_squared_error\n",
        "y_true = np.array([1,3,5,7,9,11])\n",
        "y_pred = np.array([0,1,2,3,4,5])\n",
        "mean_squared_error(y_true, y_pred)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.166666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtB0kuZUtZdI"
      },
      "source": [
        "【問題5】目的関数\n",
        "\n",
        "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。そして、これをself.loss, self.val_lossに記録するようにしてください。\n",
        "\n",
        "\n",
        "目的関数（損失関数） $J(\\theta)$ は次の式です。\n",
        "$$\n",
        "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
        "$$\n",
        "\n",
        "\n",
        "$m$ : 入力されるデータの数\n",
        "\n",
        "\n",
        "$h_\\theta()$ : 仮定関数\n",
        "\n",
        "\n",
        "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル\n",
        "\n",
        "\n",
        "$y^{(i)}$ : i番目のサンプルの正解値"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWZRBg_8eNmh"
      },
      "source": [
        "〈ヒント集より〉\n",
        "\n",
        "平均二乗誤差をさらに2で割った式になります。最小化させるべき目的の関数という意味で目的関数と呼ばれます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFBxT3nxnjni"
      },
      "source": [
        "# 問題６の箇所に解答しました。以下は、検算などのための記録です。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNQhNCrc5w4B"
      },
      "source": [
        "def _loss_func(y_pred, y):\n",
        "  \"\"\"\n",
        "  損失関数の計算\n",
        "  Parameters\n",
        "  ----------\n",
        "  h : \n",
        "    仮定関数\n",
        "  y : 次の形のndarray, shape (n_samples,)\n",
        "    正解値\n",
        "  Returns\n",
        "  ----------\n",
        "  lossfunc\n",
        "    損失値\n",
        "  \"\"\"\n",
        "  # yのサンプル数\n",
        "  m2 = y.shape[0]\n",
        "  error2 = 0\n",
        "  for i in range(m2):\n",
        "    error2 += ((y_pred[i] - y[i]) ** 2)\n",
        "    # (m×2)で割る(合計してから割るのだからインデントはfor文と同じところまで戻す)\n",
        "  loss = error2 / (2 * m2)\n",
        "  return loss"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XcJa9d06bqF",
        "outputId": "106ca1d8-27b9-468f-d6df-2b5f0b6d006f"
      },
      "source": [
        "y_pred = np.array([0,1,2,3,4,5])\n",
        "y = np.array([1,3,5,7,9,11])\n",
        "print(_loss_func(y_pred, y))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.583333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITGuXG_juHH8"
      },
      "source": [
        "# 3.検証\n",
        "\n",
        "【問題6】学習と推定\n",
        "\n",
        "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。\n",
        "\n",
        "\n",
        "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPd-Wvhkcjze"
      },
      "source": [
        "# 必要なライブラリの読み込み（追加）\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzjMi2uY2b7k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89e5869a-53c5-4cb2-d31c-404d25c73261"
      },
      "source": [
        "class ScratchLinearRegression():\n",
        "    \"\"\"\n",
        "    線形回帰をスクラッチで実装していく\n",
        "    Parameters\n",
        "    ---------\n",
        "    num_iter : int\n",
        "      イテレーション数\n",
        "    lr : float\n",
        "      学習率\n",
        "    no_bias : bool\n",
        "      バイアス項を入れない場合はTrue\n",
        "    verbose : bool\n",
        "      学習過程を出力する場合はTrue\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
        "      パラメータ\n",
        "    self.loss : 次の形のndarray, shape (self.iter,)\n",
        "      訓練データに対する損失の記録\n",
        "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
        "      検証データに対する損失の記録\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_iter, lr, no_bias, verbose):  # 必要に応じて引数を追加して下さい\n",
        "        \"\"\"\n",
        "        インスタンス変数初期化\n",
        "        \"\"\"\n",
        "        # ハイパーパラメータを属性として記録\n",
        "        self.num_iter = num_iter\n",
        "        self.lr = lr\n",
        "        self.no_bias = no_bias\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # θの初期化(ヒント集7より)\n",
        "        self.theta = np.ndarray([])\n",
        "        # 損失を記録するための配列を用意(訓練データ用)\n",
        "        self.loss = np.ndarray([])\n",
        "        # 損失を記録するための配列を用意(検証データ用)\n",
        "        self.val_loss = np.ndarray([])\n",
        "        \n",
        "    # 問題6（学習と推定）\n",
        "    def fit(self, X, y, X_val, y_val):  # 必要に応じて引数を追加して下さい\n",
        "        \"\"\"\n",
        "        線形回帰の学習。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
        "        Parameters\n",
        "        ---------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "          訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "          訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "          検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "          検証データの正解値\n",
        "        \"\"\"\n",
        "\n",
        "        m_0 = X.shape[0]\n",
        "        m = np.ones((m_0, 1))\n",
        "        X = np.hstack((m, X))\n",
        "\n",
        "        m2_0 = X_val.shape[0]\n",
        "        m2 = np.ones((m2_0, 1))\n",
        "        X_val = np.hstack((m2, X_val))\n",
        "        \n",
        "        # Xが変数でありその数だけ列があり、θとXの数は同じであることから列でshapeする。\n",
        "        self.theta = np.zeros(X.shape[1])\n",
        "        # 1次元になっているため2次元配列化する\n",
        "        self.theta = self.theta.reshape(X.shape[1], 1)\n",
        "\n",
        "        # メイン処理\n",
        "        for i in range(self.num_iter):\n",
        "            # イテレーションの回数を確認できるようにするため一旦ここでprintしておく\n",
        "            print('イテレーション{}回目'.format(i))\n",
        "            # 問題1（過程関数の計算）の解答をここに記載\n",
        "            pred1 = self._linear_hypothesis(X)\n",
        "            print(pred1)\n",
        "\n",
        "            #pred2 = self._linear_hypothesis(X_val)\n",
        "            #print(pred2)\n",
        "\n",
        "            # 問題2（最急降下法によるパラメータの更新値計算）の解答をここに記載\n",
        "            self.theta = self._gradient_descent(X, y)\n",
        "            # 問題7（学習曲線のプロット）のグラフ描画時（問題5（損失関数）で作成した関数を使用）\n",
        "            loss = self._loss_func(X, y)\n",
        "            val_loss = self._loss_func(X_val, y_val)\n",
        "            self.loss = np.append(self.loss, loss)\n",
        "            self.val_loss = np.append(self.val_loss, val_loss)\n",
        "\n",
        "            # Figureの初期化\n",
        "            #fig = plt.figure()\n",
        "            # Figure内にAxesを追加()\n",
        "            #ax = fig.add_subplot(111)\n",
        "            #ax.plot(i, loss, color = 'tab:blue', label = 'train_loss')\n",
        "            #ax.plot(i, val_loss, color = 'tab:orange', label = 'val_loss')\n",
        "            #plt.title('model loss')\n",
        "            #plt.xlabel('iter')\n",
        "            #plt.ylabel('loss')\n",
        "            # 凡例の表示\n",
        "            #plt.legened()\n",
        "\n",
        "    # 問題1\n",
        "    def _linear_hypothesis(self, X):  # 必要に応じて引数を追加して下さい\n",
        "        \"\"\"\n",
        "        仮定関数の計算を計算する\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "          訓練データ\n",
        "        Returns\n",
        "        -------\n",
        "        次の形のndarray, shape (n_samples, 1)\n",
        "        線形の仮定関数による推定結果\n",
        "        \"\"\"\n",
        "        pred = X @ self.theta\n",
        "        return pred\n",
        "\n",
        "    # 問題2\n",
        "    def _gradient_descent(self, X, y):\n",
        "        \"\"\"\n",
        "        最急降下法によるパラメータの更新\n",
        "        適当な重みを掛けてθが最小値の方に動いていくようにする\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "          訓練データ\n",
        "        Returns\n",
        "        -------\n",
        "        Theta : 次の形のndarray, shape (n_samples, 1)\n",
        "          線形の仮定関数による推定結果\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        n = X.shape[1]\n",
        "        pred = self._linear_hypothesis(X)\n",
        "        # 特徴量の数だけfor文を回す\n",
        "        for j in range(n):\n",
        "          # gradientの初期値を設定\n",
        "          gradient = 0\n",
        "          # サンプル数分だけfor文を回す\n",
        "          for i in range(m):\n",
        "            gradient += (pred[i] - y[i]) * X[i, j]\n",
        "          self.theta[j] = self.theta[j] - self.lr * (gradient / m) \n",
        "        return self.theta\n",
        "    \n",
        "    # 問題3\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        線形回帰を使い推定する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "        Returns\n",
        "        -------\n",
        "            次の形のndarray, shape (n_samples, 1)\n",
        "            線形回帰による推定結果\n",
        "        \"\"\"\n",
        "        a = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
        "        X = np.hstack([a, X])\n",
        "\n",
        "        pred_y = self._linear_hypothesis(X)\n",
        "        return pred_y\n",
        "    \n",
        "    # 問題5\n",
        "    def _loss_func(self, pred, y):\n",
        "      \"\"\"\n",
        "      損失関数の計算\n",
        "      Parameters\n",
        "      ----------\n",
        "      pred : \n",
        "        仮定関数\n",
        "      y : 次の形のndarray, shape (n_samples,)\n",
        "        正解値\n",
        "      Returns\n",
        "      ----------\n",
        "      lossfunc\n",
        "        損失値\n",
        "      \"\"\"\n",
        "      # yのサンプル数\n",
        "      m2 = y.shape[0]\n",
        "      error2 = 0\n",
        "      for i in range(m2):\n",
        "        error2 += ((pred[i] - y[i]) ** 2)\n",
        "        # (m×2)で割る(合計してから割るのだからインデントはfor文と同じところまで戻す)\n",
        "        loss = error2 / (2 * m2)\n",
        "      return loss\n",
        "\n",
        "# House Pricesコンペティションのデータ読み込み\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/DIC/train.csv\")\n",
        "\n",
        "# 説明変数GrLivAreaとYearBuilt、目的変数SalePriceをそれぞれ抜き出してXとyに設定\n",
        "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
        "y = dataset.loc[:, ['SalePrice']]\n",
        "# arrayに変換\n",
        "X = X.values\n",
        "y = y.values\n",
        "\n",
        "# trainデータとtestデータへの分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=1234)\n",
        "\n",
        "# 標準化\n",
        "scaler = StandardScaler()\n",
        "X_train_std = scaler.fit_transform(X_train)\n",
        "X_test_std = scaler.fit_transform(X_test)\n",
        "\n",
        "# スクラッチしたモデルで訓練\n",
        "num_iter = 10\n",
        "lr =0.005\n",
        "reg = ScratchLinearRegression(num_iter, lr, no_bias = True, verbose = True)\n",
        "reg.fit(X_train, y_train, X_test, y_test)\n",
        "\n",
        "# 線形回帰の推定\n",
        "y_pred = reg.predict(X_test)\n",
        "\n",
        "print('予測値：{:.3f}'.format(y_pred.mean()))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "イテレーション0回目\n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " ...\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "イテレーション1回目\n",
            "[[6.27549116e+09]\n",
            " [5.48162744e+09]\n",
            " [5.40780102e+09]\n",
            " ...\n",
            " [6.32946184e+09]\n",
            " [5.75960844e+09]\n",
            " [6.13111445e+09]]\n",
            "イテレーション2回目\n",
            "[[-1.97601724e+14]\n",
            " [-1.73499932e+14]\n",
            " [-1.71136356e+14]\n",
            " ...\n",
            " [-1.99376754e+14]\n",
            " [-1.81826762e+14]\n",
            " [-1.93308333e+14]]\n",
            "イテレーション3回目\n",
            "[[6.23244908e+18]\n",
            " [5.47304025e+18]\n",
            " [5.39845837e+18]\n",
            " ...\n",
            " [6.28849962e+18]\n",
            " [5.73530442e+18]\n",
            " [6.09725170e+18]]\n",
            "イテレーション4回目\n",
            "[[-1.96583107e+23]\n",
            " [-1.72630599e+23]\n",
            " [-1.70278117e+23]\n",
            " ...\n",
            " [-1.98351103e+23]\n",
            " [-1.80902584e+23]\n",
            " [-1.92318917e+23]]\n",
            "イテレーション5回目\n",
            "[[6.20060669e+27]\n",
            " [5.44509939e+27]\n",
            " [5.37089761e+27]\n",
            " ...\n",
            " [6.25637268e+27]\n",
            " [5.70601333e+27]\n",
            " [6.06610615e+27]]\n",
            "イテレーション6回目\n",
            "[[-1.95578985e+32]\n",
            " [-1.71748841e+32]\n",
            " [-1.69408376e+32]\n",
            " ...\n",
            " [-1.97337951e+32]\n",
            " [-1.79978565e+32]\n",
            " [-1.91336581e+32]]\n",
            "イテレーション7回目\n",
            "[[6.16893496e+36]\n",
            " [5.41728669e+36]\n",
            " [5.34346393e+36]\n",
            " ...\n",
            " [6.22441611e+36]\n",
            " [5.67686792e+36]\n",
            " [6.03512143e+36]]\n",
            "イテレーション8回目\n",
            "[[-1.94579998e+41]\n",
            " [-1.70871575e+41]\n",
            " [-1.68543064e+41]\n",
            " ...\n",
            " [-1.96329980e+41]\n",
            " [-1.79059263e+41]\n",
            " [-1.90359264e+41]]\n",
            "イテレーション9回目\n",
            "[[6.13742502e+45]\n",
            " [5.38961605e+45]\n",
            " [5.31617036e+45]\n",
            " ...\n",
            " [6.19262278e+45]\n",
            " [5.64787137e+45]\n",
            " [6.00429499e+45]]\n",
            "予測値：-180386095843379190067077363233356681899398133710848.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fSGvUzpEF87"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mlr8ODK8MnJJ"
      },
      "source": [
        "# scikit-learnによる実装と比べ、正しく動いているかを確認\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN_XKwVJ4SjI"
      },
      "source": [
        "【問題7】学習曲線のプロット\n",
        "\n",
        "学習曲線を表示する関数を作成し、実行してください。グラフを見て損失が適切に下がっているかどうか確認してください。\n",
        "\n",
        "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxRhWP47MsTZ"
      },
      "source": [
        "# 上記問題6の箇所に解答しました。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mjmzxlt_5CXa"
      },
      "source": [
        "【問題8】（アドバンス課題）バイアス項の除去\n",
        "\n",
        "バイアス項 $\\theta_0$ を抜くと学習がどう変化するか検証してください。また、線形回帰モデルにおけるバイアス項の役割の考察・調査を行ってください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vusPdBr5RIZ"
      },
      "source": [
        "【問題9】（アドバンス課題）特徴量の多次元化\n",
        "\n",
        "特徴量の二乗や三乗を入力に利用すると学習結果がどう変化するか検証してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhHFvYLM5XBA"
      },
      "source": [
        "【問題10】（アドバンス課題）更新式の導出\n",
        "\n",
        "最急降下法の更新式は以下でした。この式が導出される過程を説明してください。\n",
        "\n",
        "\n",
        "\\[\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\\]\n",
        "以下の式から説明をはじめることができます。\n",
        "\n",
        "\n",
        "\\[\\theta_j := \\theta_j - \\frac{\\partial}{\\partial \\theta_j}J(\\theta) \\\\\\]\n",
        "目的関数（損失関数） $J(\\theta)$ は次の式です。\n",
        "\n",
        "\n",
        "\\[J(\\theta)= \\frac{1 }{ 2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\\]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL1heDBe6pKI"
      },
      "source": [
        "【問題11】（アドバンス課題）局所最適解の問題\n",
        "\n",
        "最急降下法には一般的に局所最適解の問題があります。しかし、線形回帰では学習を続ければ必ず最適解を求めることができます。それはなぜか数式やグラフを用いて説明してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yjl3hTyH6eMS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EO_9WO-AkAzf"
      },
      "source": [
        "y = [3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
        "theta = np.zeros(y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}