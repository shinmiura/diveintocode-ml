{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow.ipynb",
      "provenance": [],
      "mount_file_id": "1dpIz3zIrFCrCvV-fKETRAsixvwGTo7qh",
      "authorship_tag": "ABX9TyNfIm//pHKYqACvIDgNMSHN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shinmiura/diveintocode-ml/blob/master/TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ22udHA9baO"
      },
      "source": [
        "**Sprintの目的**\n",
        "\n",
        "・フレームワークを用いたコードを読めるようになる\n",
        "\n",
        "・フレームワークを習得し続けられるようになる\n",
        "\n",
        "・理論を知っている範囲をフレームワークで動かす"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR-ebwFB9sjr"
      },
      "source": [
        "どのように学ぶか\n",
        "\n",
        "TensorFlowのサンプルコードを元に、これまで扱ってきたデータセットを学習していきます。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zZ6w_fh933j"
      },
      "source": [
        "# 2.コードリーディング\n",
        "\n",
        "TensorFlowによって2値分類を行うサンプルコードを載せました。今回はこれをベースにして進めます。\n",
        "\n",
        "\n",
        "tf.estimator などの高レベルAPIは使用していません。低レベルなところから見ていくことにします。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bey1VWAK_NGx"
      },
      "source": [
        "【問題1】スクラッチを振り返る\n",
        "\n",
        "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n",
        "\n",
        "\n",
        "（例）\n",
        "\n",
        "\n",
        "・重みを初期化する必要があった\n",
        "\n",
        "・エポックのループが必要だった\n",
        "\n",
        "それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gNnFaNt_u6E"
      },
      "source": [
        "**データセットの用意**\n",
        "\n",
        "以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n",
        "\n",
        "\n",
        "Iris Species\n",
        "\n",
        "\n",
        "目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。\n",
        "\n",
        "Iris-versicolor\n",
        "\n",
        "Iris-virginica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6TKFrN7AWJh"
      },
      "source": [
        "【問題１の解答】　　ディープラーニングを実装するために必要だったもの\n",
        "\n",
        "・　学習に時間がかかるため実装していくうえで処理のための時間を確保する必要があった。\n",
        "\n",
        "・　構造(層の数、ユニット数など)を決める必要があった。\n",
        "\n",
        "・　重みとバイアスをパラメータとして設定する必要があった。\n",
        "\n",
        "・　重みとバイアスを初期化する必要があった。\n",
        "\n",
        "・重みパラメータの勾配を求め、重みパラメータを勾配方向に微少量だけ重みの値を更新する必要があった。\n",
        "\n",
        "・\n",
        "\n",
        "・　順伝播だけでなく逆伝播も実装する必要があった。\n",
        "\n",
        "・　全結合ニューラルネットワークでは行列の計算をして活性化関数を通すという処理を繰り返す必要があった。\n",
        "\n",
        "・　各層における入力と重み付き入力を保持しておく必要があった(∵後で逆伝播と重みの更新の時に使う)\n",
        "\n",
        "・　デルタの計算のために活性化関数を微分する必要があった\n",
        "\n",
        "・　逆伝播の処理にあたっては、上記の活性化関数の微分のほか出力層のデルタ、隠れ層のデルタを実装する必要があった。\n",
        "\n",
        "・逆伝播されたデルタを使って(偏微分を求めて)パラメータを更新する必要があった。\n",
        "\n",
        "・　エポック数を（誤差を見ながら見極める)設定する必要があった。\n",
        "\n",
        "・　学習データをミニバッチと呼ばれる小さな単位に分割して学習する必要(ミニバッチの損失関数の値を減らすことを目的)があった。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ea5ry9eT2ror"
      },
      "source": [
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns #バイオリン図で必要になる（seaborn ライブラリーの violinplot メソッド）\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist\n",
        "%matplotlib inline\n",
        "# tensorflow1系\n",
        "# import tensorflow as tf\n",
        "# tensorflow2系\n",
        "import tensorflow.compat.v1 as tf  #TensorFlow 2 で 1.x のコードを走らせるには左のコードが必要(Dyberのサンプルコードだと走らない)\n",
        "tf.disable_eager_execution()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t22v3-xP2rXj"
      },
      "source": [
        "# データの読み込み\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "# データの整形\n",
        "iris_dataframe = pd.DataFrame(data = iris_dataset.data, columns = iris_dataset.feature_names)\n",
        "# .target部分を抽出し、カラム名も指定する\n",
        "iris_datalabel = pd.DataFrame(data = iris_dataset.target, columns = ['Species'])\n",
        "# .concatを使用して列方向で結合する\n",
        "df = pd.concat([iris_dataframe, iris_datalabel], axis = 1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM3gt1RVZV6B"
      },
      "source": [
        "# ミニバッチクラス\n",
        "class GetMiniBatch:\n",
        "\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd-q9Bpjwdf4"
      },
      "source": [
        "**【問題2】スクラッチとTensorFlowの対応を考える**\n",
        "\n",
        "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n",
        "\n",
        "\n",
        "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n",
        "\n",
        "\n",
        "《サンプルコード》\n",
        "\n",
        "\n",
        "＊TensorFlow バージョン 2.4 で動作を確認済みです。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3H0thCr93UX"
      },
      "source": [
        "#サンプルコード\n",
        "\"\"\"\n",
        "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "# データセットの読み込み\n",
        "dataset_path =\"Iris.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "# データフレームから条件抽出\n",
        "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X).astype(np.float32)\n",
        "# ラベルを数値に変換\n",
        "y[y=='Iris-versicolor'] = 0\n",
        "y[y=='Iris-virginica'] = 1\n",
        "y = y.astype(np.float32)[:, np.newaxis]\n",
        "# trainとtestに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "# さらにtrainとvalに分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self.X = X[shuffle_index]\n",
        "        self.y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self.X[p0:p1], self.y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self.X[p0:p1], self.y[p0:p1]\n",
        "# ハイパーパラメータの設定\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "# trainのミニバッチイテレータ\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 重みとバイアスの宣言\n",
        "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\n",
        "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\n",
        "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\n",
        "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\n",
        "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\n",
        "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\n",
        "    def call(self, x):\n",
        "        \"\"\"\n",
        "        単純な3層ニューラルネットワーク\n",
        "        \"\"\"\n",
        "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\n",
        "        layer_1 = tf.nn.relu(layer_1)\n",
        "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\n",
        "        layer_2 = tf.nn.relu(layer_2)\n",
        "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\n",
        "        return layer_output\n",
        "model = MyModel()\n",
        "\n",
        "# # 最適化手法\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "def train(x, y):\n",
        "    logits = model(x, training=True)\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\n",
        "    return loss\n",
        "\n",
        "def evaluate(x, y):\n",
        "    logits = model(x)\n",
        "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\n",
        "    # 推定結果\n",
        "    correct_pred = tf.equal(tf.sign(y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "    # 指標値計算\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "    return loss, accuracy\n",
        "\n",
        "# 計算グラフの実行\n",
        "for epoch in range(num_epochs):\n",
        "    # エポックごとにループ\n",
        "    total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n",
        "    total_loss = 0\n",
        "    total_acc = 0\n",
        "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "        # ミニバッチごとにループ\n",
        "        with tf.GradientTape() as tape:\n",
        "            loss = train(mini_batch_x, mini_batch_y)\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        total_loss += loss\n",
        "    loss = total_loss / n_samples\n",
        "    val_loss, val_acc = evaluate(X_val, y_val)\n",
        "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\n",
        "_, test_acc = evaluate(X_test, y_test)\n",
        "print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlAdl6rxG7lT"
      },
      "source": [
        "# データ準備\n",
        "# 2値分類のため絞り込み\n",
        "df2 = df[(df[\"Species\"] == 0)|(df[\"Species\"] == 1)]\n",
        "\n",
        "# 説明変数と目的変数に分割\n",
        "y = df2[\"Species\"]\n",
        "X = df2.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# 訓練データ/テストデータ/評価データに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# 正規化\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "X_val /= 255"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIrpzcWUHG0Q",
        "outputId": "a8196bbb-323f-43d3-8cc1-ac1c31c137c0"
      },
      "source": [
        "# TensorFlowで学習\n",
        "# 各種変数定義\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# 空配列定義\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# ミニバッチイテレータ生成\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
        "    Parameters\n",
        "    ---------------\n",
        "    x : 入力配列\n",
        "    \"\"\"\n",
        "    # 重み定義\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    # バイアス定義\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    # 計算グラフ構築（順伝播処理）\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 計算グラフ受け取る\n",
        "logits = example_net(X)\n",
        "# 損失定義\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "# 最適化手法の定義\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "# 最適化手法で、定義した損失を最小化するルールを作成\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# ACC計算\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# 変数を扱うためのおまじない\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# tensorflowのセッション開始\n",
        "with tf.Session() as sess:\n",
        "    # 初期化\n",
        "    sess.run(init)\n",
        "    # 学習回数分ループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # ミニバッチイテレータでループ\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        # 記録\n",
        "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        # 仮定出力\n",
        "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
        "    \n",
        "    # 学習が終了したらテストデータで実行\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n",
        "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
        "    #print(prediction)\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train_loss : 5.9152, val_loss : 7.8665, train_acc : 0.531, val_acc : 0.375\n",
            "Epoch 1, train_loss : 1.3632, val_loss : 1.0122, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 2, train_loss : 1.4577, val_loss : 1.9132, train_acc : 0.531, val_acc : 0.375\n",
            "Epoch 3, train_loss : 1.7290, val_loss : 1.2583, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 4, train_loss : 0.6383, val_loss : 0.7484, train_acc : 0.531, val_acc : 0.375\n",
            "Epoch 5, train_loss : 0.5594, val_loss : 0.5968, train_acc : 0.766, val_acc : 0.750\n",
            "Epoch 6, train_loss : 0.8380, val_loss : 0.6483, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 7, train_loss : 0.8681, val_loss : 0.6632, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 8, train_loss : 0.6439, val_loss : 0.5216, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 9, train_loss : 0.5902, val_loss : 0.4847, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 10, train_loss : 0.6312, val_loss : 0.4993, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 11, train_loss : 0.6672, val_loss : 0.5145, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 12, train_loss : 0.6298, val_loss : 0.4855, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 13, train_loss : 0.5720, val_loss : 0.4443, train_acc : 0.469, val_acc : 0.625\n",
            "Epoch 14, train_loss : 0.5380, val_loss : 0.4177, train_acc : 0.531, val_acc : 0.625\n",
            "Epoch 15, train_loss : 0.5224, val_loss : 0.4022, train_acc : 0.547, val_acc : 0.625\n",
            "Epoch 16, train_loss : 0.5068, val_loss : 0.3870, train_acc : 0.562, val_acc : 0.625\n",
            "Epoch 17, train_loss : 0.4818, val_loss : 0.3663, train_acc : 0.594, val_acc : 0.688\n",
            "Epoch 18, train_loss : 0.4506, val_loss : 0.3421, train_acc : 0.656, val_acc : 0.812\n",
            "Epoch 19, train_loss : 0.4197, val_loss : 0.3182, train_acc : 0.734, val_acc : 0.875\n",
            "Epoch 20, train_loss : 0.3916, val_loss : 0.2965, train_acc : 0.812, val_acc : 0.875\n",
            "Epoch 21, train_loss : 0.3657, val_loss : 0.2763, train_acc : 0.844, val_acc : 0.875\n",
            "Epoch 22, train_loss : 0.3405, val_loss : 0.2568, train_acc : 0.922, val_acc : 0.938\n",
            "Epoch 23, train_loss : 0.3155, val_loss : 0.2380, train_acc : 0.938, val_acc : 0.938\n",
            "Epoch 24, train_loss : 0.2915, val_loss : 0.2201, train_acc : 0.953, val_acc : 1.000\n",
            "Epoch 25, train_loss : 0.2691, val_loss : 0.2036, train_acc : 0.969, val_acc : 1.000\n",
            "Epoch 26, train_loss : 0.2489, val_loss : 0.1887, train_acc : 0.969, val_acc : 1.000\n",
            "Epoch 27, train_loss : 0.2310, val_loss : 0.1755, train_acc : 0.969, val_acc : 1.000\n",
            "Epoch 28, train_loss : 0.2150, val_loss : 0.1637, train_acc : 0.984, val_acc : 1.000\n",
            "Epoch 29, train_loss : 0.1887, val_loss : 0.1478, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 30, train_loss : 0.1964, val_loss : 0.1469, train_acc : 0.984, val_acc : 1.000\n",
            "Epoch 31, train_loss : 0.1849, val_loss : 0.1381, train_acc : 0.984, val_acc : 1.000\n",
            "Epoch 32, train_loss : 0.1648, val_loss : 0.1266, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 33, train_loss : 0.1527, val_loss : 0.1189, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 34, train_loss : 0.1472, val_loss : 0.1131, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 35, train_loss : 0.1411, val_loss : 0.1075, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 36, train_loss : 0.1326, val_loss : 0.1018, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 37, train_loss : 0.1254, val_loss : 0.0966, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 38, train_loss : 0.1199, val_loss : 0.0920, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 39, train_loss : 0.1145, val_loss : 0.0876, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 40, train_loss : 0.1090, val_loss : 0.0835, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 41, train_loss : 0.1042, val_loss : 0.0797, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 42, train_loss : 0.0998, val_loss : 0.0761, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 43, train_loss : 0.0955, val_loss : 0.0728, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 44, train_loss : 0.0915, val_loss : 0.0696, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 45, train_loss : 0.0879, val_loss : 0.0667, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 46, train_loss : 0.0844, val_loss : 0.0639, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 47, train_loss : 0.0811, val_loss : 0.0613, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 48, train_loss : 0.0780, val_loss : 0.0588, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 49, train_loss : 0.0751, val_loss : 0.0564, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 50, train_loss : 0.0723, val_loss : 0.0542, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 51, train_loss : 0.0697, val_loss : 0.0521, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 52, train_loss : 0.0673, val_loss : 0.0501, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 53, train_loss : 0.0649, val_loss : 0.0482, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 54, train_loss : 0.0627, val_loss : 0.0465, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 55, train_loss : 0.0606, val_loss : 0.0448, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 56, train_loss : 0.0586, val_loss : 0.0431, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 57, train_loss : 0.0566, val_loss : 0.0416, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 58, train_loss : 0.0548, val_loss : 0.0401, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 59, train_loss : 0.0531, val_loss : 0.0387, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 60, train_loss : 0.0514, val_loss : 0.0374, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 61, train_loss : 0.0498, val_loss : 0.0361, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 62, train_loss : 0.0483, val_loss : 0.0349, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 63, train_loss : 0.0469, val_loss : 0.0338, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 64, train_loss : 0.0455, val_loss : 0.0326, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 65, train_loss : 0.0442, val_loss : 0.0316, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 66, train_loss : 0.0429, val_loss : 0.0306, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 67, train_loss : 0.0417, val_loss : 0.0296, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 68, train_loss : 0.0405, val_loss : 0.0287, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 69, train_loss : 0.0394, val_loss : 0.0278, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 70, train_loss : 0.0383, val_loss : 0.0269, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 71, train_loss : 0.0373, val_loss : 0.0261, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 72, train_loss : 0.0363, val_loss : 0.0253, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 73, train_loss : 0.0353, val_loss : 0.0246, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 74, train_loss : 0.0344, val_loss : 0.0239, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 75, train_loss : 0.0335, val_loss : 0.0232, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 76, train_loss : 0.0327, val_loss : 0.0225, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 77, train_loss : 0.0319, val_loss : 0.0218, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 78, train_loss : 0.0311, val_loss : 0.0212, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 79, train_loss : 0.0303, val_loss : 0.0206, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 80, train_loss : 0.0296, val_loss : 0.0201, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 81, train_loss : 0.0289, val_loss : 0.0195, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 82, train_loss : 0.0282, val_loss : 0.0190, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 83, train_loss : 0.0275, val_loss : 0.0185, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 84, train_loss : 0.0269, val_loss : 0.0180, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 85, train_loss : 0.0263, val_loss : 0.0175, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 86, train_loss : 0.0257, val_loss : 0.0171, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 87, train_loss : 0.0251, val_loss : 0.0166, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 88, train_loss : 0.0245, val_loss : 0.0162, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 89, train_loss : 0.0240, val_loss : 0.0158, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 90, train_loss : 0.0234, val_loss : 0.0154, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 91, train_loss : 0.0229, val_loss : 0.0150, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 92, train_loss : 0.0224, val_loss : 0.0146, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 93, train_loss : 0.0220, val_loss : 0.0143, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 94, train_loss : 0.0215, val_loss : 0.0139, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 95, train_loss : 0.0211, val_loss : 0.0136, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 96, train_loss : 0.0206, val_loss : 0.0132, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 97, train_loss : 0.0202, val_loss : 0.0129, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 98, train_loss : 0.0198, val_loss : 0.0126, train_acc : 1.000, val_acc : 1.000\n",
            "Epoch 99, train_loss : 0.0194, val_loss : 0.0123, train_acc : 1.000, val_acc : 1.000\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njwM0hcuw6nE"
      },
      "source": [
        "# 3.他のデータセットへの適用"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIHW9K47xCqJ"
      },
      "source": [
        "これまで扱ってきた小さなデータセットが他にもいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n",
        "\n",
        "\n",
        "Iris（3種類すべての目的変数を使用）\n",
        "House Prices\n",
        "\n",
        "どのデータセットも train, val, test の3種類に分けて使用してください。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGV0jhThxIfk"
      },
      "source": [
        "**【問題3】3種類すべての目的変数を使用したIrisのモデルを作成**\n",
        "\n",
        "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類すべてを分類できるモデルを作成してください。\n",
        "\n",
        "\n",
        "Iris Species\n",
        "\n",
        "\n",
        "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n",
        "\n",
        "\n",
        "《ヒント》\n",
        "\n",
        "\n",
        "以下の2箇所は2クラス分類特有の処理です。\n",
        "\n",
        "loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n",
        "\n",
        "correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n",
        "\n",
        "メソッドは以下のように公式ドキュメントを確認してください。\n",
        "\n",
        "\n",
        "tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow\n",
        "\n",
        "\n",
        "tf.math.sign  |  TensorFlow\n",
        "\n",
        "\n",
        "＊tf.signとtf.math.signは同じ働きをします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVYkvVaoJJL7"
      },
      "source": [
        "# データ準備\n",
        "# 説明変数と目的変数に分割\n",
        "y = df[\"Species\"]\n",
        "X = df.loc[:, [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# 訓練データ/テストデータ/評価データに分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
        "\n",
        "# onehotベクトル化\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train)\n",
        "y_val_one_hot = enc.transform(y_val)\n",
        "y_test_one_hot = enc.transform(y_test)\n",
        "\n",
        "# 正規化\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "X_val /= 255"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RYo0jO_JN5Z",
        "outputId": "6d3ee62e-6b37-4434-cf83-777a3db350d0"
      },
      "source": [
        "# Tensorflowで学習\n",
        "# 各種変数定義\n",
        "learning_rate = 0.01\n",
        "batch_size = 10\n",
        "num_epochs = 100\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 3 # 2値分類からの変更箇所\n",
        "\n",
        "# 空配列定義\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# ミニバッチイテレータ生成\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train_one_hot, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
        "    Parameters\n",
        "    ---------------\n",
        "    x : 入力配列\n",
        "    \"\"\"\n",
        "    # 重み定義\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    # バイアス定義\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    # 計算グラフ構築（順伝播処理）\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 計算グラフ受け取る\n",
        "logits = example_net(X)\n",
        "# 損失定義\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
        "# 最適化手法の定義\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "# 最適化手法で、定義した損失を最小化するルールを作成\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# ACC計算\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# 変数を扱うためのおまじない\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# tensorflowのセッション開始\n",
        "with tf.Session() as sess:\n",
        "    # 初期化\n",
        "    sess.run(init)\n",
        "    # 学習回数分ループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # ミニバッチイテレータでループ\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        # 記録\n",
        "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train_one_hot})\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val_one_hot})\n",
        "        # 仮定出力\n",
        "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
        "    \n",
        "    # 学習が終了したらテストデータで実行\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
        "    #print(prediction)\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train_loss : 8.0341, val_loss : 8.2610, train_acc : 0.302, val_acc : 0.292\n",
            "Epoch 1, train_loss : 1.5219, val_loss : 1.5577, train_acc : 0.312, val_acc : 0.292\n",
            "Epoch 2, train_loss : 2.0978, val_loss : 2.1565, train_acc : 0.312, val_acc : 0.292\n",
            "Epoch 3, train_loss : 1.5079, val_loss : 1.5401, train_acc : 0.312, val_acc : 0.292\n",
            "Epoch 4, train_loss : 1.8299, val_loss : 1.8574, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 5, train_loss : 1.8376, val_loss : 1.8487, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 6, train_loss : 1.7022, val_loss : 1.7070, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 7, train_loss : 1.6048, val_loss : 1.6091, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 8, train_loss : 1.5605, val_loss : 1.5653, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 9, train_loss : 1.5391, val_loss : 1.5439, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 10, train_loss : 1.5185, val_loss : 1.5225, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 11, train_loss : 1.4897, val_loss : 1.4925, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 12, train_loss : 1.4520, val_loss : 1.4535, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 13, train_loss : 1.4073, val_loss : 1.4075, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 14, train_loss : 1.3580, val_loss : 1.3567, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 15, train_loss : 1.3059, val_loss : 1.3031, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 16, train_loss : 1.2552, val_loss : 1.2512, train_acc : 0.323, val_acc : 0.333\n",
            "Epoch 17, train_loss : 1.2016, val_loss : 1.1963, train_acc : 0.333, val_acc : 0.333\n",
            "Epoch 18, train_loss : 1.1482, val_loss : 1.1415, train_acc : 0.365, val_acc : 0.375\n",
            "Epoch 19, train_loss : 1.0944, val_loss : 1.0865, train_acc : 0.385, val_acc : 0.375\n",
            "Epoch 20, train_loss : 1.0422, val_loss : 1.0332, train_acc : 0.417, val_acc : 0.458\n",
            "Epoch 21, train_loss : 0.9915, val_loss : 0.9816, train_acc : 0.438, val_acc : 0.458\n",
            "Epoch 22, train_loss : 0.9429, val_loss : 0.9321, train_acc : 0.448, val_acc : 0.500\n",
            "Epoch 23, train_loss : 0.8969, val_loss : 0.8853, train_acc : 0.469, val_acc : 0.542\n",
            "Epoch 24, train_loss : 0.8534, val_loss : 0.8412, train_acc : 0.500, val_acc : 0.625\n",
            "Epoch 25, train_loss : 0.8124, val_loss : 0.7997, train_acc : 0.521, val_acc : 0.625\n",
            "Epoch 26, train_loss : 0.7740, val_loss : 0.7610, train_acc : 0.552, val_acc : 0.625\n",
            "Epoch 27, train_loss : 0.7379, val_loss : 0.7249, train_acc : 0.573, val_acc : 0.625\n",
            "Epoch 28, train_loss : 0.7043, val_loss : 0.6915, train_acc : 0.594, val_acc : 0.625\n",
            "Epoch 29, train_loss : 0.6734, val_loss : 0.6609, train_acc : 0.604, val_acc : 0.667\n",
            "Epoch 30, train_loss : 0.6447, val_loss : 0.6329, train_acc : 0.646, val_acc : 0.708\n",
            "Epoch 31, train_loss : 0.6183, val_loss : 0.6073, train_acc : 0.677, val_acc : 0.708\n",
            "Epoch 32, train_loss : 0.5942, val_loss : 0.5842, train_acc : 0.729, val_acc : 0.750\n",
            "Epoch 33, train_loss : 0.5721, val_loss : 0.5633, train_acc : 0.750, val_acc : 0.792\n",
            "Epoch 34, train_loss : 0.5518, val_loss : 0.5443, train_acc : 0.781, val_acc : 0.792\n",
            "Epoch 35, train_loss : 0.5332, val_loss : 0.5272, train_acc : 0.823, val_acc : 0.833\n",
            "Epoch 36, train_loss : 0.5163, val_loss : 0.5117, train_acc : 0.865, val_acc : 0.833\n",
            "Epoch 37, train_loss : 0.5007, val_loss : 0.4977, train_acc : 0.875, val_acc : 0.875\n",
            "Epoch 38, train_loss : 0.4865, val_loss : 0.4851, train_acc : 0.896, val_acc : 0.875\n",
            "Epoch 39, train_loss : 0.4733, val_loss : 0.4734, train_acc : 0.906, val_acc : 0.917\n",
            "Epoch 40, train_loss : 0.4610, val_loss : 0.4626, train_acc : 0.906, val_acc : 0.917\n",
            "Epoch 41, train_loss : 0.4494, val_loss : 0.4525, train_acc : 0.906, val_acc : 0.917\n",
            "Epoch 42, train_loss : 0.4383, val_loss : 0.4429, train_acc : 0.906, val_acc : 0.917\n",
            "Epoch 43, train_loss : 0.4279, val_loss : 0.4339, train_acc : 0.938, val_acc : 0.917\n",
            "Epoch 44, train_loss : 0.4179, val_loss : 0.4253, train_acc : 0.948, val_acc : 0.917\n",
            "Epoch 45, train_loss : 0.4083, val_loss : 0.4171, train_acc : 0.948, val_acc : 0.917\n",
            "Epoch 46, train_loss : 0.3990, val_loss : 0.4092, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 47, train_loss : 0.3899, val_loss : 0.4015, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 48, train_loss : 0.3812, val_loss : 0.3942, train_acc : 0.948, val_acc : 0.917\n",
            "Epoch 49, train_loss : 0.3726, val_loss : 0.3870, train_acc : 0.948, val_acc : 0.917\n",
            "Epoch 50, train_loss : 0.3643, val_loss : 0.3800, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 51, train_loss : 0.3562, val_loss : 0.3733, train_acc : 0.958, val_acc : 0.958\n",
            "Epoch 52, train_loss : 0.3484, val_loss : 0.3668, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 53, train_loss : 0.3407, val_loss : 0.3604, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 54, train_loss : 0.3332, val_loss : 0.3543, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 55, train_loss : 0.3258, val_loss : 0.3483, train_acc : 0.969, val_acc : 0.875\n",
            "Epoch 56, train_loss : 0.3187, val_loss : 0.3425, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 57, train_loss : 0.3117, val_loss : 0.3368, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 58, train_loss : 0.3048, val_loss : 0.3313, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 59, train_loss : 0.2982, val_loss : 0.3260, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 60, train_loss : 0.2917, val_loss : 0.3209, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 61, train_loss : 0.2853, val_loss : 0.3159, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 62, train_loss : 0.2791, val_loss : 0.3111, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 63, train_loss : 0.2732, val_loss : 0.3063, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 64, train_loss : 0.2672, val_loss : 0.3018, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 65, train_loss : 0.2615, val_loss : 0.2975, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 66, train_loss : 0.2559, val_loss : 0.2932, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 67, train_loss : 0.2500, val_loss : 0.2888, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 68, train_loss : 0.2448, val_loss : 0.2849, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 69, train_loss : 0.2397, val_loss : 0.2812, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 70, train_loss : 0.2350, val_loss : 0.2777, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 71, train_loss : 0.2299, val_loss : 0.2740, train_acc : 0.969, val_acc : 0.917\n",
            "Epoch 72, train_loss : 0.2255, val_loss : 0.2708, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 73, train_loss : 0.2208, val_loss : 0.2674, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 74, train_loss : 0.2166, val_loss : 0.2646, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 75, train_loss : 0.2125, val_loss : 0.2617, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 76, train_loss : 0.2082, val_loss : 0.2587, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 77, train_loss : 0.2041, val_loss : 0.2563, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 78, train_loss : 0.2005, val_loss : 0.2535, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 79, train_loss : 0.1965, val_loss : 0.2508, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 80, train_loss : 0.1930, val_loss : 0.2484, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 81, train_loss : 0.1898, val_loss : 0.2465, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 82, train_loss : 0.1862, val_loss : 0.2441, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 83, train_loss : 0.1829, val_loss : 0.2424, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 84, train_loss : 0.1799, val_loss : 0.2403, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 85, train_loss : 0.1767, val_loss : 0.2384, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 86, train_loss : 0.1738, val_loss : 0.2364, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 87, train_loss : 0.1710, val_loss : 0.2352, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 88, train_loss : 0.1683, val_loss : 0.2335, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 89, train_loss : 0.1656, val_loss : 0.2320, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 90, train_loss : 0.1631, val_loss : 0.2304, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 91, train_loss : 0.1606, val_loss : 0.2295, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 92, train_loss : 0.1582, val_loss : 0.2281, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 93, train_loss : 0.1559, val_loss : 0.2271, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 94, train_loss : 0.1537, val_loss : 0.2257, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 95, train_loss : 0.1515, val_loss : 0.2250, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 96, train_loss : 0.1495, val_loss : 0.2237, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 97, train_loss : 0.1475, val_loss : 0.2229, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 98, train_loss : 0.1455, val_loss : 0.2220, train_acc : 0.958, val_acc : 0.917\n",
            "Epoch 99, train_loss : 0.1437, val_loss : 0.2214, train_acc : 0.958, val_acc : 0.917\n",
            "test_acc : 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5fCARxYyYdn"
      },
      "source": [
        "**【問題4】House Pricesのモデルを作成**\n",
        "\n",
        "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n",
        "\n",
        "\n",
        "House Prices: Advanced Regression Techniques\n",
        "\n",
        "\n",
        "この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n",
        "\n",
        "\n",
        "分類問題と回帰問題の違いを考慮してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EqMaPgyNJrsz",
        "outputId": "2e7184cf-ccb3-4922-d018-6c6dae948915"
      },
      "source": [
        "%pwd"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TG31jhluJtw2",
        "outputId": "c6507a09-767e-46da-deee-0d4d82d1d144"
      },
      "source": [
        "%cd Colab Notebooks"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvHcHVfXJxHd",
        "outputId": "5847aad6-ac7e-4c6e-da3f-ee0beee3cfc3"
      },
      "source": [
        "%ls"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34mシンプソン\u001b[0m/\n",
            " 13_クラスとインスタンス.ipynb\n",
            " 15_クラス変数とメンバ変数.ipynb\n",
            " 16_クラスの使い道.ipynb\n",
            " Chapter1.ipynb\n",
            " Chapter2.ipynb\n",
            " CIFAR10_transfer_learning.ipynb\n",
            "'CNN_intro (1).ipynb'\n",
            " CNN_intro_answer.ipynb\n",
            " CNN_intro.ipynb\n",
            " data_handling_practice_answer.ipynb\n",
            " \u001b[01;34mdeep-learning-from-scratch-2-master\u001b[0m/\n",
            " \u001b[01;34mdeep-learning-from-scratch-master\u001b[0m/\n",
            " def関数練習.ipynb\n",
            "'example_answer_week3_sinyo_joho (1).ipynb'\n",
            " example_answer_week3_sinyo_joho.ipynb\n",
            "'example_answer_week4_ayame (1).ipynb'\n",
            " example_answer_week4_ayame.ipynb\n",
            " example_answer_week4_jutaku_price.ipynb\n",
            " example_answer_week4_shinyo_joho.ipynb\n",
            "'Faster R-CNNとYOLOv3_Sprint17.ipynb'\n",
            " H2O_for_preparation.ipynb\n",
            " H2O_walk_through.ipynb\n",
            " homework2_transfer_learning_answer.ipynb\n",
            " homework2_transfer_learning.ipynb\n",
            " クラスタリング.ipynb\n",
            " クラスとインスタンス.ipynb\n",
            " ニューラルネットワーク.ipynb\n",
            " ディープニューラルネットワーク.ipynb\n",
            "'janome_practice_NLP (1).ipynb'\n",
            " janome_practice_NLP.ipynb\n",
            " Keras.ipynb\n",
            " Keras入門.ipynb\n",
            " Linear_regression.ipynb\n",
            "'MEF_Sprint18_make_dataset（受講生用） (1).ipynb'\n",
            " MEF_Sprint18_make_dataset（受講生用）.ipynb\n",
            " modeling_process_walk_through.ipynb\n",
            " \u001b[01;34mNLP\u001b[0m/\n",
            "'Numpy入門1 配列の作成.ipynb'\n",
            " Numpy入門2.ipynb\n",
            "'Numpy入門2 基本的な配列処理.ipynb'\n",
            "'polarity_in_sentence_answer (1).ipynb'\n",
            " polarity_in_sentence_answer.ipynb\n",
            " polarity_in_sentence_sample_code.ipynb\n",
            "'practice_NLP (1).ipynb'\n",
            " practice_NLP.ipynb\n",
            "'pytorch_practical (1).ipynb'\n",
            "'pytorch_practical (2).ipynb'\n",
            "'pytorch_practical (3).ipynb'\n",
            " pytorch_practical-checkpoint.ipynb\n",
            " pytorch_practical.ipynb\n",
            " sample_code.ipynb\n",
            "'scraping (1).ipynb'\n",
            " scraping.ipynb\n",
            " SimpleConv1d.ipynb\n",
            " SimpleConv2d.ipynb\n",
            "'Sprint10 (1).ipynb'\n",
            " Sprint10.ipynb\n",
            "'Sprint10_歯抜け (1).ipynb'\n",
            " Sprint10_歯抜け.ipynb\n",
            " Sprint11.ipynb\n",
            " Sprint12.ipynb\n",
            " Sprint13_20210911.ipynb\n",
            " Sprint13.ipynb\n",
            " Sprint14.ipynb\n",
            " Sprint17-rcnn.ipynb\n",
            " Sprint17-yolo.ipynb\n",
            " \u001b[01;34msprint18_images\u001b[0m/\n",
            " Sprint18.ipynb\n",
            " Sprint19.ipynb\n",
            " Sprint1.ipynb\n",
            " Sprint20.ipynb\n",
            " Sprint21_歯抜け.ipynb\n",
            " Sprint22.ipynb\n",
            " Sprint23.ipynb\n",
            " Sprint24.ipynb\n",
            " Sprint25.ipynb\n",
            " sprint2.ipynb\n",
            " Sprint_2.ipynb\n",
            " Sprint5.ipynb\n",
            " Sprint6.ipynb\n",
            " Sprint7.ipynb\n",
            "'Sprint8 (1).ipynb'\n",
            " Sprint8.ipynb\n",
            "'Sprint_9_改 (1).ipynb'\n",
            " Sprint_9_改.ipynb\n",
            " Sprint_9_歯抜け_改.ipynb\n",
            " SVM.ipynb\n",
            " TensorFlow.ipynb\n",
            "'TensorFlow入門1  基本的な仕組み.ipynb'\n",
            "'TensorFlow入門2  ロジスティック回帰実装.ipynb'\n",
            " train.csv\n",
            " \u001b[01;34mtraining\u001b[0m/\n",
            " \u001b[01;34munet-master\u001b[0m/\n",
            " Untitled\n",
            " Untitled0.ipynb\n",
            "'Untitled (1)'\n",
            "'Untitled (10)'\n",
            " Untitled10.ipynb\n",
            "'Untitled (11)'\n",
            " Untitled11.ipynb\n",
            "'Untitled (12)'\n",
            " Untitled12.ipynb\n",
            "'Untitled (13)'\n",
            " Untitled13.ipynb\n",
            "'Untitled (14)'\n",
            " Untitled14.ipynb\n",
            "'Untitled (15)'\n",
            " Untitled15.ipynb\n",
            "'Untitled (16)'\n",
            " Untitled16.ipynb\n",
            "'Untitled (17)'\n",
            " Untitled17.ipynb\n",
            "'Untitled (18)'\n",
            " Untitled18.ipynb\n",
            "'Untitled (19)'\n",
            " Untitled19.ipynb\n",
            " Untitled1.ipynb\n",
            "'Untitled (2)'\n",
            "'Untitled (20)'\n",
            " Untitled20.ipynb\n",
            "'Untitled (21)'\n",
            " Untitled21.ipynb\n",
            "'Untitled (22)'\n",
            " Untitled22.ipynb\n",
            "'Untitled (23)'\n",
            " Untitled23.ipynb\n",
            "'Untitled (24)'\n",
            " Untitled24.ipynb\n",
            "'Untitled (25)'\n",
            " Untitled25.ipynb\n",
            "'Untitled (26)'\n",
            " Untitled26.ipynb\n",
            "'Untitled (27)'\n",
            " Untitled27.ipynb\n",
            "'Untitled (28)'\n",
            " Untitled28.ipynb\n",
            "'Untitled (29)'\n",
            " Untitled2.ipynb\n",
            "'Untitled (3)'\n",
            "'Untitled (30)'\n",
            "'Untitled (31)'\n",
            "'Untitled (32)'\n",
            "'Untitled (33)'\n",
            "'Untitled (34)'\n",
            "'Untitled (35)'\n",
            "'Untitled (36)'\n",
            "'Untitled (37)'\n",
            "'Untitled (38)'\n",
            "'Untitled (39)'\n",
            " Untitled3.ipynb\n",
            "'Untitled (4)'\n",
            "'Untitled (40)'\n",
            "'Untitled (41)'\n",
            "'Untitled (42)'\n",
            "'Untitled (43)'\n",
            "'Untitled (44)'\n",
            "'Untitled (45)'\n",
            "'Untitled (46)'\n",
            "'Untitled (47)'\n",
            "'Untitled (48)'\n",
            "'Untitled (49)'\n",
            " Untitled4.ipynb\n",
            "'Untitled (5)'\n",
            " Untitled5.ipynb\n",
            "'Untitled (6)'\n",
            " Untitled6.ipynb\n",
            "'Untitled (7)'\n",
            " Untitled7.ipynb\n",
            "'Untitled (8)'\n",
            " Untitled8.ipynb\n",
            "'Untitled (9)'\n",
            " Untitled9.ipynb\n",
            " アヤメの二値分類.ipynb\n",
            " 二次元配列と勾配問題.ipynb\n",
            " 住宅価格の予測.ipynb\n",
            " ダミーデータの作成.ipynb\n",
            " データセット作成.ipynb\n",
            " 信用情報の学習.ipynb\n",
            " アヤメのデータ分析.ipynb\n",
            " クラスいまにゅ動画.ipynb\n",
            " 卒業制作_中日ドラゴンズ.ipynb\n",
            " 卒業発表用.ipynb\n",
            " ロジスティック回帰.ipynb\n",
            " アンサンブル学習.ipynb\n",
            "'学習に関するテクニック(ゼロ作).ipynb'\n",
            " 富士山折紙問題.ipynb\n",
            " 小麦とチェス盤問題.ipynb\n",
            " 折紙問題20210709三浦伸太郎.ipynb\n",
            " オブジェクト指向の活用.ipynb\n",
            " 曽呂利新左衛門問題.ipynb\n",
            "'曽呂利新左衛門問題.ipynb のコピー'\n",
            " 栗まんじゅう問題.ipynb\n",
            " 機械学習フロー.ipynb\n",
            " 機械学習スクラッチ入門.ipynb\n",
            " 決定木.ipynb\n",
            " クラスとインスタンス特訓用.ipynb\n",
            " 線形回帰.ipynb\n",
            " 線形回帰とは.ipynb\n",
            " 線形回帰の基礎.ipynb\n",
            " 行列積の実装.ipynb\n",
            " 野田講師作成中の線形回帰.ipynb\n",
            " 関数いまにゅ動画.ipynb\n",
            " ヒント集.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv47wpH0JdZm"
      },
      "source": [
        "# データ準備\n",
        "dataset_path =\"train.csv\"\n",
        "df = pd.read_csv(dataset_path)\n",
        "\n",
        "y = df[\"SalePrice\"]\n",
        "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
        "y = np.array(y)\n",
        "X = np.array(X)\n",
        "y = y.astype(np.int)[:, np.newaxis]\n",
        "y = np.log(y)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "fZLpQFrMKsj5",
        "outputId": "96d747ac-19eb-43fc-9728-886c1120329e"
      },
      "source": [
        "# tensorflowで学習\n",
        "# 各種変数定義\n",
        "learning_rate = 0.001\n",
        "batch_size = 10\n",
        "num_epochs = 10\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 1\n",
        "\n",
        "# 空配列定義\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# ミニバッチイテレータ生成\n",
        "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
        "    Parameters\n",
        "    ---------------\n",
        "    x : 入力配列\n",
        "    \"\"\"\n",
        "    # 重み定義\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    # バイアス定義\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    # 計算グラフ構築（順伝播処理）\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 計算グラフ受け取る\n",
        "logits = example_net(X)\n",
        "# 損失定義\n",
        "loss_op =  tf.losses.mean_squared_error(labels=Y, predictions=logits)\n",
        "# 最適化手法の定義\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "# 最適化手法で、定義した損失を最小化するルールを作成\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "\n",
        "# 変数を扱うためのおまじない\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# tensorflowのセッション開始\n",
        "with tf.Session() as sess:\n",
        "    # 初期化\n",
        "    sess.run(init)\n",
        "    # 損失記録用リスト\n",
        "    loss_list = []\n",
        "    val_loss_list = []\n",
        "    # 学習回数分ループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # ミニバッチイテレータでループ\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        # 損失計算と格納\n",
        "        loss = sess.run(loss_op, feed_dict={X: X_train, Y: y_train})\n",
        "        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n",
        "        loss_list.append(loss)\n",
        "        val_loss_list.append(val_loss)    \n",
        "        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\n",
        "    \n",
        "    # 学習過程可視化\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.plot(loss_list, label='loss')\n",
        "    plt.plot(val_loss_list, label='val_loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # テストデータに適用\n",
        "    test_loss = sess.run(loss_op, feed_dict={X: X_test, Y: y_test})\n",
        "    print(\"test_mse : {:.3f}\".format(test_loss))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, loss : 42093588.0000, val_loss : 45590244.0000\n",
            "Epoch 1, loss : 4063574.2500, val_loss : 3944413.5000\n",
            "Epoch 2, loss : 2199407.0000, val_loss : 1608078.5000\n",
            "Epoch 3, loss : 1557475.1250, val_loss : 983844.1875\n",
            "Epoch 4, loss : 1108524.3750, val_loss : 618920.6250\n",
            "Epoch 5, loss : 812410.9375, val_loss : 395131.0000\n",
            "Epoch 6, loss : 633375.7500, val_loss : 274056.4062\n",
            "Epoch 7, loss : 511166.2500, val_loss : 201111.8594\n",
            "Epoch 8, loss : 426317.8750, val_loss : 157358.0312\n",
            "Epoch 9, loss : 357845.2500, val_loss : 125864.7344\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_mse : 276948.562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voBnB8rpykn4"
      },
      "source": [
        "**【問題5】MNISTのモデルを作成**\n",
        "\n",
        "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n",
        "\n",
        "\n",
        "3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n",
        "\n",
        "\n",
        "スクラッチで実装したモデルの再現を目指してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrxlxEZ89UMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980e9f59-745c-4cab-eaae-6ae9de0dbeb9"
      },
      "source": [
        "# データ準備\n",
        "# 読み込み\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "#　平滑化\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)\n",
        "\n",
        "# 正規化\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "# 変形\n",
        "y_train = y_train.astype(np.int)[:, np.newaxis]\n",
        "y_test = y_test.astype(np.int)[:, np.newaxis]\n",
        "\n",
        "# one-hotベクトル化\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:])\n",
        "y_test_one_hot = enc.fit_transform(y_test[:])\n",
        "\n",
        "# 分割\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULnoLe_ox3bN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1037b809-795f-4904-ab55-3d2b9cc22600"
      },
      "source": [
        "# tensorflowで学習\n",
        "# 各種変数定義\n",
        "learning_rate = 0.003\n",
        "batch_size = 1\n",
        "num_epochs = 20\n",
        "n_hidden1 = 50\n",
        "n_hidden2 = 100\n",
        "n_input = X_train.shape[1]\n",
        "n_samples = X_train.shape[0]\n",
        "n_classes = 10 # 2値分類からの変更箇所\n",
        "\n",
        "# 空配列定義\n",
        "X = tf.placeholder(\"float\", [None, n_input])\n",
        "Y = tf.placeholder(\"float\", [None, n_classes])\n",
        "\n",
        "# ミニバッチイテレータ生成\n",
        "get_mini_batch_train = GetMiniBatch(X_train[:1000], y_train[:1000], batch_size=batch_size)\n",
        "\n",
        "def example_net(x):\n",
        "    \"\"\"tensorflowを利用したニューラルネットワーク\n",
        "    Parameters\n",
        "    ---------------\n",
        "    x : 入力配列\n",
        "    \"\"\"\n",
        "    # 重み定義\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n",
        "        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n",
        "        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n",
        "    }\n",
        "    # バイアス定義\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n",
        "        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n",
        "        'b3': tf.Variable(tf.random_normal([n_classes]))\n",
        "    }\n",
        "    # 計算グラフ構築（順伝播処理）\n",
        "    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n",
        "    layer_1 = tf.nn.relu(layer_1)\n",
        "    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n",
        "    layer_2 = tf.nn.relu(layer_2)\n",
        "    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3']\n",
        "    return layer_output\n",
        "\n",
        "# 計算グラフ受け取る\n",
        "logits = example_net(X)\n",
        "# 損失定義\n",
        "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits)) # 2値分類からの変更箇所\n",
        "# 最適化手法の定義\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "# 最適化手法で、定義した損失を最小化するルールを作成\n",
        "train_op = optimizer.minimize(loss_op)\n",
        "# ACC計算\n",
        "correct_pred = tf.equal(tf.argmax(Y, 1), tf.argmax(logits, 1)) # 2値分類からの変更箇所\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
        "\n",
        "# 変数を扱うためのおまじない\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# tensorflowのセッション開始\n",
        "with tf.Session() as sess:\n",
        "    # 初期化\n",
        "    sess.run(init)\n",
        "    # 学習回数分ループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # ミニバッチイテレータでループ\n",
        "        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n",
        "            # 「最適化手法で、定義した損失を最小化するルール」を実行\n",
        "            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n",
        "        # 記録\n",
        "        train_loss, train_acc = sess.run([loss_op, accuracy], feed_dict={X: X_train, Y: y_train})\n",
        "        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n",
        "        # 仮定出力\n",
        "        print(\"Epoch {}, train_loss : {:.4f}, val_loss : {:.4f}, train_acc : {:.3f}, val_acc : {:.3f}\".format(epoch, train_loss, val_loss, train_acc, val_acc))\n",
        "    \n",
        "    # 学習が終了したらテストデータで実行\n",
        "    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test_one_hot})\n",
        "    #prediction = sess.run(logits, feed_dict={X: X_test, Y: y_test})\n",
        "    #print(prediction)\n",
        "    print(\"test_acc : {:.3f}\".format(test_acc))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, train_loss : 44.5134, val_loss : 44.5732, train_acc : 0.594, val_acc : 0.592\n",
            "Epoch 1, train_loss : 28.3701, val_loss : 28.2233, train_acc : 0.702, val_acc : 0.702\n",
            "Epoch 2, train_loss : 28.0623, val_loss : 28.0069, train_acc : 0.715, val_acc : 0.712\n",
            "Epoch 3, train_loss : 24.4618, val_loss : 24.4489, train_acc : 0.739, val_acc : 0.731\n",
            "Epoch 4, train_loss : 21.7868, val_loss : 21.6349, train_acc : 0.767, val_acc : 0.760\n",
            "Epoch 5, train_loss : 23.4652, val_loss : 23.6872, train_acc : 0.764, val_acc : 0.760\n",
            "Epoch 6, train_loss : 22.6102, val_loss : 22.6563, train_acc : 0.784, val_acc : 0.776\n",
            "Epoch 7, train_loss : 28.2148, val_loss : 28.9051, train_acc : 0.755, val_acc : 0.750\n",
            "Epoch 8, train_loss : 22.7048, val_loss : 23.0438, train_acc : 0.790, val_acc : 0.787\n",
            "Epoch 9, train_loss : 20.1201, val_loss : 20.3763, train_acc : 0.808, val_acc : 0.803\n",
            "Epoch 10, train_loss : 22.2948, val_loss : 21.9325, train_acc : 0.807, val_acc : 0.804\n",
            "Epoch 11, train_loss : 24.4859, val_loss : 24.2685, train_acc : 0.802, val_acc : 0.801\n",
            "Epoch 12, train_loss : 21.6518, val_loss : 21.5591, train_acc : 0.811, val_acc : 0.807\n",
            "Epoch 13, train_loss : 22.2695, val_loss : 22.0710, train_acc : 0.811, val_acc : 0.810\n",
            "Epoch 14, train_loss : 25.5567, val_loss : 25.0497, train_acc : 0.805, val_acc : 0.799\n",
            "Epoch 15, train_loss : 23.8475, val_loss : 24.3483, train_acc : 0.817, val_acc : 0.814\n",
            "Epoch 16, train_loss : 22.3687, val_loss : 22.7189, train_acc : 0.823, val_acc : 0.819\n",
            "Epoch 17, train_loss : 24.2571, val_loss : 24.9599, train_acc : 0.834, val_acc : 0.832\n",
            "Epoch 18, train_loss : 24.0030, val_loss : 23.9791, train_acc : 0.828, val_acc : 0.827\n",
            "Epoch 19, train_loss : 27.1417, val_loss : 27.2655, train_acc : 0.819, val_acc : 0.818\n",
            "test_acc : 0.830\n"
          ]
        }
      ]
    }
  ]
}